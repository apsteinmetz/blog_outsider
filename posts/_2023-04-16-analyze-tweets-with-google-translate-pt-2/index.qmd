---
title: "analyze tweets with google translate pt 2"
description: |
  
author: Art Steinmetz
date: 2023-04-16
draft: true
format: html
editor: visual
bibliography: references.bib
execute: 
  freeze: true
---

## In Our Last Episode...

In [part one of this series](https://outsiderdata.netlify.app/posts/2023-04-15-analyze-tweets-with-google-translate-pt-1/) we translated the afrisenti dataset using the Google Cloud Translate API. This dataset contains a little over 100,000 tweets in 13 different African languages[@Muhammad2023AfriSentiAT]. We saw that , at first glance, Google Translate does a good job on these languages. Are the translations good enough to run sentiment analysis on?

## Measuring Sentiment

Now that we have all of the tweets translated into English we can use sentiment tools calibrated to the English language. We will try two different approaches. For both we'll compare the measured sentiment of the English tweet to the assigned sentiment in the dataset. How close can we get to 100% agreement?

1.  Measure the sentiment "valence" of each word as either positive or negative and take the balance of positive vs. negative as the sentiment of the tweet.
2.  Negation turns a positive sentiment into a negative one. "I do not like" has one positive word, "like," but is clearly a negative sentiment. Rather than measure each word, we can run a sentence-level model to see if we get a better result.

## Sentiment Measurement at the Word Level

We will be using the tidytext package in this project and following the approach shown in the ["Introduction to tidytext" by Julia Silge and Dave Robinson](https://cran.r-project.org/web/packages/tidytext/vignettes/tidytext.html).

First load the needed packages and the translated tweets from part one of this project.

```{r load}
#| warning: false
#load libaries
suppressPackageStartupMessages({
  library(tidyverse)
  library(tidytext)
  library(hunspell)
  library(sentimentr)
  library(magrittr)
})
afrisenti_translated <- readr::read_csv('https://raw.githubusercontent.com/apsteinmetz/tidytuesday/master/2023-02-28_african_language/data/afrisenti_translated.csv',
                                        show_col_types = FALSE) |> 
  mutate(label = as.factor(label))
afrisenti_translated
```

The `tidytext` package makes it easy to divide each tweet into separate words and measure the sentiment valence of each one. We decide the sentiment of each word using two of the sentiment databases included with `tidytext`, "afinn" and "bing. If a word is not in one of these databases we code it as"0" or neutral. Further, we need to use the stems of words. "Idiots" is not in our sentiment databases, "idiot" is. Word stemming will fix that. The `Hunspell` package using `hunspell_stem()` will do the trick. It returns a list of possible stems (usually just one) so we have to `unnest()` the list column. The tradeoff is that if the word is not in `hunspell`'s dictionary, it drops the word. Fortunately, there are over 49,000 words in the dictionary.

```{r tokenize}
tweet_word_sentiment <- afrisenti_translated %>%
  select(tweet_num, translatedText, label) %>%
  unnest_tokens(word, translatedText) |>
  anti_join(stop_words, by = join_by(word)) %>%
  mutate(word = hunspell_stem(word)) |> 
  unnest(word) |> 
  left_join(get_sentiments("afinn"),
            multiple = "all",
            by = join_by(word)) %>%
  mutate(value = replace_na(value, 0)) %>%
  rename(sentiment_afinn = value) %>%
  left_join(get_sentiments("bing"),
            multiple = "all",
            by = join_by(word)) %>%
  mutate(sentiment = replace_na(sentiment, "neutral")) %>%
  rowwise() %>%
  mutate(sentiment_bing = switch(
    sentiment,
    "negative" = -1,
    "neutral" = 0,
    "positive" = 1
  )) |> 
  select(-sentiment)
```

There is an important difference between the two sentiment databases. "Bing" is just positive, negative or neutral. "Afinn" has degrees running from -5 to +5. I don't know who decided that "superb" gets a "5" and "euphoric" gets a "4," but there you are. We can see how the valences are distributed in our tweets.

```{r distribution}
tweet_word_sentiment |> 
  pivot_longer(cols = c("sentiment_afinn","sentiment_bing"),names_to = "database",values_to = "valence") |> 
  ggplot(aes(valence,fill = database)) + 
  geom_histogram(position = "dodge",bins = 10)
```

Let's look at just one example to see how this works.

```{r one sample}
afrisenti_translated$translatedText[45]
```

```{r}
tweet_word_sentiment |> 
  filter(tweet_num == "45")
```

The benefits and drawbacks of stemming are apparent. We can find the stem of "idiots" and "cannibals" but "unforgivable" gets changed to "forgivable" which flips its sentiment. No matter because neither word is in either sentiment database. There are no positive words in this tweet so the sum of all the valences is negative, which matches with the assigned sentiment.

We might expect that the finer granularity of the "afinn" database would yield a better result but we'll see.

Add up the valences for each tweet to arrive at the net sentiment.

```{r calc each tweet}
tweet_sentiment <- tweet_word_sentiment %>% 
  group_by(tweet_num) %>% 
  summarise(sentiment_afinn = as.integer(sum(sentiment_afinn)),
            sentiment_bing = as.integer(sum(sentiment_bing)),
            .groups = "keep") %>% 
  ungroup() |> 
  left_join(afrisenti_translated,by = join_by(tweet_num))

tweet_sentiment
```

We have numeric valences so we have to make a choice what to label each number. Obviously, zero is neutral but should we expand the neutral range to, say -1 to +1? Sadly, no. The summary below shows that we assigned far more neutral labels than the original data set has. This is not a good omen. We wish we had more granularity.

```{r}
tweet_sentiment <- tweet_sentiment |> 
  mutate(label_afinn = as.factor(cut(sentiment_afinn,
                                     breaks = c( -Inf,-1,0,Inf),                         labels=c("negative","neutral","positive")))) %>% 
  mutate(label_bing = as.factor(cut(sentiment_bing,
                          breaks = c( -Inf,-1,0,Inf), labels=c("negative","neutral","positive")))) |> 
  select(tweet_num,translatedText,label,label_afinn,label_bing)

summary(tweet_sentiment)
```

**Results** How did we do? We can generate a confusion matrix with some additional statistics to look at the agreement of our measurements vs. the human classifiers. Ideally all the observations would lie on the diagonal from top left to bottom right. First, we'll test the "afinn" sentiment.

```{r evaluate}
xt <- table(tweet_sentiment$label,tweet_sentiment$label_afinn)
xt
```

```{r}
caret::confusionMatrix(xt) |> 
  broom::tidy() |> 
  slice_head(n=2)
```

Now we'll look at the "bing" measures.

```{r}
xt <- table(tweet_sentiment$label,tweet_sentiment$label_bing)
xt
```

```{r}
caret::confusionMatrix(xt) |> 
  broom::tidy() |> 
  slice_head(n=2)
```

As we look at the matrix there are many, many incorrect classifications. The accuracy for both of these measures is scarcely more that 50%. The "Kappa" statistic shows that we are only about 26% better than random chance. It's not zero but it's not good. Why the disappointing result? First of all, our valence measure isn't opinionated enough. There are far too many neutral tweets. Further we don't address negation, as mentioned above. The tweet below illustrates the problem.

```{r illustrate error}
afrisenti_translated$translatedText[28670]
```

```{r}
tweet_word_sentiment |> 
  filter(tweet_num == "28670")
```

Unless the negation is itself a negative valence word, many negative tweets will test as positive. In this example "peaceful" is positive but "don't want" clearly flips it.

**A More Sophisticated Approach**

There are a number of approaches to solve the negation problem. A simple one is to combine negation words like "no" and "not" with the subsequent one. We will use this approach in our next post attempting machine learning. Here we'll try sentence-level measurment using the `sentimentr` package. This package understands negation better. Our test tweet from above is a challenge. This is since the negation of "peaceful" comes eight words before "not." The default of `n.before = 5` doesn't give us the right answer.

```{r}
mytweets <- "Why don't America want the world to be peaceful"

sentimentr::sentiment(mytweets,n.before = 5)
```

Setting `n.before = Inf` captures this but comes at the expense of slower processing speed and potentially more false negatives.

```{r}
sentimentr::sentiment(mytweets,n.before = Inf)
```

With that noted, lets forge ahead.

As before, we start by breaking the tweets up into constituent parts, sentences in this case. Most tweets will be just one sentence of course. Again we compute a sentiment score, this time with the `sentiment_by()` function. It yields a finer grained sentiment score.

::: callout-note
I am using a less common pipe operator below, `%$%` from the `magrittr` package, which expands the list-column created by `get_sentences`. Normally I would use `tidyr::unnest()` to do this but it loses the special object class that `sentiment_by()` needs.
:::

Before we look at the tweet-level sentiment let's group by language.  This lets us see if the sentiment measure is consistent across languages.

```{r sentences by language}
tweet_sentence <- afrisenti_translated |>  
  mutate(sentence = get_sentences(translatedText)) %$%
  sentiment_by(sentence,list(assigned_long),n.before = Inf)

# There is a built-in plot method for 'sentiment_by()' objects.
plot(tweet_sentence)
```
The red diamond is the average sentiment across all tweets. It does look consistent.

Now let's assign a valence to each tweet and look at the distribution of values.
```{r sentences by tweetnum}
tweet_sentence <- afrisenti_translated |>  
  mutate(sentence = get_sentences(translatedText)) %$%
  sentiment_by(sentence,list(tweet_num),n.before = Inf)

hist(tweet_sentence$ave_sentiment,breaks = 50)
```
The median of this distribution is zero but there are far fewer tweets that are of exactly zero valence so we can expand the range of neutral.  I tried to balance the data to match the distribution of sentiment in the original data as closely as possible.
```{r cut sentence valence}
low = -0.01
high = 0.12
tweet_sentence <- tweet_sentence |> 
  as_tibble() |> 
  mutate(label_sentence = cut(ave_sentiment,breaks = c( -Inf,low,high,Inf), labels=c("negative","neutral","positive")))

tweet_sentence |> group_by(label_sentence) |> 
  count() |> 
  ungroup() |> 
  mutate(p = n/sum(n))
```

Compare our distribution to the original.
```{r compare distribution}
orig <- tweet_sentiment |> 
  group_by(label) |> 
  count() |> 
  ungroup() |> 
  mutate(original = n/sum(n))
trans <- tweet_sentence |> 
  rename(label = label_sentence) |> 
  group_by(label) |> 
  count() |> 
  ungroup() |> 
  mutate(translation = n/sum(n))

left_join(orig,trans,by="label") |> 
  pivot_longer(c(original,translation),
               names_to = "dataset",values_to = "proportion") |> 
  group_by(label) |> 
  ggplot(aes(label,proportion,fill=dataset)) + geom_col(position = "dodge")
```
As you can see, we are pretty balanced.  This is kind of cheating because we are calibrating the category breakpoints to the known data.  We should be calibrating to a training set and then see how it works on the test set.  Let's just see if the in-sample fit is any good first.
```{r sentence results}
combo <- left_join(tweet_sentiment,tweet_sentence,by="tweet_num")
xt <- table(combo$label,combo$label_sentence)
xt
```

```{r}
caret::confusionMatrix(xt) |> 
  broom::tidy() |> 
  slice_head(n=2)
```

```

