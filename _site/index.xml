<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Outsider Data Science</title>
<link>outsiderdata.netlify.app/index.html</link>
<atom:link href="outsiderdata.netlify.app/index.xml" rel="self" type="application/rss+xml"/>
<description>Putting what&#39;s in there, out there. With R!</description>
<generator>quarto-1.2.335</generator>
<lastBuildDate>Mon, 24 Jan 2022 05:00:00 GMT</lastBuildDate>
<item>
  <title>Switching to Quarto from Blogdown</title>
  <dc:creator>Art Steinmetz</dc:creator>
  <link>outsiderdata.netlify.app/posts/2023-01-24_Switching-to-Quarto/2023-1-24_switching-to-quarto.html</link>
  <description><![CDATA[ 



<p>It all started when I decided to change up my Hugo theme. Up until that point I was happily using the Blogdown add-in for RStudio to initiate new blog posts. At some point the default directory structure for Hugo blogs changed and when I tried to update my theme the whole web site got impossibly messy and finally broke. I managed to cobble it back together but I really had no idea what I was doing and became afraid of messing with it any more. This led to bloggers block.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>This post assumes you want to change your Hugo website that you already built on Netlify to use Quarto. If you are looking to a basic tutorial on building a Quarto web site <a href="https://quarto.org/docs/websites/website-blog.html">look here</a>.</p>
</div>
</div>
<p><img src="outsiderdata.netlify.app/posts/2023-01-24_Switching-to-Quarto/img/quarto.png" class="img-fluid"></p>
<p>When Quarto came on the scene I was attracted to the idea of starting fresh but also intimidated by the thought of porting all my old content from R Markdown to Quarto. Plenty of people told me that R Markdown was not going away and there was no big need to switch. It is the big improvement over Blogdown/Hugo for blogging that has made me a convert. Quarto is simpler.</p>
<p>Ideally you should just be able to render the R Markdown code in Quarto and it will just work. For me, the problem was that many of my posts would no longer render in R because of package updates or out-of-date web links. Quarto solves that problem with the <code>freeze</code> option but it only works for content that has been rendered at least once with Quarto.</p>
<p>I needn’t have fretted. It turns out porting old blog or web content is ridiculously easy, though the existing Quarto guides don’t discuss this <strong><em>one simple trick</em>.</strong> Here’s the TL;DR version: all you have to do is change the file extension of your fully rendered HTML files from <code>.html</code> to <code>.qmd</code> and Quarto will happily render them, wrapping its own HTML code around the old HTML but not otherwise messing with it. Any theming or subsequent changes in theming will be properly rendered. You can get rid of your old R Markdown files (though they will live in GitHub forever, right?).</p>
<p>I never did figure out the directory structure of a Hugo web site. Fortunately, Quarto is much simpler. The main directory and subdirectories of your blog contain all the Quarto files. You can name them as you like. One special directory called <code>_site</code> contains the rendered HTML files. It will be created the first time you <code>render</code> your site. There are a few more things to do to bring it all together. Let’s go through them step-by-step.</p>
<ol type="1">
<li><p>Assuming you <strong>Create New Project</strong> in RStudio a subdirectory will be created called <code>posts</code>. Create a sub-folder in your <code>posts</code> folder to hold the converted file. Use any name but I like the form <strong><em>yyyy-mm-dd_name-of-post</em></strong>. This will allow displaying the posts in date order.</p></li>
<li><p>Copy old rendered HTML document (not the RMD markdown document) to the new folder.</p></li>
<li><p>Rename copied HTML file from <strong><em>&lt;filename&gt;.html</em></strong> to <strong><em>&lt;filename&gt;.qmd</em></strong>.</p></li>
<li><p>Load QMD file into RStudio for some light editing.</p></li>
<li><p>Blogdown uses tags and categories in the YAML header while Quarto uses only categories. In the editor, combine the YAML header <code>tags</code> and <code>categories</code> into just <code>categories.</code></p>
<p>So this:</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode yaml code-with-copy"><code class="sourceCode yaml"><span id="cb1-1"><span class="fu" style="color: #4758AB;">categories</span><span class="kw" style="color: #003B4F;">:</span></span>
<span id="cb1-2"><span class="at" style="color: #657422;">  </span><span class="kw" style="color: #003B4F;">-</span><span class="at" style="color: #657422;"> R</span></span>
<span id="cb1-3"><span class="fu" style="color: #4758AB;">tags</span><span class="kw" style="color: #003B4F;">:</span></span>
<span id="cb1-4"><span class="at" style="color: #657422;">  </span><span class="kw" style="color: #003B4F;">-</span><span class="at" style="color: #657422;"> quarto</span></span>
<span id="cb1-5"><span class="at" style="color: #657422;">  </span><span class="kw" style="color: #003B4F;">-</span><span class="at" style="color: #657422;"> blogging</span></span></code></pre></div>
<p>becomes this:</p>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode yaml code-with-copy"><code class="sourceCode yaml"><span id="cb2-1"><span class="fu" style="color: #4758AB;">categories</span><span class="kw" style="color: #003B4F;">:</span></span>
<span id="cb2-2"><span class="at" style="color: #657422;">  </span><span class="kw" style="color: #003B4F;">-</span><span class="at" style="color: #657422;"> R</span></span>
<span id="cb2-3"><span class="at" style="color: #657422;">  </span><span class="kw" style="color: #003B4F;">-</span><span class="at" style="color: #657422;"> quarto</span></span>
<span id="cb2-4"><span class="at" style="color: #657422;">  </span><span class="kw" style="color: #003B4F;">-</span><span class="at" style="color: #657422;"> blogging</span></span></code></pre></div></li>
</ol>
<div class="callout-important callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Old Man Yelling at Cloud
</div>
</div>
<div class="callout-body-container callout-body">
<p>Quarto is great but YAML sucks. Who’s idea was that? Just Google ‘YAML sucks’ to see all the reasons why. RStudio’s YAML completion helps….a little.</p>
</div>
</div>
<ol start="6" type="1">
<li><p>Of course you want your code snippets to have a consistent theme. Quarto uses a different HTML class id to style code snippets but that’s easily fixed. In the editor <strong>Search/Replace</strong> all instances of <code>class="r"</code> to <code>class="sourceCode r"</code>.</p></li>
<li><p>Most of your posts will have images you added or images that were generated during the render. We need to put those old images in a place where Quarto can find them. Create a sub-folder below the one just created called <code>img</code>.</p></li>
<li><p>Copy any image files from the old version over to the <code>img</code> directory we just made.</p></li>
<li><p>In the editor, fix path names of any image files to point to the <code>img</code> folder. A quick search-and-replace should do it. The HTML tags will look like:</p>
<p><code>&lt;img src="somwhere_else/my_old_folder/unnamed-chunk-13-1.png" width="672" /&gt;</code></p>
<p>which you should change to:</p>
<p><code>&lt;img src="img/unnamed-chunk-13-1.png" width="672" /&gt;</code></p></li>
<li><p>Repeat the steps above for additional posts.</p></li>
<li><p>Does your blog use any Javascript HTML widgets? In my case a post that used the the <code>Plotly</code> package created some. These will be found in your old site in the <code>rmarkdown-libs</code> folder. Copy this folder over to the base directory of your new blog. Back in the editor make sure any lines of HTML that contain <code>rmarkdown-libs</code> have a valid path to new location. As a side note, if you render the file as R code, Quarto will put those widgets in a directory called <code>site-libs</code>.</p></li>
<li><p>Save and Render. Done!</p></li>
</ol>
<p>From this point you can go down the rabbit hole and play with all the theming and formatting options that Quarto allows. You’ll make most of these changes in your <code>index.qmd</code> and <code>_quarto.yml</code> files. Any changes you make will be reflected in your old posts. Easy peasy!</p>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>I really liked the Blogdown add-in for R Studio as it made initiating a new post very easy. <a href="https://themockup.blog/about.html">Thomas Mock</a> has made a start on this with a <a href="https://themockup.blog/posts/2022-11-08-use-r-to-generate-a-quarto-blogpost/">quick function to start a new Quarto post</a>. Check it out.</p>
</div>
</div>
<section id="publishing-your-blog-on-netlify" class="level2">
<h2 class="anchored" data-anchor-id="publishing-your-blog-on-netlify">Publishing Your Blog on Netlify</h2>
<p>You will have to make some tiny changes before Netfliy will re-publish your sparkling remodeled blog.</p>
<ol type="1">
<li><p>Once you are happy with the finished port, delete everything in the old blog’s directory EXCEPT the <code>.git</code> file. Copy everything from the new blog EXCEPT the <code>.git</code> file over to the old directory. It’s scary but you are just burning the old site down.</p></li>
<li><p>Git should show all the deletions and additions to be made to the repo. Commit them all and push to GitHub.</p></li>
<li><p>Now visit your deploy settings at Netlify.com. You’ll find them at <em>https://app.netlify.com/sites/&lt;yoursitename&gt;/settings/deploys.</em></p></li>
<li><p>Click on <button class="btn btn-dark">Edit settings</button></p></li>
<li><p>You’ll change the build command from “Hugo” to nothing and change the publish directory to “_site.” Then save.</p></li>
</ol>
<p><img src="outsiderdata.netlify.app/posts/2023-01-24_Switching-to-Quarto/img/netlify_deploy.jpg" class="img-fluid" width="326"></p>
<p>That’s it! Netlfiy should regenerate your site in a few seconds and you are back in business. Have fun!</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Thank you!
</div>
</div>
<div class="callout-body-container callout-body">
<p>Thanks to <a href="https://hookedondata.org/">Emily Robinson</a> for jump starting me on this project. She pointed me to great resources on writing a Quarto blog and convinced me that “you can do it!”</p>
</div>
</div>


</section>

 ]]></description>
  <category>quarto</category>
  <category>blogging</category>
  <guid>outsiderdata.netlify.app/posts/2023-01-24_Switching-to-Quarto/2023-1-24_switching-to-quarto.html</guid>
  <pubDate>Mon, 24 Jan 2022 05:00:00 GMT</pubDate>
  <media:content url="outsiderdata.netlify.app/posts/2023-01-24_Switching-to-Quarto/img/quarto.png" medium="image" type="image/png" height="35" width="144"/>
</item>
<item>
  <title>Covid Cases vs. Deaths</title>
  <dc:creator>Art Steinmetz</dc:creator>
  <link>outsiderdata.netlify.app/posts/2020-12-06-covid-cases-vs-deaths/2020-12-06-covid-cases-vs-deaths.html</link>
  <description><![CDATA[ 



<link href="../../rmarkdown-libs/anchor-sections/anchor-sections.css" rel="stylesheet">
<script src="../../rmarkdown-libs/anchor-sections/anchor-sections.js"></script>
<section id="introduction" class="level1">
<h1>
Introduction
</h1>
<p>
I have a macabre fascination with tracking the course of the COVID-19 pandemic. I suspect there are two reasons for this. One, by delving into the numbers I imagine I have some control over this thing. Second, it feels like lighting a candle to show that science can reveal truth at a time when the darkness of anti-science is creeping across the land.
</p>
<p>
The purpose of this project is, as usual, twofold. First, to explore an interesting data science question and, second, to explore some techniques and packages in the R universe. We will be looking at the relationship of COVID-19 cases to mortality. What is the lag between a positive case and a death? How does that vary among states? How has it varied as the pandemic has progressed? This is an interesting project because is combines elements of time series forecasting and dependent variable prediction.
</p>
<p>
I have been thinking about how to measure mortality lags for a while now. What prompted to do a write-up was discovering a new function in Matt Dancho’s <code>timetk</code> package, <code>tk_augment_lags</code>, which makes short work of building multiple lags. Not too long ago, managing models for multiple lags and multiple states would have been a bit messy. The emerging “tidy models” framework from RStudio using “list columns” is immensely powerful for this sort of thing. It’s great to reduce so much analysis into so few lines of code.
</p>
<p>
This was an exciting project because I got some validation of my approach. I am NOT an epidemiologist or a professional data scientist. None of the results I show here should be considered authoritative. Still, while I was working on this project I saw <a href="https://www.wsj.com/livecoverage/covid-2020-12-02/card/kdSg0ILBfalJHzXvX0bF">this article</a> in the “Wall Street Journal” which referenced the work by <a href="https://epi.washington.edu/faculty/bedford-trevor">Dr.&nbsp;Trevor Bedford</a>, an epidemiologist at the University of Washington. He took the same approach I did and got about the same result.
</p>
</section>
<section id="aquire-and-clean-data" class="level1">
<h1>
Aquire and Clean Data
</h1>
<p>
There is no shortage of data to work with. Here we will use the NY Times COVID tracking data set which is updated daily. The package <code>covid19nytimes</code> lets us refresh the data on demand.
</p>
<pre class="sourceCode r code-with-copy"><code># correlate deaths and cases by state
library(tidyverse)
library(covid19nytimes)
library(timetk)
library(lubridate)
library(broom)
library(knitr)

# source https://github.com/nytimes/covid-19-data.git
us_states_long &lt;- covid19nytimes::refresh_covid19nytimes_states()

# if link is broken
#load("../data/us_states_long.rdata")

# use data from November 15 to stay consistent with text narrative
cutoff_start &lt;- as.Date("2020-03-15") # not widespread enough until then
cutoff_end &lt;- max(us_states_long$date) -7 # discard last week since there are reporting lags

us_states_long &lt;- us_states_long %&gt;% filter(date &gt;= cutoff_start)
us_states_long &lt;- us_states_long %&gt;% filter(date &lt;= cutoff_end)
# Remove tiny territories
territories &lt;- c("Guam","Northern Mariana Islands")
us_states_long &lt;- us_states_long %&gt;% filter(!(location %in% territories))
save(us_states_long,file="us_states_long.rdata")
us_states_long %&gt;% head() %&gt;% kable()</code></pre>

<table class="table">
<thead>
<tr class="header">
<th align="left">
date
</th>
<th align="left">
location
</th>
<th align="left">
location_type
</th>
<th align="left">
location_code
</th>
<th align="left">
location_code_type
</th>
<th align="left">
data_type
</th>
<th align="right">
value
</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">
2020-11-28
</td>
<td align="left">
Alabama
</td>
<td align="left">
state
</td>
<td align="left">
01
</td>
<td align="left">
fips_code
</td>
<td align="left">
cases_total
</td>
<td align="right">
244993
</td>
</tr>
<tr class="even">
<td align="left">
2020-11-28
</td>
<td align="left">
Alabama
</td>
<td align="left">
state
</td>
<td align="left">
01
</td>
<td align="left">
fips_code
</td>
<td align="left">
deaths_total
</td>
<td align="right">
3572
</td>
</tr>
<tr class="odd">
<td align="left">
2020-11-28
</td>
<td align="left">
Alaska
</td>
<td align="left">
state
</td>
<td align="left">
02
</td>
<td align="left">
fips_code
</td>
<td align="left">
cases_total
</td>
<td align="right">
31279
</td>
</tr>
<tr class="even">
<td align="left">
2020-11-28
</td>
<td align="left">
Alaska
</td>
<td align="left">
state
</td>
<td align="left">
02
</td>
<td align="left">
fips_code
</td>
<td align="left">
deaths_total
</td>
<td align="right">
115
</td>
</tr>
<tr class="odd">
<td align="left">
2020-11-28
</td>
<td align="left">
Arizona
</td>
<td align="left">
state
</td>
<td align="left">
04
</td>
<td align="left">
fips_code
</td>
<td align="left">
cases_total
</td>
<td align="right">
322774
</td>
</tr>
<tr class="even">
<td align="left">
2020-11-28
</td>
<td align="left">
Arizona
</td>
<td align="left">
state
</td>
<td align="left">
04
</td>
<td align="left">
fips_code
</td>
<td align="left">
deaths_total
</td>
<td align="right">
6624
</td>
</tr>
</tbody>

</table>
<p>
The NY Times data is presented in a “long” format. When we start modeling, long will suit us well but first we have to add features to help us and that will require <code>pivot</code>ing to wide, adding features and then back to long. The daily data is so irregular the first features we will add are 7-day moving averages to smooth the series. We’ll also do a nation-level analysis first so we aggregate the state data as well.
</p>
<pre class="sourceCode r code-with-copy"><code># Create rolling average changes
# pivot wider
# this will also be needed when we create lags
us_states &lt;- us_states_long %&gt;%
  # discard dates before cases were tracked.
  filter(date &gt; as.Date("2020-03-01")) %&gt;% 
  pivot_wider(names_from="data_type",values_from="value") %&gt;% 
  rename(state=location) %&gt;%
  select(date,state,cases_total,deaths_total) %&gt;%
  mutate(state = as_factor(state)) %&gt;% 
  arrange(state,date) %&gt;% 
  group_by(state) %&gt;%
  #smooth the data with 7 day moving average
  mutate(cases_7day = (cases_total - lag(cases_total,7))/7) %&gt;%
  mutate(deaths_7day = (deaths_total - lag(deaths_total,7))/7) %&gt;%
  {.}

# national analysis
# ----------------------------------------------
# aggregate state to national
us &lt;- us_states %&gt;%
  group_by(date) %&gt;% 
  summarize(across(.cols=where(is.double),
                   .fns = function(x)sum(x,na.rm = T),
                   .names="{.col}"))

us[10:20,] %&gt;% kable()</code></pre>

<table class="table">
<thead>
<tr class="header">
<th align="left">
date
</th>
<th align="right">
cases_total
</th>
<th align="right">
deaths_total
</th>
<th align="right">
cases_7day
</th>
<th align="right">
deaths_7day
</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">
2020-03-24
</td>
<td align="right">
53906
</td>
<td align="right">
784
</td>
<td align="right">
6857.571
</td>
<td align="right">
95.28571
</td>
</tr>
<tr class="even">
<td align="left">
2020-03-25
</td>
<td align="right">
68540
</td>
<td align="right">
1053
</td>
<td align="right">
8599.714
</td>
<td align="right">
127.28571
</td>
</tr>
<tr class="odd">
<td align="left">
2020-03-26
</td>
<td align="right">
85521
</td>
<td align="right">
1352
</td>
<td align="right">
10448.571
</td>
<td align="right">
162.85714
</td>
</tr>
<tr class="even">
<td align="left">
2020-03-27
</td>
<td align="right">
102847
</td>
<td align="right">
1769
</td>
<td align="right">
12121.286
</td>
<td align="right">
213.14286
</td>
</tr>
<tr class="odd">
<td align="left">
2020-03-28
</td>
<td align="right">
123907
</td>
<td align="right">
2299
</td>
<td align="right">
14199.143
</td>
<td align="right">
277.00000
</td>
</tr>
<tr class="even">
<td align="left">
2020-03-29
</td>
<td align="right">
142426
</td>
<td align="right">
2717
</td>
<td align="right">
15625.714
</td>
<td align="right">
322.85714
</td>
</tr>
<tr class="odd">
<td align="left">
2020-03-30
</td>
<td align="right">
163893
</td>
<td align="right">
3367
</td>
<td align="right">
17202.429
</td>
<td align="right">
398.42857
</td>
</tr>
<tr class="even">
<td align="left">
2020-03-31
</td>
<td align="right">
188320
</td>
<td align="right">
4302
</td>
<td align="right">
19202.000
</td>
<td align="right">
502.57143
</td>
</tr>
<tr class="odd">
<td align="left">
2020-04-01
</td>
<td align="right">
215238
</td>
<td align="right">
5321
</td>
<td align="right">
20956.857
</td>
<td align="right">
609.71429
</td>
</tr>
<tr class="even">
<td align="left">
2020-04-02
</td>
<td align="right">
244948
</td>
<td align="right">
6537
</td>
<td align="right">
22775.286
</td>
<td align="right">
740.71429
</td>
</tr>
<tr class="odd">
<td align="left">
2020-04-03
</td>
<td align="right">
277264
</td>
<td align="right">
7927
</td>
<td align="right">
24916.714
</td>
<td align="right">
879.71429
</td>
</tr>
</tbody>

</table>
</section>
<section id="exploratory-data-analysis" class="level1">
<h1>
Exploratory Data Analysis
</h1>
<p>
We might be tempted to simply regress deaths vs.&nbsp;cases but a scatter plot shows us that would not be satisfactory. As it turns out, the relationship of cases and deaths is strongly conditioned on date. This reflects the declining mortality rate as we have come to better understand the disease.
</p>
<pre class="sourceCode r code-with-copy"><code># does a simple scatterplot tell us anything 
# about the relationship of deaths to cases? No.
us %&gt;% 
  ggplot(aes(deaths_7day,cases_7day)) + geom_point() +
  labs(title = "Not Useful",
       caption = "Source: NY Times, Arthur Steinmetz")</code></pre>
<p>
<img src="outsiderdata.netlify.app/posts/2020-12-06-covid-cases-vs-deaths/img/silly plot-1.png" width="672">
</p>
<p>
We can get much more insight plotting smoothed deaths and cases over time. It is generally bad form to use two different y axes on a single plot but but this example adds insight. A couple of observations are obvious. First when cases start to rise, deaths follow with a lag. Second, we have had three spikes in cases so far and in each successive instance the mortality has risen by a smaller amount. This suggests that, thankfully, we are getting better at treating this disease. It is NOT a function of increased testing because <a href="http://91-divoc.com/pages/covid-visualization/?chart=countries&amp;highlight=United%20States&amp;show=highlight-only&amp;y=highlight&amp;scale=linear&amp;data=testPositivity-daily-7&amp;data-source=merged&amp;xaxis=right-12wk#countries">positivity rates</a> have not been falling.
</p>
<pre class="sourceCode r code-with-copy"><code>#visualize the relationship between rolling average of weekly cases and deaths
coeff &lt;- 30
us %&gt;% 
  ggplot(aes(date,cases_7day)) + geom_line(color="orange") +
  theme(legend.position = "none") +
  geom_line(aes(x=date,y=deaths_7day*coeff),color="red") +
  scale_y_continuous(labels = scales::comma,
                     name = "Cases",
                     sec.axis = sec_axis(deaths_7day~./coeff,
                                         name="Deaths",
                                         labels = scales::comma)) +
  theme(
    axis.title.y = element_text(color = "orange", size=13),
    axis.title.y.right = element_text(color = "red", size=13)
  ) +
  labs(title =  "U.S. Cases vs. Deaths",
       subtitle = "7-Day Average",
       caption = "Source: NY Times, Arthur Steinmetz",
       x = "Date")</code></pre>
<p>
<img src="outsiderdata.netlify.app/posts/2020-12-06-covid-cases-vs-deaths/img/unnamed-chunk-2-1.png" width="672"> This illustrates a problem for any modeling we might do.It looks like the more cases surge, the less the impact on deaths. This is NOT a valid conclusion. A simple regression of deaths vs.&nbsp;cases and time shows the passage of time has more explanatory power than cases in predicting deaths so we have to take that into account.
</p>
<pre class="sourceCode r code-with-copy"><code># passage of time affects deaths more than cases
lm(deaths_7day~cases_7day+date,data=us) %&gt;% tidy()</code></pre>
<pre><code>## # A tibble: 3 x 5
##   term           estimate   std.error statistic  p.value
##   &lt;chr&gt;             &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
## 1 (Intercept) 76542.      10054.           7.61 5.15e-13
## 2 cases_7day      0.00828     0.00112      7.41 1.86e-12
## 3 date           -4.11        0.547       -7.52 9.11e-13</code></pre>
</section>
<section id="build-some-models" class="level1">
<h1>
Build Some Models
</h1>
<p>
We’ll approach this by running regression models of deaths and varying lags (actually leads) of cases. We chose to lead deaths as opposed to lagging cases because it will allow us to make predictions about the future of deaths given cases today. We include the date as a variable as well. Once we’ve run regressions against each lead period, we’ll chose the lead period that has the best fit (R-Squared) to the data.
</p>
<p>
The requires a lot of leads and a lot of models. Fortunately, R provides the tools to make this work very simple and well organized. First we add new columns for each lead period using <code>timetk::tk_augment_lags</code>. This one function call does all the work but it only does lags so we have to futz with it a bit to get leads.
</p>
<p>
I chose to add forty days of leads. I don’t really think that long a lead is realistic and, given the pandemic has been around only nine months, there aren’t as many data points forty days ahead. Still, I want to see the behavior of the models. Once we have created the leads we remove any dates for which we don’t have led deaths.
</p>
<pre class="sourceCode r code-with-copy"><code>#create columns for deaths led 0 to 40 days ahead
max_lead &lt;- 40
us_lags &lt;- us %&gt;%
  # create lags by day
  tk_augment_lags(deaths_7day,.lags = 0:-max_lead,.names="auto")
  # fix names to remove minus sign
  names(us_lags) &lt;- names(us_lags) %&gt;% str_replace_all("lag-|lag","lead")

# use only case dates where we have complete future knowledge of deaths for all lead times.
us_lags &lt;- us_lags %&gt;% filter(date &lt; cutoff_end-max_lead)

us_lags[1:10,1:7] %&gt;% kable()</code></pre>

<table class="table">
<colgroup>
<col width="11%">
<col width="12%">
<col width="13%">
<col width="11%">
<col width="12%">
<col width="18%">
<col width="18%">
</colgroup>
<thead>
<tr class="header">
<th align="left">
date
</th>
<th align="right">
cases_total
</th>
<th align="right">
deaths_total
</th>
<th align="right">
cases_7day
</th>
<th align="right">
deaths_7day
</th>
<th align="right">
deaths_7day_lead0
</th>
<th align="right">
deaths_7day_lead1
</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">
2020-03-15
</td>
<td align="right">
3597
</td>
<td align="right">
68
</td>
<td align="right">
0.000
</td>
<td align="right">
0.00000
</td>
<td align="right">
0.00000
</td>
<td align="right">
0.00000
</td>
</tr>
<tr class="even">
<td align="left">
2020-03-16
</td>
<td align="right">
4504
</td>
<td align="right">
91
</td>
<td align="right">
0.000
</td>
<td align="right">
0.00000
</td>
<td align="right">
0.00000
</td>
<td align="right">
0.00000
</td>
</tr>
<tr class="odd">
<td align="left">
2020-03-17
</td>
<td align="right">
5903
</td>
<td align="right">
117
</td>
<td align="right">
0.000
</td>
<td align="right">
0.00000
</td>
<td align="right">
0.00000
</td>
<td align="right">
0.00000
</td>
</tr>
<tr class="even">
<td align="left">
2020-03-18
</td>
<td align="right">
8342
</td>
<td align="right">
162
</td>
<td align="right">
0.000
</td>
<td align="right">
0.00000
</td>
<td align="right">
0.00000
</td>
<td align="right">
0.00000
</td>
</tr>
<tr class="odd">
<td align="left">
2020-03-19
</td>
<td align="right">
12381
</td>
<td align="right">
212
</td>
<td align="right">
0.000
</td>
<td align="right">
0.00000
</td>
<td align="right">
0.00000
</td>
<td align="right">
0.00000
</td>
</tr>
<tr class="even">
<td align="left">
2020-03-20
</td>
<td align="right">
17998
</td>
<td align="right">
277
</td>
<td align="right">
0.000
</td>
<td align="right">
0.00000
</td>
<td align="right">
0.00000
</td>
<td align="right">
0.00000
</td>
</tr>
<tr class="odd">
<td align="left">
2020-03-21
</td>
<td align="right">
24513
</td>
<td align="right">
360
</td>
<td align="right">
0.000
</td>
<td align="right">
0.00000
</td>
<td align="right">
0.00000
</td>
<td align="right">
55.57143
</td>
</tr>
<tr class="even">
<td align="left">
2020-03-22
</td>
<td align="right">
33046
</td>
<td align="right">
457
</td>
<td align="right">
4204.714
</td>
<td align="right">
55.57143
</td>
<td align="right">
55.57143
</td>
<td align="right">
69.57143
</td>
</tr>
<tr class="odd">
<td align="left">
2020-03-23
</td>
<td align="right">
43476
</td>
<td align="right">
578
</td>
<td align="right">
5565.143
</td>
<td align="right">
69.57143
</td>
<td align="right">
69.57143
</td>
<td align="right">
95.28571
</td>
</tr>
<tr class="even">
<td align="left">
2020-03-24
</td>
<td align="right">
53906
</td>
<td align="right">
784
</td>
<td align="right">
6857.571
</td>
<td align="right">
95.28571
</td>
<td align="right">
95.28571
</td>
<td align="right">
127.28571
</td>
</tr>
</tbody>

</table>
<p>
…etc up to 40
</p>
<p>
Now we start the job of actually building the linear models and seeing the real power of the tidy modeling framework. Since we have our lead days in columns we revert back to long-form data. For each date we have a case count and 40 lead days with the corresponding death count. As will be seen below, the decline in the fatality rate has been non-linear, so we use a second-order polynomial to regress the <code>date</code> variable.
</p>
<p>
Our workflow looks like this:
</p>
<ol style="list-style-type: decimal">
<li>
Create the lags using <code>tk_augment_lag</code> (above).
</li>
<li>
<code>pivot</code> to long form.
</li>
<li>
<code>nest</code> the data by lead day and state.
</li>
<li>
<code>map</code> the data set for each lead day to a regression model.
</li>
<li>
Pull out the adjusted R-Squared using <code>glance</code> for each model to determine the best fit lead time.
</li>
</ol>
<p>
The result is a data frame with our lead times, the nested raw data, model and R-squared for each lead time.
</p>
<pre class="sourceCode r code-with-copy"><code># make long form to nest
# initialize models data frame
models &lt;- us_lags %&gt;% ungroup %&gt;% 
  pivot_longer(cols = contains("lead"),
               names_to = "lead",
               values_to = "led_deaths") %&gt;% 
  select(date,cases_7day,lead,led_deaths) %&gt;% 
  mutate(lead = as.numeric(str_remove(lead,"deaths_7day_lead"))) %&gt;% 

  nest(data=c(date,cases_7day,led_deaths)) %&gt;% 
  # Run a regression on lagged cases and date vs deaths
  mutate(model = map(data,
                     function(df) 
                       lm(led_deaths~cases_7day+poly(date,2),data = df)))

# Add regression coefficient
# get adjusted r squared
models &lt;- models %&gt;% 
  mutate(adj_r = map(model,function(x) glance(x) %&gt;% 
                       pull(adj.r.squared))
         %&gt;% unlist)
models</code></pre>
<pre><code>## # A tibble: 41 x 4
##     lead data               model  adj_r
##    &lt;dbl&gt; &lt;list&gt;             &lt;list&gt; &lt;dbl&gt;
##  1     0 &lt;tibble [218 x 3]&gt; &lt;lm&gt;   0.164
##  2     1 &lt;tibble [218 x 3]&gt; &lt;lm&gt;   0.187
##  3     2 &lt;tibble [218 x 3]&gt; &lt;lm&gt;   0.212
##  4     3 &lt;tibble [218 x 3]&gt; &lt;lm&gt;   0.241
##  5     4 &lt;tibble [218 x 3]&gt; &lt;lm&gt;   0.272
##  6     5 &lt;tibble [218 x 3]&gt; &lt;lm&gt;   0.307
##  7     6 &lt;tibble [218 x 3]&gt; &lt;lm&gt;   0.343
##  8     7 &lt;tibble [218 x 3]&gt; &lt;lm&gt;   0.383
##  9     8 &lt;tibble [218 x 3]&gt; &lt;lm&gt;   0.424
## 10     9 &lt;tibble [218 x 3]&gt; &lt;lm&gt;   0.467
## # ... with 31 more rows</code></pre>
<p>
To decide the best-fit lead time we choose the model with the highest R-squared.
</p>
<pre class="sourceCode r code-with-copy"><code># Show model fit by lead time
# make predictions using best model
best_fit &lt;- models %&gt;% 
  summarize(adj_r = max(adj_r)) %&gt;% 
  left_join(models,by= "adj_r")

models %&gt;%
  ggplot(aes(lead,adj_r)) + geom_line() +
  labs(subtitle = paste("Best fit lead =",best_fit$lead,"days"),
       title = "Model Fit By Lag Days",
       x = "Lead Time in Days for Deaths",
       caption = "Source: NY Times, Arthur Steinmetz",
       y= "Adjusted R-squared")</code></pre>
<p>
<img src="outsiderdata.netlify.app/posts/2020-12-06-covid-cases-vs-deaths/img/unnamed-chunk-6-1.png" width="672"> We can have some confidence that we are not overfitting the <code>date</code> variable because the significance of the case count remains. With a high enough degree polynomial on the <code>date</code> variable, cases would vanish in importance.
</p>
<pre class="sourceCode r code-with-copy"><code>best_fit$model[[1]] %&gt;% tidy()</code></pre>
<pre><code>## # A tibble: 4 x 5
##   term             estimate  std.error statistic  p.value
##   &lt;chr&gt;               &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
## 1 (Intercept)      436.      38.0           11.5 4.21e-24
## 2 cases_7day         0.0167   0.000993      16.8 5.45e-41
## 3 poly(date, 2)1 -7306.     227.           -32.2 5.87e-84
## 4 poly(date, 2)2  4511.     167.            26.9 1.02e-70</code></pre>
</section>
<section id="make-predictions" class="level1">
<h1>
Make Predictions
</h1>
<p>
The best-fit lead time is 23 days but let’s use <code>predict</code> to see how well our model fits to the actual deaths.
</p>
<pre class="sourceCode r code-with-copy"><code># ------------------------------------------
# see how well our model predicts
# Function to create prediction plot
show_predictions &lt;- function(single_model,n.ahead){
  predicted_deaths = predict(single_model$model[[1]],newdata = us)
  date = seq.Date(from=min(us$date) + n.ahead,to=max(us$date) + n.ahead,by=1)
  display = full_join(us,tibble(date,predicted_deaths))

  gg &lt;- display %&gt;% 
    pivot_longer(cols = where(is.numeric)) %&gt;% 
    filter(name %in% c("deaths_7day","predicted_deaths")) %&gt;% 
    ggplot(aes(date,value,color=name)) + geom_line() +
    labs(title="Actual vs. Predicted Deaths",
         x = "Date", 
         y = "Count",
         caption = "Source: NY Times, Arthur Steinmetz")
  gg
}

show_predictions(best_fit,best_fit$lead)</code></pre>
<p>
<img src="outsiderdata.netlify.app/posts/2020-12-06-covid-cases-vs-deaths/img/unnamed-chunk-8-1.png" width="672">
</p>
<p>
This is a satisfying result, but sadly shows deaths about to spike. This is despite accounting for the improvements in treatment outcomes we’ve accomplished over the past several months. The 23-day lead time model shows a 1.7% mortality rate over the whole length of observations but conditioned on deaths falling steadily over time.
</p>
</section>
<section id="declining-mortality-rate" class="level1">
<h1>
Declining Mortality Rate
</h1>
<p>
Once we’ve settled on the appropriate lag time, we can look at the fatality rate per identified case. This is but one possible measure of fatality rate, certainly not THE fatality rate. Testing rate, positivity rate and others variables will affect this measure. We also assume our best-fit lag is stable over time so take the result with a grain of salt. The takeaway should be how it is declining, not exactly what it is.
</p>
<p>
Early on, only people who were very sick or met strict criteria were tested so, of course, fatality rates (on this metric) were much, much higher. To minimize this we start our measure at the middle of April.
</p>
<p>
Sadly, we see that fatality rates are creeping up again.
</p>
<pre class="sourceCode r code-with-copy"><code>fatality &lt;- best_fit$data[[1]] %&gt;% 
  filter(cases_7day &gt; 0) %&gt;%
  filter(date &gt; as.Date("2020-04-15")) %&gt;%
  mutate(rate = led_deaths/cases_7day)

fatality %&gt;% ggplot(aes(date,rate)) + geom_line() + 
  geom_smooth() +
  labs(x="Date",y="Fatality Rate",
       title = "Fatality Rates are Creeping Up",
       subtitle = "Fatality Rate as a Percentage of Lagged Cases",
       caption = "Source: NY Times, Arthur Steinmetz") +
  scale_y_continuous(labels = scales::percent)</code></pre>
<p>
<img src="outsiderdata.netlify.app/posts/2020-12-06-covid-cases-vs-deaths/img/unnamed-chunk-9-1.png" width="672">
</p>
</section>
<section id="state-level-analysis" class="level1">
<h1>
State-Level Analysis
</h1>
<p>
One problem with the national model is each state saw the arrival of the virus at different times, which suggests there might also be different relationships between cases and deaths. Looking at a few selected states illustrates this.
</p>
<pre class="sourceCode r code-with-copy"><code># ------------------------------------------
# state by state analysis

state_subset &lt;- c("New York","Texas","California","Ohio")

# illustrate selected states
us_states %&gt;% 
  filter(state %in% state_subset) %&gt;% 
  ggplot(aes(date,cases_7day)) + geom_line(color="orange") +
  facet_wrap(~state,scales = "free") +
  theme(legend.position = "none") +
  geom_line(aes(y=deaths_7day*coeff),color="red") +
  scale_y_continuous(labels = scales::comma,
                     name = "Cases",
                     sec.axis = sec_axis(deaths_7day~./coeff,
                                         name="Deaths",
                                         labels = scales::comma)) +
  theme(
    axis.title.y = element_text(color = "orange", size=13),
    axis.title.y.right = element_text(color = "red", size=13)
  ) +
  labs(title =  "U.S. Cases vs. Deaths",
       subtitle = "7-Day Average",
       caption = "Source: NY Times, Arthur Steinmetz",
       x = "Date")</code></pre>
<p>
<img src="outsiderdata.netlify.app/posts/2020-12-06-covid-cases-vs-deaths/img/unnamed-chunk-10-1.png" width="576">
</p>
<p>
In particular we note New York, where the virus arrived early and circulated undetected for weeks. Testing was rare and we did not know much about the course of the disease so the death toll was much worse. Tests were often not conducted until the disease was in advanced stages so we would expect the lag to be shorter.
</p>
<p>
In Texas, the virus arrived later. There it looks like the consequences of the first wave were less dire and the lag was longer.
</p>
</section>
<section id="run-state-models" class="level1">
<h1>
Run State Models
</h1>
<p>
Now we can run the same workflow we used above over the state-by-state data. Our data set is much larger because we have a full set of lags for each state but building our data frame of list columns is just as easy.
</p>
<p>
Looking at the lags by state shows similar results to the national model, on average, as we assume, but the dispersion is large. Early in the pandemic, in New York, cases were diagnosed only for people who were already sick so the lead time before death was much shorter.
</p>
<pre class="sourceCode r code-with-copy"><code># create lags
us_states_lags &lt;- us_states %&gt;%
  # create lags by day
  tk_augment_lags(deaths_7day,.lags = -max_lead:0,.names="auto") %&gt;% 
  {.}
# fix names to remove minus sign
names(us_states_lags) &lt;- names(us_states_lags) %&gt;% str_replace_all("lag-","lead")

# make long form to nest
# initialize models data frame
models_st &lt;- us_states_lags %&gt;% ungroup %&gt;% 
  pivot_longer(cols = contains("lead"),
               names_to = "lead",
               values_to = "led_deaths") %&gt;% 
  select(state,date,cases_7day,lead,led_deaths) %&gt;% 
  mutate(lead = as.numeric(str_remove(lead,"deaths_7day_lead"))) %&gt;% 
  {.}

# make separate tibbles for each regression
models_st &lt;- models_st %&gt;% 
  nest(data=c(date,cases_7day,led_deaths)) %&gt;% 
  arrange(lead)

#Run a linear regression on lagged cases and date vs deaths
models_st &lt;- models_st %&gt;% 
  mutate(model = map(data,
                     function(df) 
                       lm(led_deaths~cases_7day+poly(date,2),data = df)))


# Add regression coefficient
# get adjusted r squared
models_st &lt;- models_st %&gt;% 
  mutate(adj_r = map(model,function(x) glance(x) %&gt;% 
                       pull(adj.r.squared))
         %&gt;% unlist)

models_st %&gt;%
  filter(state %in% state_subset) %&gt;% 
  ggplot(aes(lead,adj_r)) + geom_line() +
  facet_wrap(~state) +
  labs(title = "Best Fit Lead Time",
       caption = "Source: NY Times, Arthur Steinmetz")</code></pre>
<p>
<img src="outsiderdata.netlify.app/posts/2020-12-06-covid-cases-vs-deaths/img/unnamed-chunk-11-1.png" width="576">
</p>
<p>
To see how the fit looks for the data set as a whole we look at a histogram of all the state R-squareds. We see many of the state models have a worse accuracy than the national model.
</p>
<pre class="sourceCode r code-with-copy"><code># best fit lag by state
best_fit_st &lt;- models_st %&gt;% 
  group_by(state) %&gt;% 
  summarize(adj_r = max(adj_r)) %&gt;% 
  left_join(models_st)

best_fit_st %&gt;% ggplot(aes(adj_r)) + 
  geom_histogram(bins = 10,color="white") +
  geom_vline(xintercept = best_fit$adj_r[[1]],color="red") +
  annotate(geom="text",x=0.75,y=18,label="Adj-R in National Model") +
  labs(y = "State Count",
       x="Adjusted R-Squared",
       title = "Goodness of Fit of State Models",
       caption = "Source:NY Times,Arthur Steinmetz")</code></pre>
<p>
<img src="outsiderdata.netlify.app/posts/2020-12-06-covid-cases-vs-deaths/img/unnamed-chunk-12-1.png" width="672">
</p>
<p>
There are vast differences in the best-fit lead times across the states but the distribution is in agreement with our national model.
</p>
<pre class="sourceCode r code-with-copy"><code>best_fit_st %&gt;% ggplot(aes(lead)) + 
  geom_histogram(binwidth = 5,color="white") +
  scale_y_continuous(labels = scales::label_number(accuracy = 1)) +
  geom_vline(xintercept = best_fit$lead[[1]],color="red") +
  annotate(geom="text",x=best_fit$lead[[1]]+7,y=10,label="Lead in National Model") +
  labs(y = "State Count",
    x="Best Fit Model Days from Case to Death",
    title = "COVID-19 Lag Time From Cases to Death",
    caption = "Source:NY Times,Arthur Steinmetz")</code></pre>
<p>
<img src="outsiderdata.netlify.app/posts/2020-12-06-covid-cases-vs-deaths/img/unnamed-chunk-13-1.png" width="672">
</p>
</section>
<section id="validate-with-individual-case-data-from-ohio" class="level1">
<h1>
Validate with Individual Case Data from Ohio
</h1>
<p>
This whole exercise has involved proxying deaths by time and quantity of positive tests. Ideally, we should look at longitudinal data which follows each individual. The state of Ohio provides that so we’ll look at just this one state to provide a reality check on the foregoing analysis. In our proxy model, Ohio shows a best-fit lead time of 31 days, which is much longer than our national-level model.
</p>
<pre class="sourceCode r code-with-copy"><code># ----------------------------------------------------
best_fit_st %&gt;% select(-data,-model) %&gt;% filter(state == "Ohio") %&gt;% kable()</code></pre>

<table class="table">
<thead>
<tr class="header">
<th align="left">
state
</th>
<th align="right">
adj_r
</th>
<th align="right">
lead
</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">
Ohio
</td>
<td align="right">
0.7548416
</td>
<td align="right">
31
</td>
</tr>
</tbody>

</table>
<p>
The caveat here is the NY Times data uses the “case” date which is presumably the date a positive test is recorded. The Ohio data uses “onset” date, which is the date the “illness began.” That is not necessarily the same as the test date.
</p>
<pre class="sourceCode r code-with-copy"><code># source: https://coronavirus.ohio.gov/static/dashboards/COVIDSummaryData.csv
ohio_raw &lt;- read_csv("https://coronavirus.ohio.gov/static/dashboards/COVIDSummaryData.csv", 
                     col_types = cols(`Admission Date` = col_date(format = "%m/%d/%Y"), 
                                      `Date Of Death` = col_date(format = "%m/%d/%Y"), 
                                      `Onset Date` = col_date(format = "%m/%d/%Y")))

# helper function to fix column names to best practice
fix_df_colnames &lt;- function(df){
  names(df)&lt;-names(df) %&gt;% 
    str_replace_all(c(" " = "_" , "," = "" )) %&gt;% 
    tolower()
  return(df)
}

# clean up the data
ohio &lt;- ohio_raw %&gt;% 
  rename(death_count = `Death Due to Illness Count`) %&gt;% 
  filter(County != "Grand Total") %&gt;%
  fix_df_colnames() %&gt;% 
  # data not clean before middle of march
  filter(onset_date &gt;= cutoff_start)</code></pre>
<p>
How comparable are these data sets? Let’s compare the NY Times case count and dates to the Ohio “Illness Onset” dates.
</p>
<pre class="sourceCode r code-with-copy"><code># create rolling average function
mean_roll_7 &lt;- slidify(mean, .period = 7, .align = "right")

comps &lt;- ohio %&gt;% 
  group_by(onset_date) %&gt;% 
  summarise(OH = sum(case_count),.groups = "drop") %&gt;%
  mutate(OH = mean_roll_7(OH)) %&gt;% 
  ungroup() %&gt;% 
  mutate(state = "Ohio") %&gt;% 
  rename(date=onset_date) %&gt;% 
  left_join(us_states,by=c("date","state")) %&gt;% 
  transmute(date,OH,NYTimes = cases_7day)

comps %&gt;% 
  pivot_longer(c("OH","NYTimes"),names_to = "source",values_to = "count") %&gt;%  
  ggplot(aes(date,count,color=source)) + geom_line() +
  labs(title =  "Case Counts from Different Sources",
       caption = "Source: State of Ohio, NY Times",
       subtitle = "NY Times and State of Ohio",
       x = "Date",
       y = "Daily Case Count (7-day Rolling Average)")</code></pre>
<p>
<img src="outsiderdata.netlify.app/posts/2020-12-06-covid-cases-vs-deaths/img/unnamed-chunk-16-1.png" width="672"> We clearly see the numbers line up almost exactly but the Ohio data runs about 4 days ahead of the NY Times data.
</p>
<p>
For each individual death, we subtract the onset date from the death date. Then we aggregate the county-level data to statewide and daily data to weekly. Then take the weekly mean of deaths.
</p>
<pre class="sourceCode r code-with-copy"><code># aggregate the data to weekly
ohio &lt;- ohio %&gt;% 
  mutate(onset_to_death = as.numeric(date_of_death - onset_date),
         onset_year = year(onset_date),
         onset_week = epiweek(onset_date))


onset_to_death &lt;- ohio %&gt;%
  filter(death_count &gt; 0) %&gt;% 
  group_by(onset_year,onset_week) %&gt;%
  summarise(death_count_sum = sum(death_count),
            mean_onset_to_death = weighted.mean(onset_to_death,
                                                death_count,
                                                na.rm = TRUE)) %&gt;%
  mutate(date=as.Date(paste(onset_year,onset_week,1),"%Y %U %u")) %&gt;%
  {.}

onset_to_death %&gt;% ggplot(aes(date,death_count_sum)) + geom_col() +
    labs(title =  "Ohio Weekly Deaths",
       caption = "Source: State of Ohio, Arthur Steinmetz",
       subtitle = "Based on Illness Onset Date",
       x = "Date of Illness Onset",
       y = "Deaths")</code></pre>
<p>
<img src="outsiderdata.netlify.app/posts/2020-12-06-covid-cases-vs-deaths/img/unnamed-chunk-17-1.png" width="672"> When we measure the average lag, we find that it has been fairly stable over time in Ohio. Unfortunately, it differs substantially from our proxy model using untracked cases.
</p>
<pre class="sourceCode r code-with-copy"><code># helper function to annotate plots 
pos_index &lt;- function(index_vec,fraction){
  return(index_vec[round(length(index_vec)*fraction)])
}

avg_lag &lt;- round(mean(onset_to_death$mean_onset_to_death))

onset_to_death %&gt;% ggplot(aes(date,mean_onset_to_death)) + 
  geom_col() +
  geom_hline(yintercept = avg_lag) +
  annotate(geom="text",
           label=paste("Average Lag =",round(avg_lag)),
           y=20,x=pos_index(onset_to_death$date,.8)) +
  labs(x = "Onset Date",
       y = "Mean Onset to Death",
       title = "Ohio Days from Illness Onset Until Death Over Time",
       caption = "Source: State of Ohio, Arthur Steinmetz",
       subtitle = paste("Average =",
                     avg_lag,"Days"))</code></pre>
<p>
<img src="outsiderdata.netlify.app/posts/2020-12-06-covid-cases-vs-deaths/img/unnamed-chunk-18-1.png" width="672"> Note the drop off at the end of the date range. This is because we don’t yet know the outcome of the most recently recorded cases. Generally, while we have been successful in lowering the fatality rate of this disease, the duration from onset to death for those cases which are fatal has not changed much, at least in Ohio.
</p>
<p>
Since we have the actual number of deaths associated with every onset date we can calculate the “true” fatality rate. As mentioned, the fatality rate of the more recent cases is not yet known. Also the data is too sparse at the front of the series so we cut off the head and the tail of the data.
</p>
<pre class="sourceCode r code-with-copy"><code>ohio_fatality_rate &lt;- ohio %&gt;% 
  group_by(onset_date) %&gt;% 
  summarize(case_count = sum(case_count),
            death_count = sum(death_count),.groups="drop") %&gt;% 
  mutate(fatality_rate = death_count/case_count) %&gt;% 
  mutate(fatality_rate_7day = mean_roll_7(fatality_rate)) %&gt;% 
# filter out most recent cases we we don't know outcome yet
  filter(onset_date &lt; max(onset_date)-30) 

ohio_fatality_rate %&gt;% 
  filter(onset_date &gt; as.Date("2020-04-15")) %&gt;% 
  ggplot(aes(onset_date,fatality_rate_7day)) + geom_line() +
  geom_smooth() +
  labs(x="Illness Onset Date",y="Ohio Fatality Rate",
       caption = "Source: State of Ohio, Arthur Steinmetz",
       title = "Ohio Fatality Rate as a Percentage of Tracked Cases") +
  scale_y_continuous(labels = scales::percent,breaks = seq(0,0.12,by=.01))</code></pre>
<p>
<img src="outsiderdata.netlify.app/posts/2020-12-06-covid-cases-vs-deaths/img/unnamed-chunk-19-1.png" width="672">
</p>
<p>
The fatality rate in Ohio seems to have been worse than our national model but it is coming down. Again, this result comes from a different methodology than our proxy model.
</p>
</section>
<section id="conclusion" class="level1">
<h1>
Conclusion
</h1>
<p>
Among the vexing aspects of this terrible pandemic is that we don’t know what the gold standard is for treatment and prevention. We are learning as we go. The good news is we ARE learning. For a data analyst the challenge is the evolving relationship of of all of the disparate data. Here we have gotten some insight into the duration between a positive test and mortality. We can’t have high confidence that our proxy model using aggregate cases is strictly accurate because the longitudinal data from Ohio shows a different lag. We have clearly seen that mortality has been declining but our model suggests that death will nonetheless surge along with the autumn surge in cases.
</p>
<p>
What are the further avenues for modeling? There is a wealth of data around behavior and demographics with this disease that we don’t fully understand yet. On the analytics side, we might get more sophisticated with our modeling. We have only scratched the surface of the <code>tidymodels</code> framework and we might apply fancier predictive models than linear regression. Is the drop in the fatality rate we saw early in the pandemic real? Only people who were actually sick got tested in the early days. Now, many positive tests are from asymptomatic people. Finally, the disagreement between the case proxy model and the longitudinal data in Ohio shows there is more work to be done.
</p>
</section>



 ]]></description>
  <category>ggplot2</category>
  <category>tidymodels</category>
  <category>tidyverse</category>
  <category>COVID</category>
  <guid>outsiderdata.netlify.app/posts/2020-12-06-covid-cases-vs-deaths/2020-12-06-covid-cases-vs-deaths.html</guid>
  <pubDate>Sun, 06 Dec 2020 05:00:00 GMT</pubDate>
  <media:content url="outsiderdata.netlify.app/posts/2020-12-06-covid-cases-vs-deaths/img/unnamed-chunk-2-1.png" medium="image" type="image/png" height="103" width="144"/>
</item>
<item>
  <title>What Do The Ramones Want?</title>
  <dc:creator>Art Steinmetz</dc:creator>
  <link>outsiderdata.netlify.app/posts/2020-01-15-what-do-the-ramones-want/2020-01-15-what-do-the-ramones-want.html</link>
  <description><![CDATA[ 



<p>
Recently I saw a tweet that shared this hilarious poster of Ramones “wants”. Very cool, but how accurate is it? I asked the graphic designer and he says he took some artistic license, as he should! You may accuse me of being that pedantic “Comic Book Guy” from “The Simpsons” but, when I saw it, I immediately wondered how I could tally these Ramones lyrics myself or, rather, get R to do it for me. The <code>tidytext</code> mining package makes short work of the project, as we’ll see.
</p>
<p><img src="outsiderdata.netlify.app/posts/2020-01-15-what-do-the-ramones-want/img/Wants4thEdition.jpg" alt="Ramones Want What?"></p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Dan Gneiding (aka Grayhood) created the poster above. You can buy it <a href="http://grayhood.com/shop/ramones-vs-misfits-1-2-3-4th-edition">here</a></p>
</div>
</div>
<iframe src="https://giphy.com/embed/26tk0Emxz61hdKEog" width="480" height="360" frameborder="0" class="giphy-embed">
</iframe>
<section id="why-the-ramones" class="level2">
<h2 class="anchored" data-anchor-id="why-the-ramones">
Why the RAMONES?
</h2>
<p>
The Ramones hold a special place in my heart. As a college student in central Ohio, in the late 70s, my frat brothers and I were huge fans. We were completely ridiculous of course. Preppy nerds bobbing to “Beat on the Brat” The Sigma Chis thought we were idiots (Lynrd Skynrd? Come on! History has judged). I never saw the Ramones at CBGBs but when we heard they were coming to a cowboy bar on notorious High St.&nbsp;across from Ohio State, we were thrilled. I blew off studying for a Poly-Sci mid-term the next day. I got my worst college grade ever but it was totally worth it. I said my future self would thank me and I was right!
</p>
<p>
Without any further adieu, hey, ho, let’s go!
</p>
</section>
<section id="load-packages" class="level2">
<h2 class="anchored" data-anchor-id="load-packages">
Load Packages
</h2>
<p>
First, load packages.
</p>
<pre class="sourceCode r code-with-copy"><code>library(tidyverse)
library(tidytext)
library(rvest)
library(reshape2)
library(wordcloud)
library(scales)
library(genius)
library(ggthemr)
library(ggrepel)

ggthemr("earth",type="outer")</code></pre>
</section>
<section id="get-lyrics-from-genius-api" class="level2">
<h2 class="anchored" data-anchor-id="get-lyrics-from-genius-api">
Get Lyrics from Genius API
</h2>
<p>
Have you ever spent an enormous amount of time on something, only to discover there was a much simpler way? Yes, yes you have. For this project we need a source of Ramones lyrics. Originally, I built a very finicky web scraping routine to get lyrics from a site I commonly use in my browser. I coaxed it to get all of the lyrics but I didn’t want to share it in this post because you would likely not be able to get it to work smoothly. Months passed then it occurred to me to Google “lyrics api” and, “viola!”, I found <a href="http://genius.com" class="uri">http://genius.com</a> and the genius R package by Josiah Parry, available on CRAN. Access to the lyric API does require a free application access token. You can <a href="https://docs.genius.com/#/getting-started-h1">generate one here</a>. I will leave installing the token called GENIUS_API_TOKEN into your R environment as an exercise for the reader. There are numerous tutorials on this subject around.
</p>
<p>
As always, we will be working in the <code>tidyverse</code> veracular. First we build a data frame of album names and the year of release. This gets fed into a single function <code>genius::add_genius</code> which returns all the lyrics. I’m embarressed to think about the tangled mess of web scraping code I was previously using.
</p>
<p>
As usual, we check to see if the file with all the downloaded data is already available so, as we iterate versions of our project, we don’t hit the API over and over.
</p>
<pre class="sourceCode r code-with-copy"><code>#make sure you have a Genius API token
# my token is in the .Reviron file

# All the studio albums
ramones_albums &lt;- tribble(
  ~album, ~year,
  "Ramones", 1976,
  "Leave Home", 1977,
  "Rocket To Russia", 1977,
  "Road To Ruin", 1978,
  "End Of The Century", 1980,
  "Pleasant Dreams", 1981,
  "Subterranean Jungle", 1983,
  "Too Tough To Die", 1984,
  "Animal Boy", 1986,
  "Halfway To Sanity",1987,
  "Brain Drain",1989,
  "Mondo Bizarro",1992,
  "Acid Eaters",1993,
  "¡Adios Amigos!",1995
)
artist_albums &lt;- ramones_albums %&gt;% 
  mutate(artist="Ramones") %&gt;% 
  select(artist,album) %&gt;%
  {.}

if (file.exists("data/ramones_lyrics_genius.rdata")){
  load("data/ramones_lyrics_genius.rdata")
} else {
  ramones_lyrics_genius &lt;- genius::add_genius(artist_albums,artist,album)
  save(ramones_lyrics_genius,file="data/ramones_lyrics_genius.rdata")
}</code></pre>
</section>
<section id="put-lyics-in-tidytext-form" class="level2">
<h2 class="anchored" data-anchor-id="put-lyics-in-tidytext-form">
Put Lyics in Tidytext Form
</h2>
<p>
Most projects require a huge amount of data wrangling before we can get any real analysis done. This project is pretty clean. We are already nearly good to go. Further, <code>tidytext</code> makes the remaining manipulation of the data soooo easy! To wit, let’s tokenize the data into individual words.
</p>
<pre class="sourceCode r code-with-copy"><code>ramones_lyrics &lt;- ramones_lyrics_genius
#make factor to keep albums in order of issue date
ramones_lyrics$album &lt;- as_factor(ramones_lyrics$album)
ramones_albums$album &lt;- as_factor(ramones_albums$album)
ramones_lyrics &lt;- right_join(ramones_lyrics,ramones_albums,by="album")
lyric_words &lt;- ramones_lyrics  %&gt;% 
  unnest_tokens(word,lyric) %&gt;%
  rename(song_name=track_title)</code></pre>
<p>
See, I said it was easy.
</p>
</section>
<section id="how-needy-are-the-ramones" class="level2">
<h2 class="anchored" data-anchor-id="how-needy-are-the-ramones">
How Needy Are The Ramones?
</h2>
<p>
Out of 193 songs on all their studio albums, 16 mention wanting or not wanting in the title. “I Wanna” songs are a thing with the Ramones.
</p>
<pre class="sourceCode r code-with-copy"><code>want_phrases &lt;- "Wanna|Want"
ramones_lyrics %&gt;% 
  select(album, track_title) %&gt;% 
  distinct() %&gt;%
  filter(str_detect(track_title,want_phrases)) %&gt;% 
  {.}</code></pre>
<pre><code>## # A tibble: 16 x 2
##    album             track_title                                    
##    &lt;fct&gt;             &lt;chr&gt;                                          
##  1 Ramones           I Wanna Be Your Boyfriend                      
##  2 Ramones           Now I Wanna Sniff Some Glue                    
##  3 Ramones           I Don't Wanna Go Down to the Basement          
##  4 Ramones           I Don't Wanna Walk Around with You             
##  5 Leave Home        Now I Wanna Be a Good Boy                      
##  6 Rocket To Russia  Do You Wanna Dance?                            
##  7 Rocket To Russia  I Wanna Be Well                                
##  8 Road To Ruin      I Just Want To Have Something To Do            
##  9 Road To Ruin      I Wanted Everything                            
## 10 Road To Ruin      I Don't Want You                               
## 11 Road To Ruin      I Wanna Be Sedated                             
## 12 Road To Ruin      I Want You Around (Ed Stasium Version)         
## 13 Pleasant Dreams   We Want the Airwaves                           
## 14 Halfway To Sanity I Wanna Live                                   
## 15 Brain Drain       Merry Christmas (I Don't Want To Fight Tonight)
## 16 ¡Adios Amigos!    I Don't Want to Grow Up</code></pre>
</section>
<section id="do-some-sentiment-analysis" class="level2">
<h2 class="anchored" data-anchor-id="do-some-sentiment-analysis">
Do Some Sentiment Analysis
</h2>
<p>
Before we look at the what the Ramones want we might as well run the, now routine, sentiment analysis you may have learned about from Julia Silge and David Robinson <a href="https://cran.r-project.org/web/packages/tidytext/vignettes/tidytext.html">here</a>. The Ramones are no Jane Austen but, hey, they have feelings, ya know? SAD NOTE: They “had” feelings. All the original four are dead.
</p>
<p>
To start our sentiment analysis let’s pull out stop words that don’t provide much context and label all the words in the “bing” sentiment database as either positive or negative.
</p>
<pre class="sourceCode r code-with-copy"><code>lyric_words_cleaned &lt;- lyric_words %&gt;% anti_join(get_stopwords(),by="word")

#quick sentiment analysis
positive &lt;- get_sentiments("bing") %&gt;%
  filter(sentiment == "positive")

negative &lt;- get_sentiments("bing") %&gt;%
  filter(sentiment == "negative")

lyric_words_cleaned %&gt;%
  semi_join(positive,by="word") %&gt;%
  group_by(song_name) %&gt;% 
  count(word) %&gt;% 
  group_by(song_name) %&gt;% 
  tally(sort = TRUE,name="Happy Words")</code></pre>
<pre><code>## # A tibble: 162 x 2
##    song_name                   `Happy Words`
##    &lt;chr&gt;                               &lt;int&gt;
##  1 The Crusher                            10
##  2 It's Gonna Be Alright                   9
##  3 Palisades Park                          9
##  4 Too Tough to Die                        9
##  5 Censorshit                              8
##  6 I Don't Want to Grow Up                 8
##  7 In the Park                             8
##  8 My Back Pages                           8
##  9 Gimme Gimme Shock Treatment             7
## 10 Glad to See You Go                      7
## # ... with 152 more rows</code></pre>
<pre class="sourceCode r code-with-copy"><code>lyric_words_cleaned %&gt;%
  semi_join(negative,by="word") %&gt;%
  group_by(song_name) %&gt;% 
  count(word) %&gt;% 
  group_by(song_name) %&gt;% 
  tally(sort = TRUE,name="Sad Words")</code></pre>
<pre><code>## # A tibble: 156 x 2
##    song_name                       `Sad Words`
##    &lt;chr&gt;                                 &lt;int&gt;
##  1 I'm Not Afraid of Life                   21
##  2 Endless Vacation                         17
##  3 Don't Bust My Chops                      16
##  4 Love Kills                               15
##  5 Wart Hog                                 13
##  6 My Back Pages                            12
##  7 Cretin Family                            10
##  8 Something to Believe In                  10
##  9 Anxiety                                   9
## 10 Howling at the Moon (Sha-La-La)           9
## # ... with 146 more rows</code></pre>
<p>
Now we change the sign of the count of negative words so we can get the net balance of happy vs.&nbsp;sad words.
</p>
<pre class="sourceCode r code-with-copy"><code>lyric_words_cleaned %&gt;%
  inner_join(get_sentiments("bing"),by="word") %&gt;%
  group_by(song_name) %&gt;% 
  count(sentiment,sort=TRUE) %&gt;% 
  mutate(n = ifelse(sentiment == "negative", -n, n)) %&gt;%
  group_by(song_name) %&gt;% 
  summarise(net_sentiment=sum(n)) %&gt;% 
  filter(abs(net_sentiment) &gt; 10) %&gt;%
  mutate(song_name = reorder(song_name, net_sentiment)) %&gt;%
  mutate(sentiment=ifelse(net_sentiment&lt;0,"Negative","Positive")) %&gt;% 
  ggplot(aes(song_name, net_sentiment, fill = sentiment)) +
  geom_col() +
  coord_flip() +
  labs(title="How Happy are RAMONES Songs?",
       y = "Very Sad &lt;---   ---&gt; Very Happy",
       x= "") +
  scale_fill_manual(values = c("red","darkgrey"))+
  theme(axis.text.y =  element_text(size=7,hjust=1)) </code></pre>
<p>
<img src="outsiderdata.netlify.app/posts/2020-01-15-what-do-the-ramones-want/img/unnamed-chunk-6-1.png" width="672">
</p>
</section>
<section id="sentiment-over-time" class="level2">
<h2 class="anchored" data-anchor-id="sentiment-over-time">
Sentiment Over Time
</h2>
<p>
The average sentiment over the whole lyric corpus is about evenly split between positive and negative words but if we look at sentiment by album we see a gyrating trend with an intersting dip in their middle years.
</p>
<pre class="sourceCode r code-with-copy"><code>lyric_words_cleaned %&gt;%
  inner_join(get_sentiments("bing"),by="word") %&gt;%
  group_by(album, year) %&gt;% 
  count(sentiment,sort=TRUE) %&gt;% 
  arrange(album) %&gt;% 
  pivot_wider(values_from = n,names_from = sentiment) %&gt;% 
  mutate(fraction_happy = positive/(negative+positive)) %&gt;%
  ggplot(aes(year,fraction_happy)) + geom_line(color="red") + geom_point(color="red") +
  labs(title = "RAMONES Mood Over Time",
       y= "Fraction of Happy Words",
       x= "Album Release Year") + 
    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
    geom_text_repel(aes(label=album),
                    color="white",
                    segment.color = "white")</code></pre>
<p>
<img src="outsiderdata.netlify.app/posts/2020-01-15-what-do-the-ramones-want/img/unnamed-chunk-7-1.png" width="672">
</p>
<p>
We can generate word clouds for any album. Their “happiest” album is “Road to Ruin.”
</p>
<pre class="sourceCode r code-with-copy"><code>{par(bg="black")
  lyric_words_cleaned %&gt;%
    filter(album == "Road To Ruin") %&gt;% 
    inner_join(get_sentiments("bing"),by="word") %&gt;%
    count(word, sentiment, sort = TRUE) %&gt;%
    acast(word ~ sentiment, value.var = "n", fill = 0) %&gt;%
    #  comparison.cloud(colors = c("#F8766D", "#00BFC4"),
    #                   max.words = 100)
    comparison.cloud(colors = c("red", "grey60"),
                     max.words = 100,
                     title.bg.colors="grey60")
    text(x=1.1,y=0.5,"RAMONES",col="red",cex=4,srt=270)
    text(x=-0.1,y=0.5,"Road To Ruin",col="grey60",cex=4,srt=90)
}</code></pre>
<p>
<img src="outsiderdata.netlify.app/posts/2020-01-15-what-do-the-ramones-want/img/unnamed-chunk-8-1.png" width="672">
</p>
<p>
… and their angriest, “Animal Boy.” You start to think there is something to this sentiment analysis stuff when you read the opening of this album’s review at <a href="http://allmusic.com" class="uri">http://allmusic.com</a>:
</p>
<blockquote class="blockquote">
<p>
<em>Animal Boy wasn’t a very happy record for the Ramones. Since the release of Too Tough to Die (a slight return to form) nearly two years earlier, the band’s fortunes had gone from bad to worse; interest in the band kept dwindling with every release and the “bruthas” were constantly at each other’s throat.</em>
</p>
</blockquote>
<pre class="sourceCode r code-with-copy"><code>{par(bg="black")
  lyric_words_cleaned %&gt;%
    filter(album == "Animal Boy") %&gt;% 
    inner_join(get_sentiments("bing"),by="word") %&gt;%
    count(word, sentiment, sort = TRUE) %&gt;%
    acast(word ~ sentiment, value.var = "n", fill = 0) %&gt;%
    #  comparison.cloud(colors = c("#F8766D", "#00BFC4"),
    #                   max.words = 100)
    comparison.cloud(colors = c("red", "grey60"),
                     max.words = 100,
                     title.bg.colors="grey60")
    text(x=1.1,y=0.5,"RAMONES",col="red",cex=4,srt=270)
    text(x=-0.1,y=0.5,"Animal Boy",col="grey60",cex=4,srt=90)
}</code></pre>
<p>
<img src="outsiderdata.netlify.app/posts/2020-01-15-what-do-the-ramones-want/img/unnamed-chunk-9-1.png" width="672">
</p>
</section>
<section id="what-do-the-ramones-want-and-not-want" class="level2">
<h2 class="anchored" data-anchor-id="what-do-the-ramones-want-and-not-want">
What do the RAMONES want… and not want?
</h2>
<p>
Now lets find what the Ramones Want. An n-gram is simply a cluster of words of length n.&nbsp;Let’s look at the most common n-grams, which would include the phrases like “I want” and “I wanna.”
</p>
<p>
Start with shortest n-gram that is a complete thought and work up to longer phrases. We take the the shortest phrase that makes sense unless appending more words doesn’t change the frequency. Then we take the longer phrase. For instance if “I wanna steal some money” and “I wanna steal from the rich” both exist we take “I wanna steal” since it would have a higher frequency than either longer phrase. In this case, the only phrase starting with “I wanna steal” is “I wanna steal from the rich” so we use that.
</p>
<pre class="sourceCode r code-with-copy"><code>want_phrases &lt;- "^(i wanna |i want |we want |we wanna |i wanted |i just want |i just wanna )"

get_ngrams &lt;- function(lyrics,n,prefixes=""){
  min_instance = 0
  lyric_ngram &lt;- lyrics %&gt;% 
    unnest_tokens(ngram,lyric,token = "ngrams",n=n) %&gt;% 
    group_by(ngram) %&gt;% 
    filter(str_detect(ngram,prefixes)) %&gt;% 
    count() %&gt;% 
    arrange(desc(n)) %&gt;% 
    filter(n&gt;min_instance) %&gt;% 
    mutate(want=str_remove(ngram,prefixes)) %&gt;% 
   rowid_to_column()
  return(lyric_ngram)
  
}

want &lt;- ramones_lyrics %&gt;% get_ngrams(5,want_phrases)
want</code></pre>
<pre><code>## # A tibble: 43 x 4
## # Groups:   ngram [43]
##    rowid ngram                         n want             
##    &lt;int&gt; &lt;chr&gt;                     &lt;int&gt; &lt;chr&gt;            
##  1     1 i want i want i              14 i want i         
##  2     2 i want to be your            13 to be your       
##  3     3 i just want to walk           7 to walk          
##  4     4 i just want to have           6 to have          
##  5     5 i just want to be             4 to be            
##  6     6 i wanna be your boyfriend     4 be your boyfriend
##  7     7 i want to live my             4 to live my       
##  8     8 i want to run away            4 to run away      
##  9     9 i want to be a                3 to be a          
## 10    10 i want you by my              3 you by my        
## # ... with 33 more rows</code></pre>
<p>
What a human needs to do is decide which phrases are complete thoughts. We manually select the row numbers to build our ultimate table.
</p>
<p>
Remember what I said before about data wrangling? Well, sure, getting the words was easy. Determining meaningful phrases not (for a computer). If this was Spotify, our AI could figure these out, but this is not Spotify. This is an iterative process of manually inspecting tables of ever-longer n-grams and noting which rows have complete thoughts until we don’t see any sensible new phrases. We run through twice, first for “want” then “don’t want.” We flip the sign on the count of “don’t wants” to negative. I won’t bore you with every iteration so let’s skip ahead. Think of this as the cinematic training montage.
</p>
<pre class="sourceCode r code-with-copy"><code># WANT
# make "wanna" in to "want to" which also frequently appears so we get a good count.
ramones_lyrics &lt;- ramones_lyrics %&gt;% mutate(lyric=str_replace_all(lyric,"wanna","want to"))
do_want &lt;- tibble()
all_wants &lt;- tibble() # for debugging
# why make the code below a function, if we only call it once?
# Since we cumulatively modify all_wants each step is dependent on the prior one executing first
# this organizes the code into a block that tells future self to execute as a block
build_wants &lt;- function(all_wants) {
  want_phrases &lt;- "^(i wanna |i want |we want |we wanna |i wanted |i just want |i just wanna )"
  #select the 3-gram phrases that are complete thoughts using manual inspection
  want &lt;- ramones_lyrics %&gt;% get_ngrams(3,want_phrases)
  # visually inspect the want variable and select which lines to add to all_wants
  # pause after each instance of get_ngrams to do this.
  all_wants &lt;- bind_rows(all_wants,want[c(2,8,11,13),])
  # move to the 4-gram phrases, etc
  want &lt;- ramones_lyrics %&gt;% get_ngrams(4,want_phrases)
  all_wants &lt;- bind_rows(all_wants,want[c(5,6,9,13,14,17,24,28,30,31,37),])
  want &lt;- ramones_lyrics %&gt;% get_ngrams(5,want_phrases)
  all_wants &lt;- bind_rows(all_wants,want[c(3,4,6,9,21,22),])
  want &lt;- ramones_lyrics %&gt;% get_ngrams(6,want_phrases)
  all_wants &lt;- bind_rows(all_wants,want[c(1,11,12,22,25,28),])
  want &lt;- ramones_lyrics %&gt;% get_ngrams(7,want_phrases)
  all_wants &lt;- bind_rows(all_wants,want[c(5,6,7,9,10,12,21),])
  want &lt;- ramones_lyrics %&gt;% get_ngrams(8,want_phrases)
  all_wants &lt;- bind_rows(all_wants,want[c(7,3),])
  return (all_wants)
}

do_want &lt;- build_wants(do_want)
do_want &lt;- do_want %&gt;% 
  mutate(want=str_to_title(want)) %&gt;% 
  group_by(want) %&gt;% 
  summarise(n=sum(n)) %&gt;% 
  arrange(desc(n))

# DONT'T WANT
dont_want &lt;- tibble()
all_wants &lt;- tibble() # for debugging only
ramones_lyrics &lt;- ramones_lyrics %&gt;% mutate(lyric=str_replace_all(lyric,"wanna","want to"))
want_phrases &lt;- "^(i don't want |we don't want |i didn't want )"
build_dont_wants &lt;- function(all_wants) {
  want &lt;- ramones_lyrics %&gt;% get_ngrams(4,want_phrases)
  all_wants &lt;- bind_rows(all_wants,want[c(2),])
  want &lt;- ramones_lyrics %&gt;% get_ngrams(5,want_phrases)
  all_wants &lt;- bind_rows(all_wants,want[c(3,5,6,7,9,11,15),])
  want &lt;- ramones_lyrics %&gt;% get_ngrams(6,want_phrases)
  all_wants &lt;- bind_rows(all_wants,want[c(1,7),])
  want &lt;- ramones_lyrics %&gt;% get_ngrams(7,want_phrases)
  all_wants &lt;- bind_rows(all_wants,want[c(2,17),])
  want &lt;- ramones_lyrics %&gt;% get_ngrams(8,want_phrases)
  all_wants &lt;- bind_rows(all_wants,want[c(7,8,9,16),])
  want &lt;- ramones_lyrics %&gt;% get_ngrams(9,want_phrases)
  all_wants &lt;- bind_rows(all_wants,want[c(3,10,12),])
  want &lt;- ramones_lyrics %&gt;% get_ngrams(10,want_phrases)
  #there it is - Pet Sematary!
  all_wants &lt;- bind_rows(all_wants,want[c(1),])
}
dont_want &lt;- build_dont_wants(dont_want)
dont_want &lt;- dont_want %&gt;%
  mutate(n = -n) %&gt;% 
  mutate(want=str_to_title(want)) %&gt;% 
  group_by(want) %&gt;%
  summarise(n=sum(n)) %&gt;% 
  arrange(n)</code></pre>
<p>
Finally we put it all together to get what we’re after.
</p>
<pre class="sourceCode r code-with-copy"><code>ultimate_want &lt;- bind_rows(do_want,dont_want) %&gt;% 
  group_by(want) %&gt;%
  summarise(n=sum(n)) %&gt;%   
  mutate(Sentiment = ifelse(n &gt; 0,"Want","Don't Want")) %&gt;% 
  arrange(n) %&gt;% 
  {.}

p &lt;- ultimate_want %&gt;% mutate(want=reorder(want,n)) %&gt;% 
  filter(abs(n) &gt; 1) %&gt;% 
  ggplot(aes(want,n,fill=Sentiment)) + geom_col()+coord_flip()+
  labs(title="What Do The RAMONES Want?",
       y="How Much Do The RAMONES Want It?",
       x="")
p + 
  scale_fill_manual(values = c("red","darkgrey"))+
  theme(axis.text.y =  element_text(size=7,hjust=1)) </code></pre>
<p>
<img src="outsiderdata.netlify.app/posts/2020-01-15-what-do-the-ramones-want/img/unnamed-chunk-12-1.png" width="672">
</p>
</section>
<section id="bringing-it-full-circle" class="level2">
<h2 class="anchored" data-anchor-id="bringing-it-full-circle">
Bringing It Full Circle
</h2>
<p>
Sometimes, late at night, after everyone else is asleep, I hide under the covers, open my laptop and look at… pie charts. <a href="https://www.edwardtufte.com/bboard/q-and-a-fetch-msg?msg_id=00018S ">Ed Tufte</a> says I will go blind if I keep doing it. Still, for the sake of bringing this full circle (ahem) back to the chart that inspired it, let’s make a version of Grayhood’s poster with our data. So it’s not a complete mess, we lump any phrases that occur less than 4 times in “Other.” That takes some of the fun out of things since we lose memorable phrases like “I wanna sniff some glue” which the poster above includes. This is data science, not art. It’s not supposed to be fun! While I use ggplot2 pretty much exclusively, the base R <code>pie</code> plot produces pretty clean results that approximate the style of the poster with no embellishment.
</p>
<pre class="sourceCode r code-with-copy"><code>collapsed_want &lt;- ultimate_want %&gt;%
  filter(Sentiment=="Want") %&gt;%
  mutate(want = ifelse(n&lt;4,"Other",want)) %&gt;%
  group_by(want) %&gt;% 
  summarise(n=sum(n)) %&gt;% 
  arrange(desc(n)) %&gt;% 
  {.}

 with(collapsed_want,
      pie(n, 
          labels=paste0(as.character(want), " ", n, "%"),
          col=c("brown","red","black","darkblue","pink","purple"),
          radius=1,
          density=30,
          bg="sienna",
          main="The Ramones Want..."))</code></pre>
<p>
<img src="outsiderdata.netlify.app/posts/2020-01-15-what-do-the-ramones-want/img/unnamed-chunk-13-1.png" width="672">
</p>
<pre class="sourceCode r code-with-copy"><code>collapsed_want &lt;- ultimate_want %&gt;%
  filter(Sentiment=="Don't Want") %&gt;%
  mutate(n = -n) %&gt;% 
  mutate(want = ifelse(n&lt;2,"Other",want)) %&gt;%
  group_by(want) %&gt;% 
  summarise(n=sum(n)) %&gt;% 
  arrange(desc(n)) %&gt;% 
  {.}

 with(collapsed_want,
      pie(n, 
          labels=paste0(as.character(want), " ", n, "%"),
          col=c("brown","red","black","darkblue","pink","purple"),
          radius=1,
          density=30,
          bg="sienna",
          main="The RAMONES Don't Want..."))</code></pre>
<p>
<img src="outsiderdata.netlify.app/posts/2020-01-15-what-do-the-ramones-want/img/unnamed-chunk-14-1.png" width="672">
</p>
<p>
It must be comforting to know the Ramones want you more than anything but they aren’t going down to the basement with you. Okay, so maybe this was a little fun. Thanks for reading!
</p>
</section>



 ]]></description>
  <category>ggplot2</category>
  <category>tidytext</category>
  <category>music</category>
  <guid>outsiderdata.netlify.app/posts/2020-01-15-what-do-the-ramones-want/2020-01-15-what-do-the-ramones-want.html</guid>
  <pubDate>Wed, 15 Jan 2020 05:00:00 GMT</pubDate>
  <media:content url="outsiderdata.netlify.app/posts/2020-01-15-what-do-the-ramones-want/img/Wants4thEdition.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>State Taxes: It’s not just about Income</title>
  <dc:creator>Art Steinmetz</dc:creator>
  <link>outsiderdata.netlify.app/posts/2019-12-08-state-taxes-it-s-not-just-about-income/2019-12-08-state-taxes-it-s-not-just-about-income.html</link>
  <description><![CDATA[ 



<section id="which-states-impose-the-most-tax-pain" class="level2">
<h2 class="anchored" data-anchor-id="which-states-impose-the-most-tax-pain">Which States Impose the Most “Tax Pain?”</h2>
<p>Much of the discussion around tax burdens focuses on income taxes but, at the state level, that leaves out two other big sources of tax liability, sales and property taxes. Here we’ll quickly look at the interplay of all three taxes in a graphical way. This can inform our thinking about how attractive it is to live in each state and on public policy questions involving tax fairness. The <code>plotly</code> package lets us easily create an interactive 3D scatter plot that is uniquely useful to visualize this.</p>
<p>Sales taxes vary greatly by state but, for lower income people, might be the biggest tax burden. Indeed, since low-income families spend a larger fraction of their income, these taxes are “regressive” since the relative burden grows as income falls. Income taxes are typically “progressive” since, in most states, the rate grows with income levels. Property taxes aren’t directly levied on renters but the landlords pass the tax through via higher rents, so everyone pays. Let’s take a quick look at how tax rates vary by state and category.</p>
<p>The tax data was found in three different places:</p>
<ul>
<li>Income tax rates from https://taxfoundation.org/state-individual-income-tax-rates-brackets-2019/</li>
<li>Property tax Rates from https://wallethub.com/edu/states-with-the-highest-and-lowest-property-taxes/11585/</li>
<li>Sales Tax Rates https://www.salestaxinstitute.com/resources/rates</li>
</ul>
<p>I make some choices in how to present the data. First of all, I use the top marginal rates, so this represents the “worst-case” tax burden. It should be representative of the overall tax structure and useful to compare across states. Next, I add average municipal income taxes computed by the Tax Foundation for each state to the state income tax rate. If you live in New York City, this will substantially understate your tax burden and overstate it elsewhere. Some municipalities levy sales taxes as well but I do NOT include these because they vary so widely and we don’t have all day. Also, municipalities love to tax people who can’t vote, like out of towners, with hotel and rental car taxes. These would not affect your view of where to live. How about excise taxes on gasoline, cigarettes, etc? Not included.</p>
<p>I already combined the data from each source with the adjustments mentioned above into a single CSV file. Load it with the required libraries.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><span class="fu" style="color: #4758AB;">suppressPackageStartupMessages</span>(<span class="fu" style="color: #4758AB;">library</span>(tidyverse))</span>
<span id="cb1-2"><span class="fu" style="color: #4758AB;">suppressPackageStartupMessages</span>(<span class="fu" style="color: #4758AB;">library</span>(plotly))</span>
<span id="cb1-3"></span>
<span id="cb1-4">state_rates <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">read_csv</span>(<span class="st" style="color: #20794D;">"data/state_rate.csv"</span>,<span class="at" style="color: #657422;">col_types =</span> <span class="st" style="color: #20794D;">"fnnn"</span>) </span></code></pre></div>
</div>
<p>Let’s take a quick look at the summary statistics.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><span class="fu" style="color: #4758AB;">summary</span>(state_rates[,<span class="dv" style="color: #AD0000;">2</span><span class="sc" style="color: #5E5E5E;">:</span><span class="dv" style="color: #AD0000;">4</span>])</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   income_tax       sales_tax      property_tax  
 Min.   : 0.000   Min.   :0.000   Min.   :0.270  
 1st Qu.: 4.925   1st Qu.:4.375   1st Qu.:0.730  
 Median : 5.950   Median :6.000   Median :0.980  
 Mean   : 5.835   Mean   :5.062   Mean   :1.119  
 3rd Qu.: 7.190   3rd Qu.:6.250   3rd Qu.:1.550  
 Max.   :13.300   Max.   :7.250   Max.   :2.440  </code></pre>
</div>
</div>
<p>Some states have no personal income tax at all but have to raise revenue somehow. Most commonly, sales tax forms a big part of the budget. Is there a pattern where lower income tax rates correlate with higher sales or property taxes? A correlation matrix provides a quick check.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1">knitr<span class="sc" style="color: #5E5E5E;">::</span><span class="fu" style="color: #4758AB;">kable</span>(<span class="fu" style="color: #4758AB;">cor</span>(state_rates[<span class="dv" style="color: #AD0000;">2</span><span class="sc" style="color: #5E5E5E;">:</span><span class="dv" style="color: #AD0000;">4</span>]))</span></code></pre></div>
<div class="cell-output-display">
<table class="table table-sm table-striped">
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: right;">income_tax</th>
<th style="text-align: right;">sales_tax</th>
<th style="text-align: right;">property_tax</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">income_tax</td>
<td style="text-align: right;">1.0000000</td>
<td style="text-align: right;">0.0292638</td>
<td style="text-align: right;">0.1074844</td>
</tr>
<tr class="even">
<td style="text-align: left;">sales_tax</td>
<td style="text-align: right;">0.0292638</td>
<td style="text-align: right;">1.0000000</td>
<td style="text-align: right;">0.1115520</td>
</tr>
<tr class="odd">
<td style="text-align: left;">property_tax</td>
<td style="text-align: right;">0.1074844</td>
<td style="text-align: right;">0.1115520</td>
<td style="text-align: right;">1.0000000</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>It doesn’t look like there is any relationship.</p>
<p>Tax rates are not the same thing as cash out of pocket. As mentioned above, several issues affect the translation of rates to dollars. Ideally, we would like to know which states are the most expensive to live in, tax-wise. We don’t care which pocket it comes out of but we have to make assumptions.</p>
<p>Let’s add adjustment factors for the impact of sales and property taxes relative to income taxes. This will let us add all three together to come up with a “tax pain” index. In theory, property taxes are levied according to a percentage of the value of the home. But there are complex formulas that go beyond just the published “rate.” In New York, it turns out that the median property tax bill is roughly equal to the median income tax liability, so I chose an adjustment factor of 1.0. How much of your taxable income is spent on consumption of things that sales tax is levied on? As mentioned above, low earners typically live hand-to-mouth. Affluent people can save more for deferred consumption, philanthropy or passing to heirs. I chose to assume 30% of household income is spent where sales taxes apply. Also note that sales tax rates are flat. Not only do poor people consume a higher fraction of their income, sales taxes aren’t scaled by income. You can play around with both of these adjustment factors based on what you want to see. There is no “correct” number. Low income families might pay no income tax and property taxes only indirectly, so sales tax is really the only tax that matters for them.</p>
<p>The tax pain index can be crudely interpreted as the fraction of a high earner’s income that will be paid in just state taxes. I call it an “index” because it can also be interpreted as a comparison of the relative tax burden across states for all wage earners.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><span class="co" style="color: #5E5E5E;"># judge how to weight realized cost of sales and property relative to income tax.</span></span>
<span id="cb5-2">sales_adj    <span class="ot" style="color: #003B4F;">=</span> <span class="fl" style="color: #AD0000;">0.3</span> <span class="co" style="color: #5E5E5E;"># assume we spend 30% of our taxable income on items subject to sales tax.</span></span>
<span id="cb5-3">property_adj <span class="ot" style="color: #003B4F;">=</span> <span class="fl" style="color: #AD0000;">1.0</span> <span class="co" style="color: #5E5E5E;"># assume median income tax liability is about equal to the property tax on the median home. </span></span>
<span id="cb5-4"></span>
<span id="cb5-5"><span class="co" style="color: #5E5E5E;"># use these adjustments to create ranking that we will use to color the markers in the plot.</span></span>
<span id="cb5-6"><span class="co" style="color: #5E5E5E;"># the sum of the adjusted values is a *rough* guide to the total tax burden.</span></span>
<span id="cb5-7"></span>
<span id="cb5-8">state_rates_adj <span class="ot" style="color: #003B4F;">&lt;-</span> state_rates <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb5-9">   <span class="fu" style="color: #4758AB;">mutate</span>(<span class="at" style="color: #657422;">tax_pain =</span> income_tax <span class="sc" style="color: #5E5E5E;">+</span> (sales_tax <span class="sc" style="color: #5E5E5E;">*</span> sales_adj) <span class="sc" style="color: #5E5E5E;">+</span> (property_tax <span class="sc" style="color: #5E5E5E;">*</span> property_adj)) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb5-10">   <span class="fu" style="color: #4758AB;">arrange</span>(<span class="fu" style="color: #4758AB;">desc</span>(tax_pain))</span>
<span id="cb5-11"></span>
<span id="cb5-12">knitr<span class="sc" style="color: #5E5E5E;">::</span><span class="fu" style="color: #4758AB;">kable</span>(state_rates_adj[<span class="dv" style="color: #AD0000;">1</span><span class="sc" style="color: #5E5E5E;">:</span><span class="dv" style="color: #AD0000;">10</span>,])</span></code></pre></div>
<div class="cell-output-display">
<table class="table table-sm table-striped">
<thead>
<tr class="header">
<th style="text-align: left;">state</th>
<th style="text-align: right;">income_tax</th>
<th style="text-align: right;">sales_tax</th>
<th style="text-align: right;">property_tax</th>
<th style="text-align: right;">tax_pain</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">California</td>
<td style="text-align: right;">13.30</td>
<td style="text-align: right;">7.25</td>
<td style="text-align: right;">0.77</td>
<td style="text-align: right;">16.245</td>
</tr>
<tr class="even">
<td style="text-align: left;">New Jersey</td>
<td style="text-align: right;">11.25</td>
<td style="text-align: right;">6.63</td>
<td style="text-align: right;">2.44</td>
<td style="text-align: right;">15.679</td>
</tr>
<tr class="odd">
<td style="text-align: left;">New York</td>
<td style="text-align: right;">10.69</td>
<td style="text-align: right;">4.00</td>
<td style="text-align: right;">1.68</td>
<td style="text-align: right;">13.570</td>
</tr>
<tr class="even">
<td style="text-align: left;">Minnesota</td>
<td style="text-align: right;">9.85</td>
<td style="text-align: right;">6.88</td>
<td style="text-align: right;">1.15</td>
<td style="text-align: right;">13.064</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Hawaii</td>
<td style="text-align: right;">11.00</td>
<td style="text-align: right;">4.00</td>
<td style="text-align: right;">0.27</td>
<td style="text-align: right;">12.470</td>
</tr>
<tr class="even">
<td style="text-align: left;">Vermont</td>
<td style="text-align: right;">8.75</td>
<td style="text-align: right;">6.00</td>
<td style="text-align: right;">1.83</td>
<td style="text-align: right;">12.380</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Iowa</td>
<td style="text-align: right;">8.75</td>
<td style="text-align: right;">6.00</td>
<td style="text-align: right;">1.53</td>
<td style="text-align: right;">12.080</td>
</tr>
<tr class="even">
<td style="text-align: left;">Maryland</td>
<td style="text-align: right;">8.60</td>
<td style="text-align: right;">6.00</td>
<td style="text-align: right;">1.10</td>
<td style="text-align: right;">11.500</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Oregon</td>
<td style="text-align: right;">10.28</td>
<td style="text-align: right;">0.00</td>
<td style="text-align: right;">1.04</td>
<td style="text-align: right;">11.320</td>
</tr>
<tr class="even">
<td style="text-align: left;">District of Columbia</td>
<td style="text-align: right;">8.95</td>
<td style="text-align: right;">6.00</td>
<td style="text-align: right;">0.55</td>
<td style="text-align: right;">11.300</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1">state_rates_adj <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb6-2">   <span class="co" style="color: #5E5E5E;"># reorder the state factor levels so they display in order of tax pain, not alphabetically</span></span>
<span id="cb6-3">   <span class="fu" style="color: #4758AB;">mutate</span>(<span class="at" style="color: #657422;">state =</span> <span class="fu" style="color: #4758AB;">fct_reorder</span>(state,tax_pain)) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb6-4">   <span class="fu" style="color: #4758AB;">ggplot</span>(<span class="fu" style="color: #4758AB;">aes</span>(state,tax_pain)) <span class="sc" style="color: #5E5E5E;">+</span> <span class="fu" style="color: #4758AB;">geom_col</span>() <span class="sc" style="color: #5E5E5E;">+</span> </span>
<span id="cb6-5">   <span class="fu" style="color: #4758AB;">labs</span>(<span class="at" style="color: #657422;">title =</span> <span class="st" style="color: #20794D;">"Cumulative Impact of State Taxes"</span>,</span>
<span id="cb6-6">        <span class="at" style="color: #657422;">subtitle =</span> <span class="st" style="color: #20794D;">"Income, Sales and Property"</span>,</span>
<span id="cb6-7">        <span class="at" style="color: #657422;">x =</span> <span class="st" style="color: #20794D;">"State"</span>,</span>
<span id="cb6-8">        <span class="at" style="color: #657422;">y =</span> <span class="st" style="color: #20794D;">'"Tax Pain" Index'</span>) <span class="sc" style="color: #5E5E5E;">+</span> </span>
<span id="cb6-9">   <span class="fu" style="color: #4758AB;">theme</span>(<span class="at" style="color: #657422;">axis.text.x =</span> <span class="fu" style="color: #4758AB;">element_text</span>(<span class="at" style="color: #657422;">angle =</span> <span class="dv" style="color: #AD0000;">90</span>, <span class="at" style="color: #657422;">hjust =</span> <span class="dv" style="color: #AD0000;">1</span>, <span class="at" style="color: #657422;">vjust =</span> <span class="fl" style="color: #AD0000;">0.5</span>))</span></code></pre></div>
<div class="cell-output-display">
<p><img src="outsiderdata.netlify.app/posts/2019-12-08-state-taxes-it-s-not-just-about-income/2019-12-08-state-taxes-it-s-not-just-about-income_files/figure-html/unnamed-chunk-5-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>No big surprises here. Florida, good. California, bad. Seeing Vermont at the high tax end while New Hampshire is at the low end is interesting. The two states are about the same size and have the same climate. The low tax state has over twice the population and a 33% higher median income. Just sayin’….</p>
<p>We would like to visualize the interplay of the three tax vectors and a 3D scatterplot is ideal for this. Further, the <code>plotly</code> package lets us interactively rotate the plot, which is critical for perceiving the 3D volume on a 2D surface. There are a lot of gratuitous uses of 3D visualization out there. This is one instance where 3D really adds to our understanding.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><span class="co" style="color: #5E5E5E;"># Create 3d animated plot of 3 state tax rate dimensions,</span></span>
<span id="cb7-2"><span class="co" style="color: #5E5E5E;"># income, property and sales</span></span>
<span id="cb7-3"><span class="fu" style="color: #4758AB;">plot_ly</span>(state_rates_adj,<span class="at" style="color: #657422;">x =</span> <span class="sc" style="color: #5E5E5E;">~</span>income_tax,</span>
<span id="cb7-4">        <span class="at" style="color: #657422;">y=</span> <span class="sc" style="color: #5E5E5E;">~</span>sales_tax,</span>
<span id="cb7-5">        <span class="at" style="color: #657422;">z=</span> <span class="sc" style="color: #5E5E5E;">~</span>property_tax,</span>
<span id="cb7-6">        <span class="at" style="color: #657422;">type=</span><span class="st" style="color: #20794D;">"scatter3d"</span>, </span>
<span id="cb7-7">        <span class="at" style="color: #657422;">mode=</span><span class="st" style="color: #20794D;">"markers"</span>,</span>
<span id="cb7-8">        <span class="at" style="color: #657422;">color =</span> <span class="sc" style="color: #5E5E5E;">~</span>tax_pain,</span>
<span id="cb7-9">        <span class="at" style="color: #657422;">hoverinfo =</span> <span class="st" style="color: #20794D;">"text"</span>,</span>
<span id="cb7-10">        <span class="at" style="color: #657422;">text=</span> <span class="sc" style="color: #5E5E5E;">~</span>state) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb7-11">   <span class="fu" style="color: #4758AB;">layout</span>(<span class="at" style="color: #657422;">title =</span> <span class="st" style="color: #20794D;">"Major Tax Rates by State"</span>,</span>
<span id="cb7-12">          <span class="at" style="color: #657422;">scene =</span> <span class="fu" style="color: #4758AB;">list</span>(<span class="at" style="color: #657422;">xaxis =</span> <span class="fu" style="color: #4758AB;">list</span>(<span class="at" style="color: #657422;">title =</span> <span class="st" style="color: #20794D;">'Income Tax'</span>),</span>
<span id="cb7-13">                       <span class="at" style="color: #657422;">yaxis =</span> <span class="fu" style="color: #4758AB;">list</span>(<span class="at" style="color: #657422;">title =</span> <span class="st" style="color: #20794D;">'Sales Tax'</span>),</span>
<span id="cb7-14">                       <span class="at" style="color: #657422;">zaxis =</span> <span class="fu" style="color: #4758AB;">list</span>(<span class="at" style="color: #657422;">title =</span> <span class="st" style="color: #20794D;">'Property Tax'</span>)))</span></code></pre></div>
<div class="cell-output-display">
<div class="plotly html-widget html-fill-item-overflow-hidden html-fill-item" id="htmlwidget-a8065310d181dc352fa5" style="width:100%;height:464px;"></div>
<script type="application/json" data-for="htmlwidget-a8065310d181dc352fa5">{"x":{"visdat":{"2cd852b63b7e":["function () ","plotlyVisDat"]},"cur_data":"2cd852b63b7e","attrs":{"2cd852b63b7e":{"x":{},"y":{},"z":{},"mode":"markers","hoverinfo":"text","text":{},"color":{},"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"scatter3d"}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"title":"Major Tax Rates by State","scene":{"xaxis":{"title":"Income Tax"},"yaxis":{"title":"Sales Tax"},"zaxis":{"title":"Property Tax"}},"hovermode":"closest","showlegend":false,"legend":{"yanchor":"top","y":0.5}},"source":"A","config":{"modeBarButtonsToAdd":["hoverclosest","hovercompare"],"showSendToCloud":false},"data":[{"x":[13.3,11.25,10.69,9.85,11,8.75,8.75,8.6,10.28,8.95,7.65,6.99,7.5,6.84,7.15,5.99,7.08,6.9,6.93,5.95,6.01,7,4.95,5.7,6.5,5.9,5.05,5,5.75,6,5.75,7.23,4.79,6.9,5.25,5,4.9,5,5.5,4.95,4.54,4.63,2.9,2,0,0,0,0,0,0,0],"y":[7.25,6.63,4,6.88,4,6,6,6,0,6,5,6.35,5.75,5.5,5.5,7,6,6.5,6,6,6,6,6.25,6.5,6,4.23,6.25,7,4,4.45,4.3,0,7,0,4.75,4.5,5.13,0,4,4.85,5.6,2.9,5,7,6.25,6.5,6,6.85,4.5,4,0],"z":[0.77,2.44,1.68,1.15,0.27,1.83,1.53,1.1,1.04,0.55,1.94,2.07,1.57,1.8,1.35,1.66,0.86,0.63,0.75,1.64,1.58,0.57,2.31,1.4,0.59,0.99,1.22,0.8,0.91,0.52,0.8,0.56,0.87,0.84,0.86,0.9,0.78,2.2,0.42,0.66,0.72,0.55,1.01,0.74,1.83,1.03,0.98,0.69,1.32,0.61,1.19],"mode":"markers","hoverinfo":["text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text"],"text":["California","New Jersey","New York","Minnesota","Hawaii","Vermont","Iowa","Maryland","Oregon","District of Columbia","Wisconsin","Connecticuit","Ohio","Nebraska","Maine","Rhode Island","Kentucky","Arkansas","Idaho","Michigan","Pennsylvania","South Carolina","Illinois","Kansas","West Virginia","Missouri","Massachussetts","Mississippi","Georgia","Louisiana","Virginia","Delaware","Indiana","Montana","North Carolina","Oklahoma","New Mexico","New Hampshire","Alabama","Utah","Arizona","Colorado","North Dakota","Tennessee","Texas","Washington","Florida","Nevada","South Dakota","Wyoming","Alaska"],"type":"scatter3d","marker":{"colorbar":{"title":"tax_pain","ticklen":2},"cmin":1.19,"cmax":16.245,"colorscale":[["0","rgba(68,1,84,1)"],["0.0416666666666667","rgba(70,19,97,1)"],["0.0833333333333333","rgba(72,32,111,1)"],["0.125","rgba(71,45,122,1)"],["0.166666666666667","rgba(68,58,128,1)"],["0.208333333333333","rgba(64,70,135,1)"],["0.25","rgba(60,82,138,1)"],["0.291666666666667","rgba(56,93,140,1)"],["0.333333333333333","rgba(49,104,142,1)"],["0.375","rgba(46,114,142,1)"],["0.416666666666667","rgba(42,123,142,1)"],["0.458333333333333","rgba(38,133,141,1)"],["0.5","rgba(37,144,140,1)"],["0.541666666666667","rgba(33,154,138,1)"],["0.583333333333333","rgba(39,164,133,1)"],["0.625","rgba(47,174,127,1)"],["0.666666666666667","rgba(53,183,121,1)"],["0.708333333333333","rgba(79,191,110,1)"],["0.75","rgba(98,199,98,1)"],["0.791666666666667","rgba(119,207,85,1)"],["0.833333333333333","rgba(147,214,70,1)"],["0.875","rgba(172,220,52,1)"],["0.916666666666667","rgba(199,225,42,1)"],["0.958333333333333","rgba(226,228,40,1)"],["1","rgba(253,231,37,1)"]],"showscale":false,"color":[16.245,15.679,13.57,13.064,12.47,12.38,12.08,11.5,11.32,11.3,11.09,10.965,10.795,10.29,10.15,9.75,9.74,9.48,9.48,9.39,9.39,9.37,9.135,9.05,8.89,8.159,8.145,7.9,7.86,7.855,7.84,7.79,7.76,7.74,7.535,7.25,7.219,7.2,7.12,7.065,6.94,6.05,5.41,4.84,3.705,2.98,2.78,2.745,2.67,1.81,1.19],"line":{"colorbar":{"title":"","ticklen":2},"cmin":1.19,"cmax":16.245,"colorscale":[["0","rgba(68,1,84,1)"],["0.0416666666666667","rgba(70,19,97,1)"],["0.0833333333333333","rgba(72,32,111,1)"],["0.125","rgba(71,45,122,1)"],["0.166666666666667","rgba(68,58,128,1)"],["0.208333333333333","rgba(64,70,135,1)"],["0.25","rgba(60,82,138,1)"],["0.291666666666667","rgba(56,93,140,1)"],["0.333333333333333","rgba(49,104,142,1)"],["0.375","rgba(46,114,142,1)"],["0.416666666666667","rgba(42,123,142,1)"],["0.458333333333333","rgba(38,133,141,1)"],["0.5","rgba(37,144,140,1)"],["0.541666666666667","rgba(33,154,138,1)"],["0.583333333333333","rgba(39,164,133,1)"],["0.625","rgba(47,174,127,1)"],["0.666666666666667","rgba(53,183,121,1)"],["0.708333333333333","rgba(79,191,110,1)"],["0.75","rgba(98,199,98,1)"],["0.791666666666667","rgba(119,207,85,1)"],["0.833333333333333","rgba(147,214,70,1)"],["0.875","rgba(172,220,52,1)"],["0.916666666666667","rgba(199,225,42,1)"],["0.958333333333333","rgba(226,228,40,1)"],["1","rgba(253,231,37,1)"]],"showscale":false,"color":[16.245,15.679,13.57,13.064,12.47,12.38,12.08,11.5,11.32,11.3,11.09,10.965,10.795,10.29,10.15,9.75,9.74,9.48,9.48,9.39,9.39,9.37,9.135,9.05,8.89,8.159,8.145,7.9,7.86,7.855,7.84,7.79,7.76,7.74,7.535,7.25,7.219,7.2,7.12,7.065,6.94,6.05,5.41,4.84,3.705,2.98,2.78,2.745,2.67,1.81,1.19]}},"frame":null},{"x":[0,13.3],"y":[0,7.25],"type":"scatter3d","mode":"markers","opacity":0,"hoverinfo":"none","showlegend":false,"marker":{"colorbar":{"title":"tax_pain","ticklen":2,"len":0.5,"lenmode":"fraction","y":1,"yanchor":"top"},"cmin":1.19,"cmax":16.245,"colorscale":[["0","rgba(68,1,84,1)"],["0.0416666666666667","rgba(70,19,97,1)"],["0.0833333333333333","rgba(72,32,111,1)"],["0.125","rgba(71,45,122,1)"],["0.166666666666667","rgba(68,58,128,1)"],["0.208333333333333","rgba(64,70,135,1)"],["0.25","rgba(60,82,138,1)"],["0.291666666666667","rgba(56,93,140,1)"],["0.333333333333333","rgba(49,104,142,1)"],["0.375","rgba(46,114,142,1)"],["0.416666666666667","rgba(42,123,142,1)"],["0.458333333333333","rgba(38,133,141,1)"],["0.5","rgba(37,144,140,1)"],["0.541666666666667","rgba(33,154,138,1)"],["0.583333333333333","rgba(39,164,133,1)"],["0.625","rgba(47,174,127,1)"],["0.666666666666667","rgba(53,183,121,1)"],["0.708333333333333","rgba(79,191,110,1)"],["0.75","rgba(98,199,98,1)"],["0.791666666666667","rgba(119,207,85,1)"],["0.833333333333333","rgba(147,214,70,1)"],["0.875","rgba(172,220,52,1)"],["0.916666666666667","rgba(199,225,42,1)"],["0.958333333333333","rgba(226,228,40,1)"],["1","rgba(253,231,37,1)"]],"showscale":true,"color":[1.19,16.245],"line":{"color":"rgba(255,127,14,1)"}},"z":[0.27,2.44],"frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
</div>
</div>
<p>Play around with dragging the image and you start to appreciate the volume. Each piece of the tax picture gets an axis. The tax pain index is represented by color of the markers. You can quickly see that income tax is still the big driver of tax pain across the nation. New Jersey applies high taxes in all dimensions. California is heavily skewed to income tax but is comparatively low in the property tax dimension.</p>
<p>Nevada is a great state to live in if you have income and property. The state gets about a third of its revenue from out-of-state tourists who are spending liberally. Gambling is big, obviously, but a high sales tax is a way to get revenue from visitors while making the tax burden lighter on residents. As we know, sales taxes are regressive so, at first glance, the poor residents of Nevada might be the unintended losers from this scheme. Fortunately, Nevada lightens the relative burden on the poor by exempting drugs and groceries from sales tax.</p>
<p>Another great place to live if you hate taxes is in Washington State, on the Oregon border. Washington levies no income tax and Oregon levies no sales tax. I was surprised to see, in a quick Google maps search, no evidence that big box retailers shun the Washington side of the border. In theory, if an Oregon business knows you live in Washington they are supposed to charge taxes (Ha!). Across the border, Oregon residents could avoid paying sales tax in Washington by flashing an Oregon ID but that ended in the summer of 2019.</p>
<p>Finally, Alaska is the most tax-friendly state overall with low taxes in all dimensions. The state goes even further, though. Oil revenues go into a fund which pays a cash dividend to every resident, every year. Most recently it was $1,600 so some residents, in effect, receive taxes from the state. So, move there.</p>


</section>

 ]]></description>
  <category>tax</category>
  <category>plotly</category>
  <guid>outsiderdata.netlify.app/posts/2019-12-08-state-taxes-it-s-not-just-about-income/2019-12-08-state-taxes-it-s-not-just-about-income.html</guid>
  <pubDate>Sun, 08 Dec 2019 05:00:00 GMT</pubDate>
  <media:content url="outsiderdata.netlify.app/posts/2019-12-08-state-taxes-it-s-not-just-about-income/img/thumbnail.png" medium="image" type="image/png" height="131" width="144"/>
</item>
<item>
  <title>Gender Diversity in R and Python Package Contributors</title>
  <dc:creator>Art Steinmetz</dc:creator>
  <link>outsiderdata.netlify.app/posts/2019-07-16-gender-diversity-in-r-and-python-package-contributors/2019-07-16-gender-diversity-in-r-and-python-package-contributors.html</link>
  <description><![CDATA[ 



<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">
Introduction
</h2>
<p>
Over the last few years I have really enjoyed becoming part of the R community. One of the best things about the community is the welcoming, inclusive and supportive nature of it. I can’t speak for other communities in the computer or data science worlds but I am well aware of the “brogrammer” culture in some circles that can be off-putting at times. The rise of codes of conduct across the open source world is changing things for the better, I think.
</p>
<p>
A couple months ago the creator of Python was interviewed saying <a href="https://qz.com/1624252/pythons-creator-thinks-it-has-a-diversity-problem/">he thinks open source programming languages have a gender diversity problem</a>. This got me to thinking about whether the inclusive environment I observe in the R community is reflected in female contributions to popular packages and how it compares to the Python world. Most of these packages are maintained on Github which includes all the contributors who use the Github environment to contribute. Let’s take a stab at identifying the gender of these contributors by name.
</p>
<p>
We will take a multi-stage approach to getting an answer to this question.
</p>
<ol style="list-style-type: decimal">
<li>
Get the names of the top packages in R and Python.
</li>
<li>
Identify which those packages which are maintained on Github.
</li>
<li>
Get the contributors to those packages (not as easy as it sounds).
</li>
<li>
Get baby names by gender from the U.S. Social Security database.
</li>
<li>
Decide whether a name is likely to be female or male.
</li>
<li>
Map all package conrtributors to gender, where possible.
</li>
</ol>
<p>
As usual I follow a couple conventions. The Tidyverse dialect is used throughout. All functions to fetch data from the Web are wrapped in a test to see if the data was already retrieved. This ensures that this notebook won’t break if things in the wild change. In that event, you must get the data files from this Github repo for this to work.
</p>

<pre class="sourceCode r code-with-copy"><code>library(tidyverse)
library(purrr)
library(jsonlite)
library(rvest)
library(data.table) #for downloading CRAN/RStudio logs
library(httr)
library(gh)
library(formattable) #percent</code></pre>
</section>
<section id="identify-the-top-packages-in-r-and-python." class="level2">
<h2 class="anchored" data-anchor-id="identify-the-top-packages-in-r-and-python.">
Identify the top packages in R and Python.
</h2>
<p>
Use the cranlogs api from RStudio to get top package downloads from their CRAN mirror. This is potentially a slow function but the top package downloads are pretty stable so we choose five randomly selected dates.
</p>
<pre class="sourceCode r code-with-copy"><code># ----------------------------------------------------------------
#select 5 random days from the last six months
# Read data from RStudio site
# custom version of a function from the installr package. See my Github repo.
source(file="data/download_RStudio_CRAN_data.R") 

if (!file.exists("data/r_pkg_list.rdata")) {
 RStudio_CRAN_dir &lt;- download_RStudio_CRAN_data(START = Sys.Date()-180,END = Sys.Date(),sample=5)
 # read .gz compressed files form local directory
 RStudio_CRAN_data &lt;- read_RStudio_CRAN_data(RStudio_CRAN_dir)
 
 dim(RStudio_CRAN_data)
 
 # Find the most downloaded packages
 r_pkg_list &lt;- most_downloaded_packages(RStudio_CRAN_data,n=100) %&gt;% 
  as_tibble(.name_repair = make.names,c("downloads")) %&gt;% 
  rename(package=X)
 
 save(r_pkg_list,file="data/r_pkg_list.rdata")
} else load("data/r_pkg_list.rdata")</code></pre>
<p>
With Python the work as already been done for us here: <a href="https://hugovk.github.io/top-pypi-packages/" class="uri">https://hugovk.github.io/top-pypi-packages/</a>. How helpful!
</p>
<pre class="sourceCode r code-with-copy"><code>if (!file.exists("data/python_pkg_list.rdata")){
 
 py_pkgs_raw&lt;-read_json("https://hugovk.github.io/top-pypi-packages/top-pypi-packages-365-days.json",
             simplifyVector = TRUE)
 python_pkg_list &lt;- py_pkgs_raw$rows[1:100,] %&gt;% 
  as_tibble() %&gt;% 
  rename(package=project,downloads=download_count)
 save(python_pkg_list,file="data/python_pkg_list.rdata")
} else load("data/python_pkg_list.rdata")</code></pre>
</section>
<section id="get-the-contributor-names-for-each-package-repo." class="level2">
<h2 class="anchored" data-anchor-id="get-the-contributor-names-for-each-package-repo.">
Get the contributor names for each package repo.
</h2>
<p>
This is the messy stuff. We build functions to get contributors to packages and then the real names of those contributors.
</p>
<p>
We start with a search for the relevant repo with just repo name and optionally the language. Suppose we want to know the names of the R <code>dplyr</code> contributors. The workflow looks like this:
</p>
<p>
Call the API:
</p>
<p>
<a href="https://api.github.com/search/repositories?q=dplyr+language:r" class="uri">https://api.github.com/search/repositories?q=dplyr+language:r</a>
</p>
<p>
Github returns a list of the most relevant results based on their point system. In practice this means the package we care about will be the first item in the list. In this case:
</p>
<p>
“full_name”: “tidyverse/dplyr”
</p>
<p>
One problem I encountered is that not all R packages are tagged as being in the R language. In particular, <code>Rcpp</code> and <code>data.table</code> are considered C language repos by Github. This is one reason why not all the top packages appear to have Github repos. I manually grab the contributors for the two packages mentioned above but, out of laziness, I didn’t go looking for any other missing packages. As we will see, most of the top 100 packages for both languages are found so we have a fairly representative sample…I assume.
</p>
<p>
Once we have the full package name we can create URLs to get the usernames of all the contributors.
</p>
<p>
Contributor url: <a href="https://api.github.com/repos/tidyverse/dplyr/contributors" class="uri">https://api.github.com/repos/tidyverse/dplyr/contributors</a>
</p>
<p>
This JSON object will not contain the “real” names but the links to user profiles. We have to make yet another call to the API to extract the real names. Note some people use pseudonyms so the real name won’t be available.
</p>
<p>
Calling the endpoint for the username “<a href="https://api.github.com/users/romainfrancois" class="uri">https://api.github.com/users/romainfrancois</a>”,
</p>
<p>
will return, among other things:
</p>
<p>
“name”: “Romain François”
</p>
<p>
Finally, we get what we are after!
</p>
<p>
NOTE: You will need a Github API key for this work. Please refer to the documentation for the <code>gh</code> package.
</p>
<p>
The utility functions are below:
</p>
<pre class="sourceCode r code-with-copy"><code>my_gh &lt;- function(end_point) {
  return(jsonlite::fromJSON(jsonlite::toJSON(gh::gh(end_point)),simplifyVector = T))
}

json_to_df &lt;- function(json){
  return(jsonlite::fromJSON(jsonlite::toJSON(json),simplifyVector = T))
}

# --------------------------------------------------------------------
get_contributor_ids &lt;- function(target_repo){
# loop through all pages of contributors 
 search_url &lt;- paste0("/repos/",
            target_repo,
            "/contributors")
 contributors_json &lt;- gh(search_url)
 
 # return null in case of no contributors
 if (nchar(contributors_json[1])==0) return(NULL)
 
 contrib_node &lt;- contributors_json
 repeat {
  contrib_node &lt;- try(gh_next(contrib_node),silent=TRUE)
  if (is(contrib_node) == "try-error") break
  contributors_json &lt;- c(contributors_json,contrib_node) 
 }

 contributor_ids &lt;- json_to_df(contributors_json) %&gt;%
  bind_rows() %&gt;%  
  select(login,url,avatar_url)
 return(contributor_ids)
}

# ---------------------------------------------------------------------------
 get_name &lt;- function(contrib_url){
  user_data &lt;- my_gh(contrib_url)
  # just return login name if real name is missing
  if (is_empty(user_data$name)) return(user_data$login) else return(user_data$name)
 }

# --------------------------------------------------------------------
get_contrib_info &lt;- function(repo_name="dplyr",language=NULL){
  print(repo_name)
  # we don't know the Github username associated with the package
  #so construct a search to get the most likely candidate
  search_url &lt;- paste0("/search/repositories?q=",
                       repo_name)
  if (!is.null(language)){
    search_url &lt;- paste0(search_url,"+language:", language)
  }
  # first api call.
  repos &lt;- my_gh(search_url) %&gt;% .$items
  # return NULL if no repos in Github are found
  if (length(repos) == 0) return(NULL)
  
  # get full path for exact match on repo name
  # there might be more than one user with repo of the same name
  # Since they will be in order of Github "score", take just the first one
  target_repo &lt;- repos %&gt;% 
    select(name,full_name) %&gt;% 
    filter(name == repo_name) %&gt;%
    pull(full_name) %&gt;% 
    .[1] %&gt;% 
    unlist()
  # return NULL if no repos in Github are found
  if (is.null(target_repo)) return(NULL)
  
  #second api call
  # get user urls for all contributors
  contributor_ids &lt;- get_contributor_ids(target_repo)
  
  # return null in case of no contributors
  if (is.null(contributor_ids)) return(NULL)
  if (is.null(language)) language &lt;- "none"
  contrib_names&lt;-map(contributor_ids$url,get_name) %&gt;% unlist()
  print(paste(length(contrib_names)," contributors"))
  contrib_info &lt;- tibble(language=language,
                         package=repo_name,
                         path=target_repo,
                         contributor=contrib_names) %&gt;% 
    bind_cols(contributor_ids) %&gt;% 
    select(-url) %&gt;% unnest()
  return(contrib_info)
}</code></pre>
<p>
Now let’s do the work of iterating through the package lists. As mentioned above, I get two packages manually before looping through the remaining packages. I chose to use a <code>for</code> loop, as opposed to <code>map</code> or <code>apply</code> so we can save the intermediate results. It is a fairly slow process and you may reach your API data limit before finishing. You don’t want to start from scratch halfway through! If you have to do this in multiple sessions, manually edit the package lists to include just what is left to retrieve.
</p>

<pre class="sourceCode r code-with-copy"><code>load("data/r_pkg_list.rdata")
if (!file.exists("data/r_pkg_contributors.rdata")){
  r_pkg_contributors &lt;- NULL
  # Rcpp package is categorized as C++, not R, langauge so get it manually.
  contrib_info_rcpp &lt;- get_contrib_info("Rcpp")
  contrib_info_rcpp &lt;- contrib_info_rcpp %&gt;% mutate(language = "r")
  r_pkg_contributors &lt;- bind_rows(r_pkg_contributors,contrib_info_rcpp)
  r_pkg_list &lt;- r_pkg_list %&gt;% filter(package != "Rcpp")
  
  # data.table package is categorized as C++, not R, langauge so get it manually.
  contrib_info_dt &lt;- get_contrib_info("data.table")
  contrib_info_dt &lt;- contrib_info_dt %&gt;% mutate(language = "r")
  r_pkg_contributors &lt;- bind_rows(r_pkg_contributors,contrib_info_dt)
  r_pkg_list &lt;- r_pkg_list %&gt;% filter(package != "dt")
  
  # use for loop instead of map or apply so we can save intermediate steps
  for(pkg in r_pkg_list$package) {
    r_pkg_contributors &lt;- r_pkg_contributors %&gt;% 
      bind_rows(get_contrib_info(pkg,language="r"))
    save(r_pkg_contributors,file="data/r_pkg_contributors.rdata")
  }
} else load("data/r_pkg_contributors.rdata")

load("data/python_pkg_list.rdata")
if (!file.exists("data/python_pkg_contributors.rdata")){
 python_pkg_contributors &lt;- NULL
 for(pkg in python_pkg_list$package) {
  python_pkg_contributors &lt;- python_pkg_contributors %&gt;% 
   bind_rows(get_contrib_info(pkg,language="python"))
  save(python_pkg_contributors,file="data/python_pkg_contributors.rdata")
 } 
} else load("data/python_pkg_contributors.rdata")

#Let's merge the two datasets to simplify handling.
pkg_contributors &lt;- bind_rows(r_pkg_contributors,python_pkg_contributors)</code></pre>
</section>
<section id="analysis" class="level1">
<h1>
Analysis
</h1>
<p>
Since we went through a lot of trouble to get this data, let’s do some exploratory work on package contributors in general before we come to the main question of this post.
</p>
<pre class="sourceCode r code-with-copy"><code>load("data/r_pkg_contributors.rdata")

#summarize what we found
r_pkg_contributors %&gt;% 
 group_by(package) %&gt;% 
 summarise(contributors=n()) %&gt;% 
 summary()</code></pre>
<pre><code>##    package           contributors   
##  Length:75          Min.   :  1.00  
##  Class :character   1st Qu.:  3.50  
##  Mode  :character   Median : 10.00  
##                     Mean   : 30.49  
##                     3rd Qu.: 32.50  
##                     Max.   :300.00</code></pre>
<p>
There are 75 out of the top 100 R packages with repos we <em>easily</em> found on Github (remember, I’m lazy). The median number of contributors is 10. 300 people have contributed to the <code>fs</code> package, which is implements the linux file library <code>libuv</code>.
</p>
<p>
How did we do with the Python packages?
</p>
<pre class="sourceCode r code-with-copy"><code>load("data/python_pkg_contributors.rdata")

#summarize what we found
python_pkg_contributors %&gt;% 
 group_by(package) %&gt;% 
 summarise(contributors=n()) %&gt;% 
 summary()</code></pre>
<pre><code>##    package           contributors  
##  Length:81          Min.   :  1.0  
##  Class :character   1st Qu.: 10.0  
##  Mode  :character   Median : 44.0  
##                     Mean   :101.7  
##                     3rd Qu.:116.0  
##                     Max.   :429.0</code></pre>
<p>
Right off the bat, it looks like Python package development is more of a community effort. The median package has 44 contributors. The venerable <code>matplotlib</code> takes the prize of most contributors at 429.
</p>
<p>
Let’s look at the most contributed-to packages. Remember this is NOT a ranking of the most popular packages. It is a ranking of the number of contributors among the most popular packages. For R, the takeaway is that the Tidyverse is very much a shared effort.
</p>
<pre class="sourceCode r code-with-copy"><code>pkg_contributors %&gt;% 
 filter(language=="r") %&gt;% 
 group_by(package) %&gt;%
 summarise(contributors=n()) %&gt;% 
 arrange(contributors) %&gt;%
 top_n(10,wt=contributors) %&gt;% 
 ggplot(aes(as_factor(package),contributors)) + 
 geom_col()+ 
 labs(title = "R Packages", x = "Package Name",y = "Contributor Count") +
 coord_flip()</code></pre>
<p>
<img src="outsiderdata.netlify.app/posts/2019-07-16-gender-diversity-in-r-and-python-package-contributors/img/unnamed-chunk-5-1.png" width="672">
</p>
<pre class="sourceCode r code-with-copy"><code>pkg_contributors %&gt;% 
 filter(language=="python") %&gt;% 
 group_by(package) %&gt;%
 summarise(contributors=n()) %&gt;% 
 arrange(contributors) %&gt;%
 top_n(10,wt=contributors) %&gt;% 
 ggplot(aes(as_factor(package),contributors)) + 
 geom_col()+ 
 labs(title = "Python Packages", x = "Package Name",y = "Contributor Count") +
 coord_flip()</code></pre>
<p>
<img src="outsiderdata.netlify.app/posts/2019-07-16-gender-diversity-in-r-and-python-package-contributors/img/unnamed-chunk-6-1.png" width="672">
</p>
<p>
Let’s compare the number of contributors for the top packages in both languages, we find that Python packages tend to have many more contributors.
</p>
<pre class="sourceCode r code-with-copy"><code>summary_contrib&lt;-pkg_contributors %&gt;% 
 group_by(language,package) %&gt;% 
 summarise(num_contributors=n()) %&gt;% 
 group_by(language) %&gt;% 
 mutate(ranking=rank(num_contributors,ties.method = "first")) %&gt;%
 mutate(ranking=max(ranking)-ranking+1) %&gt;% 
 arrange(ranking)

summary_contrib %&gt;% 
 ggplot(aes(ranking,num_contributors,group=language,color=language)) + 
 geom_path(size=2) + 
 theme(axis.text.x=element_blank()) +
 labs(main="Top Python Packages Show More Collaboration",
    x="Packages in Order of Contributors",
    y="Number of Contributors")</code></pre>
<p>
<img src="outsiderdata.netlify.app/posts/2019-07-16-gender-diversity-in-r-and-python-package-contributors/img/unnamed-chunk-7-1.png" width="672"> Who are the most prolific contributors among the top packages? We note, with R, that many of the top packages are part of the tidyverse ecosystem and will have a very high degree of overlap among package contributors. No one Tidyverse package is a good proxy of the contributors to the rest, however.
</p>
<section id="who-are-the-most-prolific-r-package-contributors" class="level2">
<h2 class="anchored" data-anchor-id="who-are-the-most-prolific-r-package-contributors">
Who are the most prolific R package contributors?
</h2>
<pre class="sourceCode r code-with-copy"><code>pkg_contributors %&gt;% 
 filter(language=="r") %&gt;% 
 group_by(contributor) %&gt;% 
 summarise(packages=n()) %&gt;% 
 arrange(desc(packages))</code></pre>
<pre><code>## # A tibble: 1,299 x 2
##    contributor            packages
##    &lt;chr&gt;                     &lt;int&gt;
##  1 Hadley Wickham               45
##  2 Jim Hester                   36
##  3 Kirill Müller                31
##  4 Mara Averick                 26
##  5 Jennifer (Jenny) Bryan       24
##  6 Gábor Csárdi                 19
##  7 Hiroaki Yutani               18
##  8 Lionel Henry                 16
##  9 Yihui Xie                    16
## 10 Christophe Dervieux          15
## # ... with 1,289 more rows</code></pre>
</section>
<section id="who-are-the-most-prolific-python-package-contributors" class="level2">
<h2 class="anchored" data-anchor-id="who-are-the-most-prolific-python-package-contributors">
Who are the most prolific Python package contributors?
</h2>
<pre class="sourceCode r code-with-copy"><code>pkg_contributors %&gt;% 
 filter(language=="python") %&gt;% 
 group_by(contributor) %&gt;% 
 summarise(packages=n()) %&gt;% 
 arrange(desc(packages))</code></pre>
<pre><code>## # A tibble: 6,194 x 2
##    contributor     packages
##    &lt;chr&gt;              &lt;int&gt;
##  1 Jon Dufresne          30
##  2 Hugo                  27
##  3 Marc Abramowitz       24
##  4 Jason R. Coombs       18
##  5 Jakub Wilk            17
##  6 Alex Gaynor           16
##  7 Anthony Sottile       15
##  8 Felix Yan             15
##  9 Ville Skyttä          15
## 10 Donald Stufft         14
## # ... with 6,184 more rows</code></pre>
</section>
<section id="people-who-swing-both-ways." class="level2">
<h2 class="anchored" data-anchor-id="people-who-swing-both-ways.">
People who swing both ways.
</h2>
<p>
Who are the awesome humans who have contributed to both top R and Python packages? Grouping by login name ensures that we don’t get two different people with the same name but we drop it for display. There are 44 people who have contributed to some of both the top Python and R packages.
</p>
<pre class="sourceCode r code-with-copy"><code>two_lang_contrib &lt;- pkg_contributors %&gt;% 
 group_by(login,contributor,language) %&gt;%
 summarise(packages=n()) %&gt;% 
 spread(language,packages) %&gt;% 
 ungroup() %&gt;% 
 select(-login)

two_lang_contrib &lt;- two_lang_contrib[complete.cases(two_lang_contrib),] %&gt;% 
 arrange(desc(r))

two_lang_contrib </code></pre>
<pre><code>## # A tibble: 46 x 3
##    contributor              python     r
##    &lt;chr&gt;                     &lt;int&gt; &lt;int&gt;
##  1 Craig Citro                   3     7
##  2 Elliott Sales de Andrade      2     5
##  3 Philipp A.                    4     4
##  4 Aaron Schumacher              2     3
##  5 Ayappan                       1     2
##  6 Chapman Siu                   1     2
##  7 Ethan White                   1     2
##  8 Katrin Leinweber              2     2
##  9 Mark Sandan                   1     2
## 10 Tim D. Smith                  3     2
## # ... with 36 more rows</code></pre>
</section>
<section id="try-to-determine-gender-of-contributors." class="level2">
<h2 class="anchored" data-anchor-id="try-to-determine-gender-of-contributors.">
Try to determine gender of contributors.
</h2>
<p>
I hope you found the digressions above interesting. Now let’s do what we came to do.
</p>
<p>
To flag names by gender we use the Social Security baby names database for 1990. It is important to be aware of the limitations of this.
</p>
<ol style="list-style-type: decimal">
<li>
<p>
I used 1990 because I guess that is close to the average birth year of most package contributors. Is it? My birth year is (cough) 1958. I am an outlier.
</p>
</li>
<li>
<p>
The dataset contains registered births for only the United States. Many contributors were born, or live today, outside the U.S. The U.S, while more of a melting pot than many countries, will have a subset of global names.
</p>
</li>
<li>
<p>
Transliteration of names from languages that don’t use Western characters don’t follow hard and fast rules. The same name might be transliterated multiple ways. “Sergey” or “Sergei?”
</p>
</li>
<li>
<p>
Ordering of surname and given name. Chinese names typically are reported surname first. Many Chinese people follow western conventions in global settings but maybe not. I may be tagging the surname as the given name in some (many?) cases.
</p>
</li>
<li>
<p>
Many names are used for “both” (yes, I know) genders. I choose an aribitrary ratio of gender predominance of 75% to pronounce certainty. Noteworthy: “Hadley” is in our “Uncertain” bucket.
</p>
</li>
<li>
<p>
Gender identity becomes a choice at some age. People may choose (or not choose) a gender inconsistant with the identification in this dataset.
</p>
</li>
<li>
<p>
Some people use pseudonyms that are not common names.
</p>
</li>
</ol>
<p>
Knowing all that, let’s plunge on.
</p>
<p>
You can find the link to the <a href="https://catalog.data.gov/dataset/baby-names-from-social-security-card-applications-national-level-data/resource/fdfd2c5c-6190-4fac-9ead-ae478e0c2790">baby names data set here</a>. There are CSV files for each birth year in a zip file. Download, extract and import the file called “yob1990.txt”
</p>
<pre class="sourceCode r code-with-copy"><code>names_90 &lt;- read_csv("data/yob1990.txt",
           col_names=c("first","gender","count"),
           col_types = list(col_character(),col_character(),col_number()))

names_90 &lt;- names_90 %&gt;% 
 mutate(first = tolower(first)) %&gt;% 
 select(first,gender,count) %&gt;% 
 spread(gender,count) %&gt;% 
 mutate_if(is.numeric, ~replace(., is.na(.), 0)) %&gt;% 
 mutate(prob_female=F/(F+M))

cutoff = 0.75 # threshhold probability for calling gender
names_90 &lt;- names_90 %&gt;% mutate(gender="Uncertain")
names_90 &lt;- names_90 %&gt;% mutate(gender=if_else(prob_female&gt;cutoff,"Female",gender))
names_90 &lt;- names_90 %&gt;% mutate(gender=if_else(prob_female&lt;(1-cutoff),"Male",gender))
names_90_subset &lt;- names_90 %&gt;% select(first,gender)</code></pre>
<p>
Now let’s join the baby names to our contributors.
</p>
<pre class="sourceCode r code-with-copy"><code>pkg_contributors &lt;-pkg_contributors %&gt;%
 separate("contributor",into=c("first"),remove=FALSE,extra="drop")
genders &lt;- pkg_contributors %&gt;% 
 select(-path,-avatar_url,-login) %&gt;% 
 mutate(first = tolower(first)) %&gt;% 
 left_join(names_90_subset,by="first") %&gt;% 
 mutate_all(~replace(., is.na(.),"Uncertain")) </code></pre>
<p>
Our answer now looms into view. Base R has a nice tile plot that illustrates the proportions and sizes of the cells in a crosstab so we’ll use that.
</p>

<pre class="sourceCode r code-with-copy"><code>agg_gender &lt;- genders %&gt;%  
 select(language,gender) %&gt;% 
 table() 
agg_gender %&gt;% plot(main="Gender Representation in Package Contributions")</code></pre>
<p>
<img src="outsiderdata.netlify.app/posts/2019-07-16-gender-diversity-in-r-and-python-package-contributors/img/unnamed-chunk-13-1.png" width="672">
</p>
<p>
Right away we note the large fraction of “Uncertain” genders, about a third. As we noted above, there are many more contributors to Python packages, as reflected in the width of the tiles. We also can see that the fraction of women contributing to R packages looks greater.
</p>
<p>
For our ultimate conclusion, let’s assume that the “Uncertain” gender breaks into male and female in the same proportions that already exist.
</p>

<pre class="sourceCode r code-with-copy"><code>agg_gender &lt;- genders %&gt;% 
 filter(gender != "Uncertain") %&gt;% 
 select(language,gender) %&gt;% 
 table() %&gt;% prop.table(margin=1) 

percent(agg_gender,digits = 0)</code></pre>
<pre><code>##         gender
## language Female Male
##   python  4%    96% 
##   r       8%    92%</code></pre>
<p>
There it is. This was certainly a lot of work to get to a four cell crosstab but we have our answer. <strong>Women contribute to the top R packages at twice the rate of top Python packages.</strong> Can we speculate as to a reason? R is almost exclusively a data science language and most of the top packages reflect that. Python is more of a general purpose language that is also quite popular for data science, but as we look down the list of most popular Python packages we see more utility packages. Perhaps women are less represented in general computer science than they are in data science. With both languages, more than 90% of the contributors are men. Clearly, we have a way to go with gender diversity in both communities. Narrowing down the package list to focus on just data science packages is an avenue for further exploration.
</p>
<p>
As a bonus, what are the most “feminine” packages?
</p>
<pre class="sourceCode r code-with-copy"><code>genders %&gt;% group_by(language,package,gender) %&gt;% 
  filter(gender != "Uncertain") %&gt;% 
  count() %&gt;% 
  spread(gender,n) %&gt;% 
  mutate(frac_female = Female/(Female+Male)) %&gt;% 
  arrange(desc(frac_female))</code></pre>
<pre><code>## # A tibble: 147 x 5
## # Groups:   language, package [147]
##    language package    Female  Male frac_female
##    &lt;chr&gt;    &lt;chr&gt;       &lt;int&gt; &lt;int&gt;       &lt;dbl&gt;
##  1 r        cellranger      1     1       0.5  
##  2 r        hms             1     2       0.333
##  3 r        rprojroot       1     2       0.333
##  4 r        tidyselect      3     6       0.333
##  5 r        forcats         7    18       0.28 
##  6 r        ellipsis        1     3       0.25 
##  7 python   decorator       2     7       0.222
##  8 r        scales          5    18       0.217
##  9 r        tidyr          11    41       0.212
## 10 r        pillar          1     4       0.2  
## # ... with 137 more rows</code></pre>
<p>
That’s interesting. There are 28 popular packages across both languages where more than 10% of the contributors are female. Of those 25 are R packages and only 3 are Python packages.
</p>
<p>
There are other dimensions of diversity we might look at that are beyond the ability to infer from names. It would be nice if we could see actual images of all contributors so we might make some observations about racial diversity or remove some of the ambiguities around gender identification. This approach would come with its own set of challenges and risks, however.
</p>
<p>
As mentioned at the start of this ariticle, there are many reasons to take our conclusions with a grain of salt. I certainly do not claim this analysis is definitive. A better approach might be to simply survey the contributors. Still, the results conform with what intuition might provide.
</p>
<p>
I welcome critiques of my methods or conclusions. I have a sneaky suspicion I got the Github contributor names the hard way. Thanks for reading!
</p>
</section>
</section>



 ]]></description>
  <category>github</category>
  <category>gender</category>
  <guid>outsiderdata.netlify.app/posts/2019-07-16-gender-diversity-in-r-and-python-package-contributors/2019-07-16-gender-diversity-in-r-and-python-package-contributors.html</guid>
  <pubDate>Tue, 16 Jul 2019 04:00:00 GMT</pubDate>
  <media:content url="outsiderdata.netlify.app/posts/2019-07-16-gender-diversity-in-r-and-python-package-contributors/img/unnamed-chunk-13-1.png" medium="image" type="image/png" height="103" width="144"/>
</item>
<item>
  <title>Why I migrated from Excel to R</title>
  <dc:creator>Art Steinmetz</dc:creator>
  <link>outsiderdata.netlify.app/posts/2019-06-12-why-i-migrated-from-excel-to-r/2019-06-12-why-i-migrated-from-excel-to-r.html</link>
  <description><![CDATA[ 



<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="outsiderdata.netlify.app/posts/2019-06-12-why-i-migrated-from-excel-to-r/img/spreadsheet_mosaic.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Old Spreadsheets</figcaption><p></p>
</figure>
</div>
<p>I’ve been a spreadsheet power user from the days of Visicalc for the Apple ][. I migrated to Lotus 1-2-3, to Borland Quattro and finally to Excel. With Excel, I’ve bludgeoned Visual Basic to create some pretty complicated dashboards and analytics. When I started using R I used tools like <a href="http://rcom.univie.ac.at/">RExcel</a> that plug R into Excel as an analytic server, or I would use Excel to download data from investment databases and export it for use in R. But now I find I open up Excel only rarely and do all my quantitative investigations entirely within R. Why?</p>
<ol type="1">
<li><p>Simplicity. R uses is a very different paradigm than a spreadsheet so it takes some getting used to. On the surface, R is a programming language, like C or Java. Spreadsheets were invented to free humans to get real analytic work done without becoming coders. Yet R comes at the coding angle from a very different direction than application languages. It is a data science tool first and a programming language second. Like a spreadsheet, the central unit of analytic work is data in a row and column format. Once you master the vernacular of manipulating these “data frames” the rest is easy. Like a spreadsheet, R is interactive. You try single operations and can paste the successful ones into your “program” in a sequential fashion, building up your analysis step by step. In doing so you are creating a log of your work that lets you pick up later where you left off without missing a beat or reuse bits in other projects. Most people use the <a href="https://www.rstudio.com/products/rstudio/">R Studio development environment</a> as their workbench. It’s simple, powerful and free!</p></li>
<li><p>Auditability. Big Excel spreadsheets are a labyrinth of linked formulas. Tracing errors is extremely difficult. Noticing errors at all is often tough. They often go unnoticed for the life of the spreadsheet. When I come back to a spreadsheet I’ve created months ago, I often can’t remember how I did something. If something breaks, tracking down the broken piece takes a long time. So while spreadsheets are easy to master, mastery comes at the expense of maintainability. With R, each step of the analysis proceeds in a roughly linear fashion. Each piece building on the previous one. It is easy to see where the problems are and to insert the fixes without blowing up something else you didn’t realize was connected.</p></li>
<li><p>Reproducibility. How often do you share a big spreadsheet with someone else? Can they use it? Have you been bequeathed a spreadsheet that is part of the team workflow that you have no idea how to maintain? With R the logic of the analysis flows in discrete steps so every step is immediately visible. The code is its own log of all the work performed. That’s not to say you are off the hook for documentation. Well commented code is a sign of disciplined thinking and a courtesy to both others and your future self. Inserting a comment line in R code comes naturally, I find, while it requires conscious effort in Excel.</p></li>
<li><p>Shareability. “Notebooks” are the new thing and I love them. These are HTML documents, <a href="https://outsiderdata.netlify.com/">like this blog</a>, that include descriptive text, R code (or whatever language you use) and the output of the code. They make sharing and showing off your work visually attractive and simple to follow. You can attach the HTML document to an email, render it as a PDF or publish it to a web site. Embedding markup language is a little extra work but the R Studio IDE creates the templates automatically, and the result is sharp.</p></li>
<li><p>Visualization. It is easy to create charts in Excel. It is easy to create charts in R, though it is done in code, not interactively. What R can do that Excel can’t, is to go further to make great looking charts because of the customization that is possible. I won’t claim it is easy, though. The learning curve is <a href="http://r-statistics.co/Top50-Ggplot2-Visualizations-MasterList-R-Code.html">steep, but rewarding</a>.</p></li>
</ol>
<p><img src="outsiderdata.netlify.app/posts/2019-06-12-why-i-migrated-from-excel-to-r/http:/r-statistics.co/screenshots/ggplot_masterlist_2.png" class="img-fluid"> <a href="https://r-statistics.co/Top50-Ggplot2-Visualizations-MasterList-R-Code.html">SOURCE</a></p>
<ol start="6" type="1">
<li><p>Scalability. As your data sets become larger, R scales with them. Excel becomes unwieldy.</p></li>
<li><p>Plug-in packages. This is the single biggest reason to drop spreadsheets. There are <a href="https://cran.r-project.org/web/packages/available_packages_by_name.html">hundreds of plug-in packages</a> for R that extend its analytic power. They can all be installed with a couple clicks. Any new task I want to perform starts with asking if there is a package that will do it for me. The answer is almost always yes. Further, my own education in data science has advanced by leaps and bounds as I’ve learned to use these powerful new analytic tools. I would go so far as to say this is a big career hack opportunity! If you are producing highly sophisticated analyses you are going to get noticed compared to the person that is confined to the primitive capabilities of Excel.</p></li>
</ol>
<p>At the end of the day the tool that gets the job done is the right tool. For me, R is a big step forward in efficiency, power and fun over Excel.</p>
<p>UPDATE June 2019: I originally wrote this note for an internal corporate blog in April 2017. Then, I recommended to NOT to use R for real-time live dashboards. I would amend that statement to say “it depends.” The Shiny interactive web framework from RStudio makes interactive <a href="https://www.rstudio.com/products/shiny/shiny-user-showcase/">dashboards look very good indeed</a>. Whether or not you can query live data APIs at the requisite frequency depends on the availability of a data feed, a package to grab it or your ability to write your own.</p>



 ]]></description>
  <category>R</category>
  <category>Excel</category>
  <guid>outsiderdata.netlify.app/posts/2019-06-12-why-i-migrated-from-excel-to-r/2019-06-12-why-i-migrated-from-excel-to-r.html</guid>
  <pubDate>Wed, 12 Jun 2019 04:00:00 GMT</pubDate>
  <media:content url="outsiderdata.netlify.app/posts/2019-06-12-why-i-migrated-from-excel-to-r/img/spreadsheet_mosaic.png" medium="image" type="image/png" height="65" width="144"/>
</item>
<item>
  <title>Solving the Letterboxed Puzzle in the New York Times</title>
  <dc:creator>Arthur Steinmetz</dc:creator>
  <link>outsiderdata.netlify.app/posts/2019-04-16-solving-the-letterboxed-puzzle-in-the-new-york-times/2019-04-16-solving-the-letterboxed-puzzle-in-the-new-york-times.html</link>
  <description><![CDATA[ 



<p>
What is the difference between “computer programming” and “data science?” To someone not invovled in either they look much the same. Most data scientists are also coders, though they don’t need to be. Data scientists (especially amateurs like me) don’t need to be concerned with pointers, stacks, heaps, recursion, etc., but this is not a data science post.
</p>
<p>
For this post, I go back to my roots in the 1980s as an amateur computer scientist to solve a new <em>New York Times</em> puzzle called “Letterboxed.” In particular I employ recursion to build a tree of possible solutions. This exercise reminded me how languages like R allow such easy plug-ins to high-powered algorithms written by “real” computer scientists in “real” languages like C++. Data scientists stand on the shoulders of giants who wrote the low-level code.
</p>
<p>
I’ll confess, I don’t like playing games and doing puzzles much. I also take the fun out of it for other people. When someone gave my kids “Cards Against Humanity” as a gift, I went through the deck and removed all the really filthy cards (the kids were relieved to see I left in plenty of poop references). When I see a puzzle I immediately think about an algorithm to play or solve it.
</p>
<div class="figure">
<img src="outsiderdata.netlify.app/posts/2019-04-16-solving-the-letterboxed-puzzle-in-the-new-york-times/img/sample_board.png" alt="Sample board from The Times">
<p class="caption">
Sample board from <em>The Times</em>
</p>
</div>
<p>
In “Letterboxed” the object is to string words together that use all the letters in the square using as few words as possible by tracing the spelling of each word in the square. You must start each new word with the last letter of the previous word and you may not use any consecutive letters that lie on the same side of the square. In the example above, “NIL” and “LAP” would not be permitted. “TORN” followed by “NEAR” would be fine.
</p>
<p>
Today we will forsake the usual data science workflow of: ask a question, source data, clean data, explore data and, finally, analyze data. Those proceed in a linear (in practice, circular) fashion but here we’ll go over the functions that do specific subroutines to generate and solve these puzzles.
</p>
<p>
The steps we’ll take are
</p>
<ol style="list-style-type: decimal">
<li>
Generate the puzzle with random letters.
</li>
<li>
Draw the board.
</li>
<li>
Solve the puzzle.
</li>
<li>
Print the answer that solves the puzzle in the fewest words.
</li>
</ol>
<p>
Generating the puzzle is the easy part.
</p>
<p>
The first task is to generate the puzzle with random letters. It would be cruel to place no requirement to use vowels so we also specify a minimum number of vowels. We sample the required number of consonants and vowels and assign them to each segment of the polygon. The default is four sides with two consonants and one vowel per side.
</p>
<p>
Just to be cute, let’s write the function so we can optionally expand the geometry of the puzzle to an arbitrary number of sides and number of letters per side, not just a square as we see in <em>The Times</em>.
</p>
<p>
If you are playing along at home, delete the <code>set.seed()</code> line in the code below after you have established you get the same results I do or you will get the same puzzle every time you call <code>generate_puzzle</code>.
</p>
<pre class="sourceCode r code-with-copy"><code># letterboxed game
library(tidyverse)
library(wfindr)
library(gtools)

sides &lt;- 4
letters_per_side &lt;- 3
vowels &lt;- c("a","e","i","o","u")
consonants &lt;- letters[!(letters %in% vowels)]

# scrabble dictionary from wfinder. You can subsitute any list
# you desire, in any language.
word_list &lt;- words.eng

# ------------------------------------------------------------
generate_puzzle &lt;- function(sides=4,letters_per_side=3,
                            vowel_count=4,replacement = FALSE){
  set.seed(1234567) # DELETE THIS LINE OR YOU WILL GET THE SAME PUZZLE EVERY TIME
  if(sides &lt; 4){
    print("Minimum Side is 4, changing to 4")
    sides = 4
  }
  if (vowel_count &lt; sides) replacement=TRUE
  if (vowel_count &gt; length(vowels)) replacement=TRUE
  use_vowels &lt;- sample(vowels,vowel_count,replace = replacement)
  use_consonants &lt;- sample(consonants,letters_per_side*sides-vowel_count,replace = replacement)
  # deal out the letters
  letter = NULL
  vowels_used = 1
  consonants_used = 1
  spot = 1
  for (i in 1:letters_per_side){
    for(j in 1:sides){
      # don't put vowel at edge of side but it's just cosmetic
      if (i == 2 &amp; vowels_used &lt;= vowel_count){
        letter[spot] &lt;- use_vowels[vowels_used]
        vowels_used &lt;- vowels_used + 1
        spot &lt;- spot + 1
      } else{
        letter[spot] &lt;- use_consonants[consonants_used]
        consonants_used &lt;- consonants_used + 1      
        spot &lt;- spot + 1
        
      }
    }
  }
  puzzle &lt;- tibble(side=rep(1:sides,letters_per_side),
                   spot=unlist(map(1:letters_per_side,rep,sides)), 
                   letter=letter) %&gt;% arrange(side,spot)
  return(puzzle)
}

# let's see what this does
generate_puzzle()</code></pre>
<pre><code>## # A tibble: 12 x 3
##     side  spot letter
##    &lt;int&gt; &lt;int&gt; &lt;chr&gt; 
##  1     1     1 v     
##  2     1     2 i     
##  3     1     3 l     
##  4     2     1 m     
##  5     2     2 u     
##  6     2     3 r     
##  7     3     1 c     
##  8     3     2 o     
##  9     3     3 y     
## 10     4     1 t     
## 11     4     2 a     
## 12     4     3 f</code></pre>
<p>
Now we have a data frame with twelve random letters, including four vowels, assigned to one of three spots on four sides.
</p>
<p>
It’s not necessary to solve the puzzle, but it would be nice to draw the puzzle in the style that appears in <em>The Times</em>. If all we needed to do was make a square the task of drawing it would be trivial but, as noted above, I can’t leave well enough alone. If we want to make polygons of arbitrary sizes we need to do a bit of trigonometry. First we generate the vertices of our polygon, then the points on each segment where the letters will go (as an aside, I say “vertices,” the proper Latin plural. The “newspaper of record” abandoned Latin plurals a decade ago. It grinds my gears to see the Times printing “vertexes”).
</p>
<pre class="sourceCode r code-with-copy"><code># -------------------------------------------------------------
get_polygon &lt;- function(sides=4){
  x_center &lt;- 0
  y_center &lt;- 0
  radius &lt;- 5
  y &lt;- NULL
  x &lt;- NULL
  angle = 3.925
  angle_increment &lt;-  2 * pi / sides
  for (i in 1:sides){
    x[i] = x_center + radius * cos(angle)
    y[i] = y_center + radius * sin(angle)
    angle = angle + angle_increment
  }
  #close figure
  x[i+1] &lt;- x[1]
  y[i+1] &lt;- y[1]
  return(data.frame(x=x,y=y))
}
# -------------------------------------------------------------
get_points_on_segment &lt;- function(end_points,num_points){
  # poin tdistance is fraction of segment length
  a &lt;- as.numeric(end_points[1,])
  b &lt;- as.numeric(end_points[2,])
  # Use atan2!
  th = atan2( b[2]-a[2] , b[1]-a[1] )
  # length of segment AB
  AB = sqrt( (b[2]-a[2])^2 + (b[1]-a[1])^2 )
  AB_fraction &lt;- AB / (num_points +1 )
  # points equidistant on the line
  AP = sapply(1:(num_points),function(x) x * AB_fraction)
  # The points of interest
  c = sapply(AP,function(d) c(x = a[1] + d*cos( th ),
                              y = a[2] + d*sin( th ))) %&gt;% 
    t() %&gt;%
    as.data.frame()
  return(c)
}
# -----------------------------------------------------
get_letter_coords &lt;- function(puzzle,sides=4,letters_per_side=3){
  
  puzzle_shape &lt;- get_polygon(sides)
  puzzle&lt;-lapply(1:(nrow(puzzle_shape)-1),
                     function(p) get_points_on_segment(puzzle_shape[p:(p+1),],
                                                       letters_per_side)) %&gt;% 
    bind_rows() %&gt;% 
    bind_cols(puzzle)
  return(puzzle)
}
# -------------------------------------------------------------
draw_puzzle &lt;-function(puzzle,sides=4,letters_per_side=3){
  puzzle_shape &lt;- get_polygon(sides)
  gg &lt;- puzzle_shape %&gt;% ggplot(aes(x,y)) + geom_path() + coord_fixed() +
    geom_point(data = puzzle,aes(x,y),size=20,color="white") + 
    geom_text(data = puzzle,aes(x,y,label = letter),size=10) + 
    theme_void() + 
    theme(panel.background = element_rect(fill="pink")) + 
    NULL 
return(gg)
}

# Draw puzzle sample
generate_puzzle() %&gt;%
  get_letter_coords(sides=sides,letters_per_side = letters_per_side) %&gt;% 
  draw_puzzle()</code></pre>
<p>
<img src="outsiderdata.netlify.app/posts/2019-04-16-solving-the-letterboxed-puzzle-in-the-new-york-times/img/unnamed-chunk-2-1.png" width="672">
</p>
<p>
Remember we designed the generator to work with arbitrary dimensions. Let’s try five sides with two letters per side.
</p>
<pre class="sourceCode r code-with-copy"><code>generate_puzzle(5,2) %&gt;%
  get_letter_coords(5,2) %&gt;% 
  draw_puzzle(5,2)</code></pre>
<p>
<img src="outsiderdata.netlify.app/posts/2019-04-16-solving-the-letterboxed-puzzle-in-the-new-york-times/img/unnamed-chunk-3-1.png" width="672">
</p>
<p>
Fun!
</p>
<section id="solve-the-puzzle" class="level1">
<h1>
Solve the Puzzle
</h1>
<p>
Much of the grunt work is done by the <code>wfinder</code> package, which generates a word list from an aribtrary set of letters, as in Scrabble. Unlike Scrabble, we CAN reuse the same letter more than once. This package also contains a list of English words we use. You can substitute any word list you like, in any language. My Mom, whose native language was German, was the champion in our family. I always struggled even though I liked to brag about my high SAT verbal score. I am grateful to Mom for knocking me down a peg. Anyhoo, I am really in awe of the power of the <code>grep</code> function. Regexes are a dark art to me. The idea that a short line could find every possible word in an instant boggles (don’t like that game either) the mind. Suppose you pull the Scrabble tiles “ABAHRTY”.
</p>
<pre class="sourceCode r code-with-copy"><code>grep("^[abahrty]*$",word_list,value = T)</code></pre>
<pre><code>##   [1] "aa"       "aah"      "ab"       "aba"      "abaya"    "abb"     
##   [7] "abba"     "abray"    "aby"      "ah"       "aha"      "ahh"     
##  [13] "ar"       "araba"    "arar"     "arb"      "arba"     "arhat"   
##  [19] "arrah"    "array"    "art"      "arty"     "ary"      "at"      
##  [25] "atar"     "att"      "attar"    "ay"       "ayah"     "ba"      
##  [31] "baa"      "baba"     "baby"     "bah"      "baht"     "bar"     
##  [37] "barb"     "barra"    "barrat"   "barratry" "baryta"   "bat"     
##  [43] "batata"   "bath"     "batt"     "batta"    "batty"    "bay"     
##  [49] "bayt"     "bra"      "brat"     "bratty"   "bray"     "brr"     
##  [55] "brrr"     "by"       "ha"       "haar"     "hah"      "haha"    
##  [61] "harry"    "hart"     "hat"      "hath"     "hay"      "rabat"   
##  [67] "rah"      "rat"      "rata"     "ratatat"  "rath"     "ratty"   
##  [73] "ray"      "raya"     "rayah"    "rhy"      "rhyta"    "rya"     
##  [79] "rybat"    "ta"       "tab"      "tabby"    "taha"     "tahr"    
##  [85] "tar"      "tara"     "tarry"    "tart"     "tartar"   "tarty"   
##  [91] "tat"      "tatar"    "tath"     "tatt"     "tatty"    "tay"     
##  [97] "tayra"    "thar"     "that"     "thy"      "trat"     "tratt"   
## [103] "tray"     "try"      "ya"       "yabby"    "yah"      "yar"     
## [109] "yarr"     "yarta"    "yatra"    "yay"</code></pre>
<p>
112 words out of a corpus of over 260 thousand. Instantly. That’s all the code it takes? That’s nuts! That’s efficient low-level coding. <code>wfindr</code> wraps that bit of magic with some bells and whistles to aid with word puzzles. In particular it crafts regexes that conform to the rules of scrabble. The example above creates a word list that might use more of a letter than we have in our tiles. To fix that, the simple regex I show above gets converted to a much fancier one.
</p>
<pre class="sourceCode r code-with-copy"><code>model_to_regex(allow="abahrty",type="scrabble")</code></pre>
<pre><code>## [1] "(?=^((([^a]*a[^a]*){1,2})|([^a]*))$)(?=^((([^b]*b[^b]*){1,1})|([^b]*))$)(?=^((([^h]*h[^h]*){1,1})|([^h]*))$)(?=^((([^r]*r[^r]*){1,1})|([^r]*))$)(?=^((([^t]*t[^t]*){1,1})|([^t]*))$)(?=^((([^y]*y[^y]*){1,1})|([^y]*))$)^[abhrty]*$"</code></pre>
<p>
Whoa! Like I said. It’s regex is a dark art.
</p>
<p>
Now we have all the possible words to use in the puzzle. Just throwing random words around from the solution set would eventually find some answers but we can do much better than that. To find the “best” next word, we can pick the word that has the most yet-unused letters. By default, the function below returns one word but it could return more. In practice, I found iterating through more words was rarely necessary to get a solution but drastically increased computation time and memory usage of the recursive function that calls it.
</p>
<pre class="sourceCode r code-with-copy"><code>find_next_best_words &lt;- function(w,needed_letters,max_return=1){
  # the higher max_return is the more words will be traversed.  Careful,
  # computation times will geometrically increase.
  # puzzle_words is global
  # find words that start with last letter of w
  next_words&lt;-puzzle_words[str_starts(puzzle_words,str_sub(w,-1))]
  # prioritize words by greatest overlap with unused letters
  next_word_chars &lt;-  map(next_words,strsplit,split="") %&gt;% unlist(recursive = F)
  temp &lt;- map(next_word_chars,function(x) length(setdiff(needed_letters,x))) %&gt;% unlist()
  if (is.vector(temp)){
    next_words &lt;- next_words[order(temp)]
    max_return &lt;- min(length(next_words),max_return)
    return(next_words[1:max_return])  
  } else{
    return()
  }
}
# -----------------------------------------------------
# check if we have used all the letters yet
test_needed_letters &lt;- function(word_chain){
  word_chain_chars &lt;-  paste0(word_chain,collapse = "") %&gt;% 
    strsplit(split="") %&gt;%
    unlist() %&gt;% 
    unique()
  return(setdiff(all_puzzle_letters,
                     word_chain_chars))
}</code></pre>
<p>
Now we come to the workhorse recursive function. “Recursive” just means it calls itself. I’ve learned the trick to recursive functions is getting out of them, otherwise you get deeper and deeper into the “Beyond” section of “Bed, Bath and Beyond” and run out of memory pretty quickly. At least nowadays you kids don’t have to worry about the whole machine crashing. You can just nuke the process that’s stuck.
</p>
<p>
We start by preparing to iterate <code>make_chain</code> over the full list of valid words. Naturally we expect to find a solution before traversing much of the list. We build the solution chain by choosing a word that ends with a letter that has not been an ending letter yet. Otherwise we might chase our tail forever if a solution doesn’t lie on that path. Then we pick the best next word as described above. Then we call <code>make_chain</code> again and again and again.
</p>
<p>
Here we limit the solution chain to a maximum of five words. Each time <code>make_chain</code> is called we run some tests and climb back out of the recursive stack if one of these conditions has been met:
</p>
<ol style="list-style-type: decimal">
<li>
The chain is more than five words with no solution.
</li>
<li>
A solution is found.
</li>
<li>
We run out of last letter/first letter possibilities
</li>
<li>
The are no next words found.
</li>
</ol>
<pre class="sourceCode r code-with-copy"><code>make_chain &lt;- function(word_chain,used_last_letters){
  needed_letters &lt;- test_needed_letters(word_chain)
  if (length(word_chain)&gt;6){
    # Come on, if you can't solve in 5 words, you suck!
    return()
  }
  if (length(needed_letters)==0) {
    # Yay! We have a solution.
    return(list(word_chain))
  }
  else {
    last_word &lt;- tail(word_chain,1)
    last_letter &lt;-str_sub(last_word,-1L)
    if (str_detect(used_last_letters,last_letter,negate=T)){
      used_last_letters &lt;- paste0(last_letter,used_last_letters,collapse = "")
      next_word&lt;-find_next_best_words(last_word,needed_letters,max_return=1)
       if (length(next_word)&gt;0){
        word_chain &lt;- make_chain(c(word_chain,next_word),used_last_letters)
        } else {
          # no next word found
          return()
        }
    } else{
      # start of next word would be a letter that has already been used
      return()
    }
  }
} </code></pre>
<p>
The function <code>solve_puzzle</code> is a wrapper around <code>make_chain</code> that first gets all the possible words that our letters allow, removing words that violate the rule of no consecutive letters from the same line. Note the use of the <code>&lt;&lt;–</code> assignment operator that accesses global variables from within functions. This practice is frowned upon in some circles but, since we are using nested recursion, I didn’t want to make new copies of every variable each time <code>make_chain</code> is called.
</p>
<pre class="sourceCode r code-with-copy"><code># dplyr chain-friendly permuatations
d_permute &lt;- function(v, n, r,  set, repeats.allowed){
  return(permutations(n, r, v, set, repeats.allowed))
}

get_line_combos &lt;- function(a_side,puzzle){
  combos &lt;- puzzle %&gt;% filter(side==a_side) %&gt;% 
    pull(letter) %&gt;% 
    as.character() %&gt;% 
    d_permute(n=3,r=2,set=F,repeats.allowed = T) %&gt;% 
    apply(1,paste0,collapse="")
  return(combos)
}


solve_puzzle &lt;- function (puzzle) {
  # get all letter combos that are invalid because they lie on the same line segment
  bans &lt;- map(1:sides,get_line_combos,puzzle=puzzle) %&gt;% unlist()
  #get all possible words
  puzzle_words &lt;&lt;- scrabble(paste0(puzzle$letter,collapse = ""),words=word_list)
  length(puzzle_words)
  #winnow out illegal ones
  banned_words &lt;- map(bans,function(x) puzzle_words[str_which(puzzle_words,x)]) %&gt;% 
    unlist()
  puzzle_words &lt;&lt;- puzzle_words[!(puzzle_words %in% banned_words)]
  length(puzzle_words)
  puzzle_words &lt;&lt;-puzzle_words[order(nchar(puzzle_words),decreasing = TRUE, puzzle_words)]
  
  
  all_puzzle_letters &lt;&lt;- puzzle$letter %&gt;% as.vector()
  
  solutions &lt;- map(puzzle_words,make_chain,"") %&gt;% unlist(recursive = F)
  return(solutions)
}</code></pre>
<p>
Whew! Now let’s actually solve a puzzle. The <code>solve_puzzle</code> function returns a list of lists with all the found solutions.
</p>
<pre class="sourceCode r code-with-copy"><code>vowel_count &lt;- sides
# global variables
all_puzzle_letters &lt;- NULL
puzzle_words &lt;- NULL
puzzle &lt;- generate_puzzle(sides=sides,
                          letters_per_side = letters_per_side,
                          vowel_count = vowel_count)
# add letter coordinates for plot
puzzle &lt;- get_letter_coords(puzzle,
                            sides=sides,
                            letters_per_side = letters_per_side)
#draw_puzzle(puzzle)
solutions &lt;- solve_puzzle(puzzle)

solutions %&gt;% head()</code></pre>
<pre><code>## [[1]]
## [1] "vortical" "loamy"    "yuca"     "aimful"  
## 
## [[2]]
## [1] "voracity" "ymolt"    "trayful" 
## 
## [[3]]
## [1] "foulmart" "trifoly"  "yuca"     "avoutry" 
## 
## [[4]]
## [1] "vacuity" "ymolt"   "trifoly"
## 
## [[5]]
## [1] "trayful" "lorica"  "avoutry" "ymolt"  
## 
## [[6]]
## [1] "flavory" "ymolt"   "toluic"</code></pre>
<p>
We may have hundreds of solutions or none. You can look at the <code>solutions</code> variable to see all we found. The goal of <em>The Times</em> puzzle is to solve in the minimum number of words so we’ll take the solution with the least number of words (there may be many) and print that on the puzzle.
</p>
<pre class="sourceCode r code-with-copy"><code># ---------------------------------------------------------
draw_solution &lt;- function(puzzle, solutions){
  if (is.null(solutions)) {
    solution &lt;- "No Solution"
  } else {
    ideal &lt;- map(solutions,length) %&gt;% unlist() %&gt;% which.min()
    solution &lt;- c(solutions[[ideal]],paste(length(solutions)-1,"other solutions")) 
  }
  gg &lt;- draw_puzzle(puzzle)
  gg &lt;- gg + annotate("text",x=0,y=0.9,label=paste(solution, collapse = "\n"), size = 6)
  print (gg)
}

draw_solution(puzzle, solutions)</code></pre>
<p>
<img src="outsiderdata.netlify.app/posts/2019-04-16-solving-the-letterboxed-puzzle-in-the-new-york-times/img/unnamed-chunk-10-1.png" width="672">
</p>
<p>
Let’s go back to the image at the top of this post which is from <em>The Times</em>. We’ll use those letters to solve an actual puzzle. Do the puzzle authors generate the puzzles randomly or do they work backword from a selected word list? I have no idea.
</p>
<pre class="sourceCode r code-with-copy"><code>sample_letters &lt;- "taperdnilyco"
puzzle &lt;- generate_puzzle() %&gt;% get_letter_coords()
#replace random letters with the one in the known puzzle
puzzle$letter &lt;- strsplit(sample_letters,split = NULL) %&gt;% unlist()
solutions &lt;- solve_puzzle(puzzle)
solutions %&gt;% head()</code></pre>
<pre><code>## [[1]]
## [1] "lectionary" "yealdon"    "noplace"   
## 
## [[2]]
## [1] "centroidal" "lectionary" "yipe"      
## 
## [[3]]
## [1] "rantipole" "etypical"  "leporid"  
## 
## [[4]]
## [1] "planetoid" "dielytra"  "article"  
## 
## [[5]]
## [1] "placitory" "yealdon"  
## 
## [[6]]
## [1] "clarionet" "torpidly"</code></pre>
<p>
We found 851 solutions to this particular puzzle, quite a few. Furthermore, If you are really good, you could solve this puzzle with two words!
</p>
<pre class="sourceCode r code-with-copy"><code>draw_solution(puzzle, solutions)</code></pre>
<p>
<img src="outsiderdata.netlify.app/posts/2019-04-16-solving-the-letterboxed-puzzle-in-the-new-york-times/img/unnamed-chunk-12-1.png" width="672">
</p>
<p>
There you have it. You might grumble that too many of the words in the scrabble dictionary are not in your vocabulary. They certainly aren’t in mine. Feel free to use a shorter word list with more common words. <a href="https://norvig.com/ngrams/">Here are a bunch</a>. That will increase the liklihood that no solution is found, though.
</p>
<p>
Further work that might be done would be to filter for completely unique solutions, with no overlapping words. Also we might create a Shiny application that does pretty animation drawing lines across the puzzle of the solution.
</p>
<p>
Naturally, you should only use this code to check your answer. No cheating!
</p>
</section>



 ]]></description>
  <category>puzzle</category>
  <category>recursion</category>
  <guid>outsiderdata.netlify.app/posts/2019-04-16-solving-the-letterboxed-puzzle-in-the-new-york-times/2019-04-16-solving-the-letterboxed-puzzle-in-the-new-york-times.html</guid>
  <pubDate>Tue, 16 Apr 2019 04:00:00 GMT</pubDate>
  <media:content url="outsiderdata.netlify.app/posts/2019-04-16-solving-the-letterboxed-puzzle-in-the-new-york-times/img/sample_board.png" medium="image" type="image/png" height="143" width="144"/>
</item>
<item>
  <title>Where Are The Libertarians?</title>
  <dc:creator>Art Steinmetz</dc:creator>
  <link>outsiderdata.netlify.app/posts/2019-02-07-where-are-the-libertarians/2019-02-07-where-are-the-libertarians.html</link>
  <description><![CDATA[ 



<section id="or" class="level3">
<h3 class="anchored" data-anchor-id="or">
<em>…or…</em>
</h3>
</section>
<section id="the-tough-road-ahead-for-howard-schultz" class="level3">
<h3 class="anchored" data-anchor-id="the-tough-road-ahead-for-howard-schultz">
<em>The Tough Road Ahead for Howard Schultz</em>
</h3>
</section>
<section id="or-1" class="level3">
<h3 class="anchored" data-anchor-id="or-1">
<em>…or…</em>
</h3>
</section>
<section id="my-preconceived-notions-are-shattered" class="level3">
<h3 class="anchored" data-anchor-id="my-preconceived-notions-are-shattered">
<em>My Preconceived Notions Are Shattered</em>
</h3>
<p>
At the risk of losing half my readers in the first paragraph, I’ll share my political views. Generally, I believe in “free people and free markets.” That makes me a small-L libertarian. I stress “generally” does not mean everywhere, all the time. I find it useful to think of, not a political spectrum from left to right, but a compass with four points. People have distinct views on social issues that may be separate from their views on government’s role in the economy. The traditional political party platforms don’t capture that nuance. There is no room in the Democratic or Republican Parties for a right-to-life advocate who also wants universal health care, for example.
</p>
<p>
I have always assumed that, free of the shackles of traditional party labels, most Americans are tolerant of people with different lifestyles than their own. At the same time we mostly believe that private markets do a better job of allocating resources than government bureaucrats. <em>YOU</em> may not believe that but, as I said, that’s been my working assumption about most people. We lean libertarian under the skin.
</p>
<p>
Last week the founder of Starbucks announced his candidacy for President. <a href="https://www.nytimes.com/2019/01/30/upshot/could-howard-schultz-help-re-elect-the-president.html">This article</a> in the <em>New York Times</em> suggested he has centrist appeal but that group might be “smaller than he realizes.” The article goes on to say:
</p>
<blockquote class="blockquote">
<p>
These dissatisfied centrist voters fit the profile of affluent, socially moderate and fiscally conservative suburban voters. They are twice as likely to make more than $100,000 per year than voters who have a favorable view of a party, and 78 percent of these voters say Democrats “too often see government as the only way to solve problems.”
</p>
</blockquote>
<blockquote class="blockquote">
<p>
Mr.&nbsp;Schultz could certainly play to these voters, but it is not a particularly electorally fruitful group. In an analysis of the Voter Study Group, Lee Drutman, a political scientist, found that just 4 percent of voters were conservative on economic issues and liberal on cultural issues. In comparison, populists represented 29 percent of the 2018 electorate. Mr.&nbsp;Schultz’s candidacy might be the reverse of Mr.&nbsp;Perot’s, but Mr.&nbsp;Perot’s pitch probably had broader appeal.
</p>
</blockquote>
<p>
Then I read the <a href="https://www.voterstudygroup.org/publications/2016-elections/political-divisions-in-2016-and-beyond">article by Lee Drutman</a> referenced by The Times and was discouraged to see little support for libertarian positioning in the 2016 survey of voters by <a href="YouGov.com" class="uri">YouGov.com</a>. Fortunately, the raw data is all available. This survey was updated in December 2018 and the data can be found <a href="https://www.voterstudygroup.org/publications/2018-voter-survey/2018-voter-survey-top-lines">here.</a>.
</p>
<p>
This survey looks like the perfect opportunity to see if my sensibilities are widely shared. In the process we will learn how to pick apart the VOTER data set. The huge number of questions are a rich trove for exploration by any political junkie. The survey results are a CSV file in a zipped package. The file is in my Github repo but, as a matter of courtesy, <em>please request it from The Voter Study Group</em>, as their usage terms stipulate. It’s free and easy.
</p>
<p>
A quick disclaimer: This is a recreational excercise. I am not a professional scientist, social, data or otherwise. This is “outsider” science. I welcome critiques from people who know what they are talking about.
</p>
<p>
Start by loading in the raw data. As a matter of style, I like to keep a raw data frame in pristine form and manipulate a copy so if I mess up I can always restart the data munging from the same base. If memory allows, additional intermediate versions might be helpful. As always, we will be working in the <strong>Tidyverse</strong> dialect.
</p>
<pre class="sourceCode r code-with-copy"><code># load libararies and files
library(tidyverse)
library(knitr)
voter_18_raw &lt;- read_csv("data/VOTER_Survey_April18_Release1.csv")</code></pre>
</section>
<section id="choose-the-questions-to-use" class="level2">
<h2 class="anchored" data-anchor-id="choose-the-questions-to-use">
Choose the Questions to Use
</h2>
<p>
While Mr.&nbsp;Drutman’s analysis of the survey did not show many libertarian-leaning voters, I hoped that selecting my own set of questions narrowly focused on the relevant issues might provide more support for my point of view. So, to be honest, I went into this with a preconceived notion of the answer I wanted to get. Beware.
</p>
<p>
Out of the dozens of questions the survey asked, I pulled out those which seemed to go to the separate dimensions of the conservative/liberal spectrum. The questions involved:
</p>
<p>
<strong>Fiscal Issues</strong>
</p>
<ul>
<li>
Trust of the government in Washington
</li>
<li>
Amount of regulation of business by the government
</li>
<li>
Importance of reducing the federal deficit
</li>
<li>
Role of government in economy
</li>
<li>
Desired third party position on economic issues
</li>
</ul>
<p>
<strong>Social Issues</strong>
</p>
<ul>
<li>
Difficulty of foreigners to immigrate to US
</li>
<li>
Gender Roles “Women belong in the kitchen!”
</li>
<li>
Views about the holy scriptures of own religion, literal truth?
</li>
<li>
Opinion on gay marriage
</li>
<li>
Public restroom usage of transgender people
</li>
<li>
View on abortion
</li>
<li>
Desired third party position on social and cultural issues
</li>
</ul>
</section>
<section id="pull-out-demographic-features" class="level2">
<h2 class="anchored" data-anchor-id="pull-out-demographic-features">
Pull Out Demographic Features
</h2>
<p>
Now we massage the raw data a few ways. First we <code>gather()</code> the data to group the interesting demographic features as separate variables and tidy up all the remaining questions and answers into two variables.
</p>
<pre class="sourceCode r code-with-copy"><code>voter_18&lt;- gather(voter_18_raw,"question","answer",
                  -caseid,
                  -pid3_2018,
                  -race_2018,
                  -gender_2018,
                  -faminc_new_2018,
                  -inputstate_2018
                  ) %&gt;% 
  as_tibble() %&gt;%
  filter(!is.na(caseid)) %&gt;% 
  filter(!is.na(answer)) %&gt;% 
  distinct()

# labels of the questions we want to keep, with a (f)iscal or (s)ocial tag
questions_to_keep &lt;-  read_csv(
  "axis_flag,question\n
  f,trustgovt_2018\n
  s,immi_makedifficult_2018\n
  f,tax_goal_federal_2018\n
  f,govt_reg_2016\n
  s,sexism1_2018\n
  s,holy_2018\n
  s,gaymar_2016\n
  s,abortview3_2016\n
  s,third_soc_2018\n
  f,third_econ_2018\n
  f,gvmt_involment_2016\n",trim_ws=T)

voter_18 &lt;- voter_18 %&gt;% filter(question %in% questions_to_keep$question)

voter_18 &lt;- voter_18 %&gt;% mutate(answer=as.numeric(answer))
# make demographic variables factors
voter_18 &lt;- voter_18 %&gt;%
  mutate(caseid =as.character(caseid)) %&gt;% 
  mutate(gender_2018=as.factor(gender_2018)) %&gt;% 
  mutate(race_2018=as.factor(race_2018)) %&gt;% 
  mutate(faminc_new_2018=as.factor(faminc_new_2018)) %&gt;% 
  mutate(pid3_2018=as.factor(pid3_2018)) %&gt;% 
  rename(party_2018=pid3_2018) %&gt;% 
  rename(state_2018=inputstate_2018) %&gt;% 
  rename(income_2018=faminc_new_2018)
  
#map state numbers to state abbreviations
state_plus &lt;- c(state.abb[1:8],"DC",state.abb[9:50])
voter_18$state_2018 &lt;- factor(voter_18$state_2018)
levels(voter_18$state_2018) &lt;- state_plus


levels(voter_18$gender_2018) &lt;- c("Male","Female")
levels(voter_18$race_2018) &lt;- c("White","Black","Hispanic",
                                "Asian","Native Amerian","Mixed",
                                "Other","Middle Eastern")

levels(voter_18$party_2018) &lt;- c("Democrat","Republican","Independent",
                                "Other","Not Sure")
#Make human-readable income column
income_key&lt;-read_csv(
  "Response,Label\n
  1, Less than $10\n
  2, $10 - $19\n
  3,  $20 - $29\n
  4,  $30 - $39\n
  5,  $40 - $49\n
  6,  $50 - $59\n
  7,  $60 - $69\n
  8,  $70 - $79\n
  9,  $80 - $99\n
  10,   $100 - $119\n
  11,   $120 - $149\n
  12,   $150 - $199\n
  13,   $200 - $249\n
  14,   $250 - $349\n
  15,   $350 - $499\n
  16,   $500 or more\n
  97,   Prefer not to say\n"
  ,col_types = "ff",trim_ws = TRUE)

voter_18 &lt;- voter_18 %&gt;% mutate(income_2018_000=income_2018)
levels(voter_18$income_2018_000)&lt;-levels(income_key$Label)

# now make income_2018 continuous again, keeping income_2018_000 as a factor
# for labeling
# "Prefer not to say" (coded as 97) is set to NA.   
voter_18 &lt;- voter_18 %&gt;% mutate(income_2018=ifelse(income_2018==97,NA,income_2018)) %&gt;%
                    mutate(income_2018=as.numeric(income_2018))
voter_18[1:10,]</code></pre>
<pre><code>## # A tibble: 10 x 9
##    caseid gender_2018 race_2018 income_2018 party_2018 state_2018 question
##    &lt;chr&gt;  &lt;fct&gt;       &lt;fct&gt;           &lt;dbl&gt; &lt;fct&gt;      &lt;fct&gt;      &lt;chr&gt;   
##  1 38248~ Female      Hispanic            7 Democrat   CA         trustgo~
##  2 38216~ Female      White               8 Republican AZ         trustgo~
##  3 38216~ Male        White               6 Independe~ WI         trustgo~
##  4 38233~ Male        White               7 Republican TX         trustgo~
##  5 38248~ Female      White               5 Democrat   CA         trustgo~
##  6 38329~ Male        White              NA Republican WI         trustgo~
##  7 38222~ Female      White               3 Democrat   VT         trustgo~
##  8 38233~ Female      White              12 Independe~ FL         trustgo~
##  9 38226~ Female      White               9 Democrat   AZ         trustgo~
## 10 38216~ Female      White               6 Independe~ NE         trustgo~
## # ... with 2 more variables: answer &lt;dbl&gt;, income_2018_000 &lt;fct&gt;</code></pre>
<pre class="sourceCode r code-with-copy"><code># We did a lot of work.  Save it.
save(voter_18,file="data/voter_18.rdata")
# free up 30mb of memory
rm(voter_18_raw)</code></pre>
<p>
Look at some of the demographics.
</p>
<pre class="sourceCode r code-with-copy"><code>demographics &lt;- voter_18 %&gt;% 
  distinct(caseid,.keep_all = TRUE) %&gt;% 
  select(-question,-answer)

demographics %&gt;% group_by(gender_2018) %&gt;%
  summarise(count=n()) %&gt;% kable()</code></pre>

<table class="table">
<thead>
<tr class="header">
<th align="left">
gender_2018
</th>
<th align="right">
count
</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">
Male
</td>
<td align="right">
2762
</td>
</tr>
<tr class="even">
<td align="left">
Female
</td>
<td align="right">
3239
</td>
</tr>
</tbody>

</table>
<pre class="sourceCode r code-with-copy"><code>demographics %&gt;%
  ggplot(aes(race_2018))+geom_bar()+coord_flip() + 
  labs(caption = "Source: VoterStudyGroup.org")</code></pre>
<p>
<img src="outsiderdata.netlify.app/posts/2019-02-07-where-are-the-libertarians/img/unnamed-chunk-4-1.png" width="672">
</p>
</section>
<section id="rescale-answers-for-consistency" class="level2">
<h2 class="anchored" data-anchor-id="rescale-answers-for-consistency">
Rescale Answers for Consistency
</h2>
<p>
The final step in massaging the data is to rescale all the question answers to between one and minus one, interpreted as liberal to conservative, respectively, in two dimensions. “Don’t know” (usually coded as 8) is treated as neutral (zero). If the question is “fiscal”, set “social” to <code>NA</code> and vice versa.
</p>
<pre class="sourceCode r code-with-copy"><code>#add two new columns to hold scaled answers.
voter_18_scaled&lt;-voter_18 %&gt;% mutate(fiscal=NA,social=NA)
# -1 is fiscal liberal
voter_18_scaled &lt;- voter_18_scaled %&gt;% 
  mutate(fiscal=ifelse(question=="trustgovt_2018",-(answer-2),fiscal))

# -1 is social liberal
voter_18_scaled &lt;- voter_18_scaled %&gt;%
  mutate(social=ifelse(question=="immi_makedifficult_2018",(answer-3)*0.5,social))
voter_18_scaled &lt;- voter_18_scaled %&gt;%
  mutate(social=ifelse(question=="immi_makedifficult_2018",
                       ifelse(answer==8,0,social),social))

# -1 is fiscal liberal
voter_18_scaled &lt;- voter_18_scaled %&gt;%
  mutate(fiscal=ifelse(question=="tax_goal_federal_2018",(answer-2.5)*-(2/3),fiscal))

# -1 is fiscal liberal
voter_18_scaled &lt;- voter_18_scaled %&gt;%
  mutate(fiscal=ifelse(question=="govt_reg_2016",-(answer-2),fiscal))
voter_18_scaled &lt;- voter_18_scaled %&gt;%
  mutate(fiscal=ifelse(question=="govt_reg_2016",
                       ifelse(answer==8,0,fiscal),fiscal))

# -1 is social liberal
voter_18_scaled &lt;- voter_18_scaled %&gt;%
  mutate(social=ifelse(question=="sexism1_2018",(answer-2.5)*-(2/3),social))

# -1 is social liberal
voter_18_scaled &lt;- voter_18_scaled %&gt;%
  mutate(social=ifelse(question=="holy_2018",-(answer-2),social))

# -1 is social liberal
voter_18_scaled &lt;- voter_18_scaled %&gt;%
  mutate(social=ifelse(question=="gaymar_2016",(answer-1.5)*2,social))
voter_18_scaled &lt;- voter_18_scaled %&gt;%
  mutate(social=ifelse(question=="gaymar_2016",
                       ifelse(answer==8,0,social),social))

# -1 is social liberal
voter_18_scaled &lt;- voter_18_scaled %&gt;%
  mutate(social=ifelse(question=="view_transgender_2016",(answer-1.5)*2,social))
voter_18_scaled &lt;- voter_18_scaled %&gt;%
  mutate(social=ifelse(question=="view_transgender_2016",
                       ifelse(answer==8,0,social),social))

# -1 is social liberal
voter_18_scaled &lt;- voter_18_scaled %&gt;%
  mutate(social=ifelse(question=="abortview3_2016",(answer-2),social))
voter_18_scaled &lt;- voter_18_scaled %&gt;%
  mutate(social=ifelse(question=="abortview3_2016",
                       ifelse(answer==8,0,social),social))

# -1 is social liberal
voter_18_scaled &lt;- voter_18_scaled %&gt;%
  mutate(social=ifelse(question=="third_soc_2018",(answer-3)*0.5,social))

# -1 is fiscal liberal
voter_18_scaled &lt;- voter_18_scaled %&gt;%
  mutate(fiscal=ifelse(question=="third_econ_2018",(answer-3)*0.5,fiscal))

# -1 is fiscal liberal
voter_18_scaled &lt;- voter_18_scaled %&gt;%
  mutate(fiscal=ifelse(question=="gvmt_involment_2016",(answer-1),fiscal))
voter_18_scaled &lt;- voter_18_scaled %&gt;%
  mutate(fiscal=ifelse(question=="gvmt_involment_2016",
                       ifelse(answer==8,0,fiscal),fiscal))

# We did a lot of work.  Save it.
save(voter_18_scaled,file="data/voter_18_scaled.rdata")</code></pre>
<p>
Now we have values that we can aggregate for each question. They are all normalized and given equal weight. Should each question be given equal weight? I don’t know, but now we can compute average scores for each <code>caseid</code> (each one is one voter) . We also add the demographic features to each observation. So now every <code>caseid</code> in the survey is assigned a separate fiscal and social temperament score.
</p>
<pre class="sourceCode r code-with-copy"><code>scores &lt;- voter_18_scaled %&gt;% 
  group_by(caseid) %&gt;% 
  summarise(social=mean(social,na.rm = T),fiscal=mean(fiscal,na.rm = T)) %&gt;% 
  left_join(demographics)   #Add demographics to scores</code></pre>
<p>
Let’s start off at the highest level. What are the mean values for each dimension?
</p>
<pre class="sourceCode r code-with-copy"><code>mean_social &lt;- mean(scores$social,na.rm = T)
mean_fiscal &lt;- mean(scores$fiscal,na.rm = T)
print(paste("Mean Fiscal=",round(mean_fiscal,2)))</code></pre>
<pre><code>## [1] "Mean Fiscal= 0.06"</code></pre>
<pre class="sourceCode r code-with-copy"><code>print(paste("Mean Social=",round(mean_social,2)))</code></pre>
<pre><code>## [1] "Mean Social= -0.16"</code></pre>
<p>
Well that is an encouraging start. The signs are in the libertarian quadrant, anyway, but are they statistically significant? Specifically, can we reject the hypothesis that the true mean is greater than zero for social, and less than zero for fiscal?
</p>
<pre class="sourceCode r code-with-copy"><code>t_s &lt;-t.test(scores$social,mu=0,conf.level = 0.95,alternative="greater") %&gt;% broom::tidy()
t_f &lt;-t.test(scores$fiscal,mu=0,conf.level = 0.95,alternative="less") %&gt;% broom::tidy()
t_both&lt;-bind_cols(Dimension=c("Social","Fiscal"),bind_rows(t_s,t_f)) %&gt;% 
  select(Dimension,estimate,statistic,conf99.low=conf.low,conf99.high=conf.high)
t_both</code></pre>
<pre><code>## # A tibble: 2 x 5
##   Dimension estimate statistic conf99.low conf99.high
##   &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;
## 1 Social     -0.157      -24.6     -0.167    Inf     
## 2 Fiscal      0.0585      11.4   -Inf          0.0669</code></pre>
<p>
With such a large sample size we can be pretty confident that the true mean is close to the sample mean and therefore leans libertarian. Alas, that is not enough to form an opinion. The magnitudes are still very small and a slight relative shift in the aggregate may not support my hypothesis that most people have a libertarian bias when you break down the issues. Further, we haven’t even touched on the survey methodology. It is an online survey and therefore means the respondents have computers and are facile with internet access. That population is closer and closer to “everyone” with each passing day but is still not universal.
</p>
<p>
With our data all cleaned up, let’s look at some pictures!
</p>
<pre class="sourceCode r code-with-copy"><code>gg &lt;- ggplot(scores,aes(fiscal,social)) + geom_point()
gg &lt;- gg +  geom_jitter(width=0.05,height=0.05)
gg &lt;- gg + geom_hline(yintercept = 0,color="red")
gg &lt;- gg + geom_vline(xintercept = 0,color="red")
gg &lt;- gg + annotate("text",label=c("Libertarian"),x=0.9,y=-0.9,color="red")
gg &lt;- gg + labs(title="Separation of Social and Fiscal Values",
                y = "Social Score (Lower=More Liberal)",
                x = "Fiscal Score (Lower=More Liberal)",
                caption = "Source: VoterStudyGroup.org")
gg &lt;- gg + annotate("text",x=-0.7,y=1.0,color="red",
                    label=paste("Mean Fiscal=",round(mean_fiscal,2),
                                "Mean Social=",round(mean_social,2)))
gg &lt;- gg + geom_smooth(method="lm")
gg</code></pre>
<p>
<img src="outsiderdata.netlify.app/posts/2019-02-07-where-are-the-libertarians/img/unnamed-chunk-9-1.png" width="672">
</p>
<p>
The first thing to note is the values are all over the chart. We’ve added some random “jitter” noise to the position of each point with <code>geom_jitter()</code>. Otherwise, many of the points would overlap and obscure the density of the points. Even so, careful scrutiny of of the standard error range around the regression line shows that a huge number of points lie very close to the line.
</p>
<p>
Sadly, for a libertarian, the scores tend to line up close to the 45 degree axis, which means people who are more socially conservative are more likely to be fiscally conservative as well. The libertarian quadrant is the lower right, which is more sparsely populated.
</p>
<pre class="sourceCode r code-with-copy"><code>lm(scores$social~scores$fiscal) %&gt;% broom::tidy()</code></pre>
<pre><code>## # A tibble: 2 x 5
##   term          estimate std.error statistic   p.value
##   &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;
## 1 (Intercept)     -0.200   0.00518     -38.5 5.39e-290
## 2 scores$fiscal    0.732   0.0129       56.7 0.</code></pre>
<p>
Let’s count voter incidence in each quadrant.
</p>
<pre class="sourceCode r code-with-copy"><code>#call zero scores "Neutral"
scores &lt;- scores %&gt;% 
  mutate(fiscal_label=cut(scores$fiscal,c(-1,-0.0001,0.0001,1),
                      labels=c("Liberal","Neutral","Conservative"))) %&gt;% 
  mutate(social_label=cut(scores$social,c(-1,-0.01,0.01,1),
                      labels=c("Liberal","Neutral","Conservative")))

xtabs(~fiscal_label+social_label,scores) %&gt;% 
  as_tibble() %&gt;% 
  arrange(desc(n)) %&gt;% 
  filter(fiscal_label != "Neutral",social_label != "Neutral")</code></pre>
<pre><code>## # A tibble: 4 x 3
##   fiscal_label social_label     n
##   &lt;chr&gt;        &lt;chr&gt;        &lt;int&gt;
## 1 Liberal      Liberal       1903
## 2 Conservative Conservative  1745
## 3 Conservative Liberal       1046
## 4 Liberal      Conservative   387</code></pre>
<p>
The largest quadrant is Liberal/Liberal followed by Conservative/Conservative. Leaving out the neutral axes, the libertarian quadrant (liberal social, conservative fiscal) is third with a respectable number of respondents. This is about 18% of the sample, far more than the 4% Mr.&nbsp;Drutman found. The liberal fiscal, conservative social quadrant, which is populist I suppose, includes the fewest voters.
</p>
<p>
This is suggestive of traditional party platforms so how does this look broken out by party?
</p>
<pre class="sourceCode r code-with-copy"><code>gg &lt;-ggplot(scores,aes(fiscal,social,color=party_2018))+geom_point()
gg &lt;- gg +  geom_jitter(width=0.05,height=0.05)
gg &lt;- gg + geom_hline(yintercept = 0)
gg &lt;- gg + geom_vline(xintercept = 0)
gg &lt;- gg + annotate("text",label=c("Libertarian"),x=0.9,y=-0.9)
gg &lt;- gg + scale_color_manual(values=c(Republican='#e41a1c',
                                       Democrat='#377eb8',
                                       Independent='#4daf4a',
                                       Other='#984ea3',
                                       `Not Sure`='#ff7f00'))
gg &lt;- gg + labs(title="Party Lines Align With Temperament",
                y = "Social Score (Lower=More Liberal)",
                x = "Fiscal Score (Lower=More Liberal)",
                caption = "Source: VoterStudyGroup.org")
gg</code></pre>
<p>
<img src="outsiderdata.netlify.app/posts/2019-02-07-where-are-the-libertarians/img/unnamed-chunk-12-1.png" width="672">
</p>
<p>
There is a clear bifurcation around party, which is exactly what we’d expect.
</p>
<p>
The survey respondents are overwhelmingly white. What does the plot look like if we remove them from data set?
</p>
<pre class="sourceCode r code-with-copy"><code>gg &lt;- scores %&gt;% filter(race_2018 != "White") %&gt;% 
  ggplot(aes(fiscal,social,color=race_2018))+geom_point()
gg &lt;- gg +  geom_jitter(width=0.05,height=0.05)
gg &lt;- gg + geom_hline(yintercept = 0)
gg &lt;- gg + geom_vline(xintercept = 0)
gg &lt;- gg + annotate("text",label=c("Libertarian"),x=0.9,y=-0.9)
gg &lt;- gg + labs(title="Minorities Are Not Too Different from Whites",
                y = "Social Score (Lower=More Liberal)",
                x = "Fiscal Score (Lower=More Liberal)",
                caption = "Source: VoterStudyGroup.org")
gg</code></pre>
<p>
<img src="outsiderdata.netlify.app/posts/2019-02-07-where-are-the-libertarians/img/unnamed-chunk-13-1.png" width="672">
</p>
<p>
The sub sample above looks very similar to the whole data set. Black voters do skew more to the Liberal/Liberal side but Hispanic voters do not.
</p>
<p>
Let’s meet some individuals. Who are the folks who show strong libertarian sentiments (greater than 0.5 social, less than -0.5 fiscal), all nineteen of them?
</p>
<pre class="sourceCode r code-with-copy"><code>scores %&gt;% filter(fiscal &lt; (0.5),social &gt; (-0.5)) %&gt;% select(gender_2018,race_2018,party_2018,income_2018_000,state_2018)</code></pre>
<pre><code>## # A tibble: 2,984 x 5
##    gender_2018 race_2018 party_2018  income_2018_000 state_2018
##    &lt;fct&gt;       &lt;fct&gt;     &lt;fct&gt;       &lt;fct&gt;           &lt;fct&gt;     
##  1 Male        White     Independent $150 - $199     MI        
##  2 Male        White     Independent $60 - $69       SD        
##  3 Male        White     Republican  $100 - $119     OK        
##  4 Female      White     Republican  $10 - $19       WI        
##  5 Male        White     Republican  $30 - $39       CA        
##  6 Female      White     Republican  $40 - $49       WA        
##  7 Female      White     Republican  $10 - $19       IN        
##  8 Female      Hispanic  Democrat    $50 - $59       NY        
##  9 Female      White     Independent $30 - $39       MD        
## 10 Female      White     Independent $120 - $149     MA        
## # ... with 2,974 more rows</code></pre>
<p>
These folks are almost all white, but our set is a tiny sub sample so I doubt any generalizations are significant. There is only one Democrat in the bunch. They are not rich and they’re spread around the country. They are men and women.
</p>
<p>
We have a number of additional demographic variables but let’s just look at one more of them. How do scores look conditioned on income?
</p>
<pre class="sourceCode r code-with-copy"><code>gg &lt;- scores %&gt;% filter(!is.na(income_2018)) %&gt;%
  ggplot(aes(income_2018_000,fiscal,group=income_2018)) + geom_boxplot()
gg &lt;- gg + coord_flip() + theme(axis.text.x = element_text(angle=-90))
gg &lt;- gg + geom_hline(yintercept = 0,color="red")
gg &lt;- gg + labs(title="Higher Income Does  Not Make a Fiscal Conservative",
                x = "Annual Income ($000)",
                y = "Fiscal Score (Lower=More Liberal)",
                caption = "Source: VoterStudyGroup.org")
gg</code></pre>
<p>
<img src="outsiderdata.netlify.app/posts/2019-02-07-where-are-the-libertarians/img/unnamed-chunk-15-1.png" width="672">
</p>
<p>
Surprisingly, to me, there is no trend to prefer less government as income rises. The desire for government involvement in the economy is close to neutral across all income cohorts. Note, I did not include any tax questions for this measure. People are happy to favor higher taxes on anybody who makes more money than they do.
</p>
<pre class="sourceCode r code-with-copy"><code>gg &lt;- scores %&gt;% filter(!is.na(income_2018)) %&gt;%
  ggplot(aes(income_2018_000,social,group=income_2018))+ geom_boxplot()

gg &lt;- gg + coord_flip() + theme(axis.text.x = element_text(angle=-90))
gg &lt;- gg + geom_hline(yintercept = 0,color="red")
gg &lt;- gg + labs(title="Higher Income Does Make One More Socially Liberal",
                x = "Annual Income ($000)",
                y = "Social Score (Lower=More Liberal)",
                caption = "Source: VoterStudyGroup.org")
gg</code></pre>
<p>
<img src="outsiderdata.netlify.app/posts/2019-02-07-where-are-the-libertarians/img/unnamed-chunk-16-1.png" width="672">
</p>
<p>
There is some association with more socially liberal views as income rises. The richer you are, the more tolerant you are of other’s lifestyles, I guess. During the 2016 election there was some questioning around why poor people (mainly rural whites) voted “against their economic interest.” This suggests that voting WITH their conservative social interests was more important (I am not saying that our current president embodies conservative social values). <a href="https://www.politico.com/magazine/story/2017/12/31/trump-white-working-class-history-216200">Most pundits put a racial angle on this</a>. In all income cohorts the median voter is at least a shade liberal on social issues.
</p>
</section>
<section id="how-much-does-location-matter" class="level2">
<h2 class="anchored" data-anchor-id="how-much-does-location-matter">
How Much Does Location Matter?
</h2>
<p>
Let’s look at average scores by state. To remind us of the influence that larger states have on the overall numbers we’ll grab population data from the Census Bureau. There are a number of R packages to access the census API but those are more than we need and require an API key. Here, we’ll just grab a summary CSV file from the web site.
</p>
<pre class="sourceCode r code-with-copy"><code># download population summary from census bureau
state_pop_raw&lt;-read_csv("https://www2.census.gov/programs-surveys/popest/datasets/2010-2018/national/totals/nst-est2018-alldata.csv")</code></pre>
<pre><code>## Parsed with column specification:
## cols(
##   .default = col_double(),
##   SUMLEV = col_character(),
##   REGION = col_character(),
##   DIVISION = col_character(),
##   STATE = col_character(),
##   NAME = col_character()
## )</code></pre>
<pre><code>## See spec(...) for full column specifications.</code></pre>
<pre class="sourceCode r code-with-copy"><code>save(state_pop_raw,file="data/state_pop_raw.rdata")
load("data/state_pop_raw.rdata")
# filter to keep only state level data and add abbreviations
# manually insert District of Columbia as a state
state_pop &lt;- state_pop_raw %&gt;% 
  transmute(state=NAME,population_2018=POPESTIMATE2018) %&gt;%
  filter(state %in% c(state.name,"District of Columbia")) %&gt;%
  bind_cols(state_2018=as_factor(state_plus))

gg &lt;- scores %&gt;% group_by(state_2018) %&gt;% 
  summarize(fiscal=mean(fiscal,na.rm = T),social=mean(social,na.rm = T)) %&gt;%
  left_join(state_pop, by = "state_2018") %&gt;% 
  ggplot(aes(fiscal,social)) + geom_point(aes(color=population_2018,
                                              size=population_2018))
gg &lt;- gg + ggrepel::geom_text_repel(aes(label=state_2018))
gg &lt;- gg + scale_size(trans="log10",
                      labels=c("0","1 mm","3 mm","10 mm","30 mm","More"))
gg &lt;- gg + scale_color_gradient(trans="log10",
                                labels=c("0","1 mm","3 mm","10 mm","30 mm","More"))
gg &lt;- gg + geom_hline(yintercept = 0)
gg &lt;- gg + geom_vline(xintercept = 0)
gg &lt;- gg + annotate("text",label=c("Libertarian"),x=0.15,y=-0.4)
gg &lt;- gg + labs(title="Separation of Social and Fiscal Values",
                y = "Social Score (Lower=More Liberal)",
                x = "Fiscal Score (Lower=More Liberal)",
                caption = "Source: VoterStudyGroup.org")

gg</code></pre>
<p>
<img src="outsiderdata.netlify.app/posts/2019-02-07-where-are-the-libertarians/img/unnamed-chunk-17-1.png" width="672">
</p>
<p>
If I had created this chart first I might have been excited. It shows that the average voter in most states is in the libertarian quadrant. That is NOT the same thing as saying most voters in the “libertarian” states are libertarian. We already showed that the vast majority of voters fall outside the libertarian quadrant. Still, there are some interesting things to note. The fiscal sentiments of New Hampshire voters are far different than their Vermont neighbors. I don’t see Bernie Sanders sporting this license plate:
</p>
<div class="figure">
<img src="outsiderdata.netlify.app/posts/2019-02-07-where-are-the-libertarians/img/live-free-or-die.jpg" alt="Live Free or Die" width="400">
<p class="caption">
Live Free or Die
</p>
</div>
<p>
By the way, I wish I knew how to get color and size combined into one legend.
</p>
</section>
<section id="my-last-attempt-at-validation" class="level2">
<h2 class="anchored" data-anchor-id="my-last-attempt-at-validation">
My Last Attempt at Validation
</h2>
<p>
I went through the <em>YouGov.com</em> survey and picked out the questions I feel are relevant, a highly subjective exercise. Even so,the results do not support my belief that maybe a plurality of people have libertarian sensibilities. But there were hints that gave me some hope.
</p>
<p>
First, there is a clear yearning for a choice beyond the existing parties as this question shows:
</p>
<p>
<em>In your view, do the Republican and Democratic parties do an adequate job of representing the American people, or do they do such a poor job that a third major party is needed?</em>
</p>

<table class="table">
<thead>
<tr class="header">
<th align="right">
Count
</th>
<th>
Answer
</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">
1,851
</td>
<td>
Do adequate job
</td>
</tr>
<tr class="even">
<td align="right">
4,036
</td>
<td>
Third party is needed
</td>
</tr>
</tbody>

</table>
<p>
The fact that most people want another choice tells us nothing about what that choice is. Another question does seem to suggest libertarian economic sentiment in excess of what the number of Republicans might indicate:
</p>
<p>
<em>In general, do you think there is too much or too little regulation of business by the government?</em>
</p>

<table class="table">
<thead>
<tr class="header">
<th align="right">
Count
</th>
<th>
Answer
</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">
3,473
</td>
<td>
Too much
</td>
</tr>
<tr class="even">
<td align="right">
1,628
</td>
<td>
About the right amount
</td>
</tr>
<tr class="odd">
<td align="right">
1,999
</td>
<td>
Too little
</td>
</tr>
<tr class="even">
<td align="right">
871
</td>
<td>
Don’t know
</td>
</tr>
</tbody>

</table>
<p>
Finally, there are two questions in the survey that go explicitly to the separation of social and fiscal values.
</p>
<p>
<em>1. If you were to vote for a new third party, where would you like it to stand on social and cultural issues—like abortion and same-sex marriage?</em>
</p>
<p>
<em>2. If you were to vote for a new third party, where would you like it to stand on economic issues—like how much the government spends and how many services it provides?</em>
</p>
<p>
The range of answers for both is:
</p>

<table class="table">
<thead>
<tr class="header">
<th align="right">
Score
</th>
<th>
Answer
</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">
1.0
</td>
<td>
More liberal than the Democratic Party
</td>
</tr>
<tr class="even">
<td align="right">
0.5
</td>
<td>
About where the Democratic Party is now
</td>
</tr>
<tr class="odd">
<td align="right">
0.0
</td>
<td>
In between the Democratic Party and the Republican Party
</td>
</tr>
<tr class="even">
<td align="right">
-0.5
</td>
<td>
About where the Republican Party is now
</td>
</tr>
<tr class="odd">
<td align="right">
-1.0
</td>
<td>
More conservative than the Republican Party
</td>
</tr>
</tbody>

</table>
<p>
Let’s re-do the scatter based on the answers to just those two questions. Since we are using only two questions with possible values of only 1,0 and minus 1, there are many more respondents than possible values. Again we add some random jitter to make the density clear. Every dot within each square is actually the same value. The result is a visual cross tab. I quite like the effect.
</p>
<pre class="sourceCode r code-with-copy"><code>scores_narrow &lt;- voter_18_scaled %&gt;% 
  filter(str_detect(question,"third_")) %&gt;%  
  group_by(caseid) %&gt;% 
  summarise(social=mean(social,na.rm = T),fiscal=mean(fiscal,na.rm = T)) %&gt;% 
  left_join(demographics)

gg &lt;- ggplot(scores_narrow,aes(fiscal,social,color=party_2018))+geom_point() + geom_jitter()
gg &lt;- gg + geom_hline(yintercept = 0)
gg &lt;- gg + geom_vline(xintercept = 0)
gg &lt;- gg + labs(title="What Kind of Third Party Would Voters Prefer?",
                y = "Social Score (Lower=More Liberal)",
                x = "Fiscal Score (Lower=More Liberal)",
                caption = "Source: VoterStudyGroup.org")
gg &lt;- gg + annotate("text",label=c("Libertarian"),x=0.9,y=-0.9,fontface="bold")
gg &lt;- gg + annotate("text",label=c("Populist?"),x=-0.9,y=0.9,fontface="bold")
gg &lt;- gg + annotate("text",label=c("Left of Democrats"),x=-0.9,y=-0.9,fontface="bold")
gg &lt;- gg + annotate("text",
                    label=c("Right of Republicans"),
                    x=0.9,y=0.9,
                    fontface="bold")
gg &lt;- gg + scale_color_manual(values=c(Republican='#e41a1c',
                                       Democrat='#377eb8',
                                       Independent='#4daf4a',
                                       Other='#984ea3',
                                       `Not Sure`='#ff7f00'))

gg</code></pre>
<p>
<img src="outsiderdata.netlify.app/posts/2019-02-07-where-are-the-libertarians/img/unnamed-chunk-18-1.png" width="672">
</p>
<p>
What I don’t like is the result. Contrary to my pre-conceived notion, it’s clear the American electorate is not crypto-libertarian. Rather, voters want a third party that is highly centrist or highly polarized along traditional liberal/conservative lines. This makes it unlikely that any single third party could be successful at the ballot box. Rather, both an extreme left-wing and an extreme right wing party could take votes away from the traditional parties. The Republican party is more hollowed out in its relative middle than the Democrats.
</p>
<p>
Could Howard Schulz be something of a spoiler from the center? Possibly. There are a large number of voters who would like an alternative that is less intrusive than the Democrats on economic issues and less intrusive than the Republicans on moral issues. I disagree with the Times’ assessment that, since there are so few absolute libertarians, Schulz will not find a base. As we see, there are many people who lean toward the center and away from the extremes within their parties, even if they are not libertarian, <em>per se</em>. But, far too many people are happy with the status quo or would like their party more on the left or right to make this likely as we see below.
</p>
<pre class="sourceCode r code-with-copy"><code>tmp &lt;-scores_narrow %&gt;% 
  mutate(social_direction=cut(abs(social),breaks=c(-0.1,0.25,1.1),
                    labels=c("To the Center",
                             "Status Quo or More Extreme"))) %&gt;% 
  mutate(fiscal_direction=cut(abs(fiscal),breaks=c(-0.1,0.25,1.1),
                    labels=c("To the Center",
                             "Status Quo or More Extreme")))



xtabs(~social_direction+fiscal_direction,tmp) %&gt;% as_tibble() %&gt;% kable()</code></pre>

<table class="table">
<thead>
<tr class="header">
<th align="left">
social_direction
</th>
<th align="left">
fiscal_direction
</th>
<th align="right">
n
</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">
To the Center
</td>
<td align="left">
To the Center
</td>
<td align="right">
1211
</td>
</tr>
<tr class="even">
<td align="left">
Status Quo or More Extreme
</td>
<td align="left">
To the Center
</td>
<td align="right">
1116
</td>
</tr>
<tr class="odd">
<td align="left">
To the Center
</td>
<td align="left">
Status Quo or More Extreme
</td>
<td align="right">
496
</td>
</tr>
<tr class="even">
<td align="left">
Status Quo or More Extreme
</td>
<td align="left">
Status Quo or More Extreme
</td>
<td align="right">
3037
</td>
</tr>
</tbody>

</table>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">
Conclusion
</h2>
<p>
I started this exercise hoping to find some support for my personal views among the broader electorate. Sadly, I didn’t find much. The strongest statement I can make is there is a slight bias among both Republicans and Democrats for more centrist policies. But the fun of data science is finding things you didn’t expect and in validating or refuting hunches and feelings with good science. I know something today I didn’t know yesterday so I’ll call it a win!
</p>
<p>
UPDATE 2/8/2019: Based on feedback, I changed party colors and left/right positions to those most Americans are accomstomed to.
</p>
<pre class="sourceCode r code-with-copy"><code>sessionInfo()</code></pre>

<pre><code>## R version 3.5.1 (2018-07-02)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 17134)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=English_United States.1252 
## [2] LC_CTYPE=English_United States.1252   
## [3] LC_MONETARY=English_United States.1252
## [4] LC_NUMERIC=C                          
## [5] LC_TIME=English_United States.1252    
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] bindrcpp_0.2.2  knitr_1.21      forcats_0.3.0   stringr_1.3.1  
##  [5] dplyr_0.7.8     purrr_0.3.0     readr_1.3.1     tidyr_0.8.2    
##  [9] tibble_2.0.1    ggplot2_3.1.0   tidyverse_1.2.1
## 
## loaded via a namespace (and not attached):
##  [1] tidyselect_0.2.5 xfun_0.4         haven_2.0.0      lattice_0.20-35 
##  [5] colorspace_1.4-0 generics_0.0.2   htmltools_0.3.6  yaml_2.2.0      
##  [9] utf8_1.1.4       rlang_0.3.1      pillar_1.3.1     glue_1.3.0      
## [13] withr_2.1.2      modelr_0.1.2     readxl_1.2.0     bindr_0.1.1     
## [17] plyr_1.8.4       munsell_0.5.0    blogdown_0.10    gtable_0.2.0    
## [21] cellranger_1.1.0 rvest_0.3.2      evaluate_0.12    labeling_0.3    
## [25] curl_3.3         fansi_0.4.0      highr_0.7        broom_0.5.1     
## [29] Rcpp_1.0.0       scales_1.0.0     backports_1.1.3  jsonlite_1.6    
## [33] hms_0.4.2        digest_0.6.18    stringi_1.2.4    ggrepel_0.8.0   
## [37] bookdown_0.9     grid_3.5.1       cli_1.0.1        tools_3.5.1     
## [41] magrittr_1.5     lazyeval_0.2.1   crayon_1.3.4     pkgconfig_2.0.2 
## [45] xml2_1.2.0       lubridate_1.7.4  assertthat_0.2.0 rmarkdown_1.11  
## [49] httr_1.4.0       rstudioapi_0.9.0 R6_2.3.0         nlme_3.1-137    
## [53] compiler_3.5.1</code></pre>
</section>



 ]]></description>
  <category>politics</category>
  <category>ggplot2</category>
  <guid>outsiderdata.netlify.app/posts/2019-02-07-where-are-the-libertarians/2019-02-07-where-are-the-libertarians.html</guid>
  <pubDate>Thu, 07 Feb 2019 05:00:00 GMT</pubDate>
  <media:content url="outsiderdata.netlify.app/posts/2019-02-07-where-are-the-libertarians/img/unnamed-chunk-18-1.png" medium="image" type="image/png" height="103" width="144"/>
</item>
<item>
  <title>Rick and Morty Palettes</title>
  <dc:creator>Art Steinmetz</dc:creator>
  <link>outsiderdata.netlify.app/posts/2019-02-04-rick-and-morty-palettes/2019-02-04-rick-and-morty-palettes.html</link>
  <description><![CDATA[ 



<p>
This was just a fun morning exercise. Let’s mix multiple images to make a palette of their principal colors using k-means. We’ll also use the totally awesome list-columns concept to put each image’s jpeg data into a data frame of lists that we can <code>map</code> to a function that turns the jpeg data into a list of palette colors in a new data frame.
</p>
<p>
This more-or-less copies <a href="http://www.milanor.net/blog/build-color-palette-from-image-with-paletter/" class="uri">http://www.milanor.net/blog/build-color-palette-from-image-with-paletter/</a> with the added twist of using multiple images before creating the palette. We’ll also get into the weeds a bit more with dissecting the images. I wanted to see if some cartoon show palettes using this method matched those in the <a href="https://cran.r-project.org/web/packages/ggsci/vignettes/ggsci.html"><code>ggsci</code></a> package. Did the authors use the algorithmic approach I will use here? Will my approach look any better? Don’t know. I decided to use “Rick and Morty” because my kids like it. I would certainly never watch such drivel. I’m a scientist.
</p>
<p>
For the record, the one pop culture derived palette I really like is the <a href="https://github.com/karthik/wesanderson">Wes Anderson palette</a> and on CRAN. These are presumably lovingly curated and created, not like the ones created by the stupid robot I use here.
</p>
<p>
The drawback to using K-means to create palettes from images is that it’s likely that <em>none</em> of the colors created are actually in the image. They just represent the mathematical centers of the clusters of colors.
</p>
<p>
Load libraries.
</p>
<pre class="sourceCode r code-with-copy"><code>library(tidyverse)
library(jpeg) #import images
library(scales) #just for for the show_col() function
library(ggsci) #to compare my palettes to its palettes
library(ggfortify) #to support kmeans plots
library(gridExtra) #multiple plots on a page</code></pre>
<p>
Load mulitple images. They are all Google image search thumbnails so the size is the same. This matters since we are combining images. A larger image would have a disproportional weight in our analysis.
</p>
<p>
I first thought that, since I am combining multiple images to get one palette, I needed to tile the images then process. No.&nbsp;We just care about the pixel color values so it really doesn’t matter what position they are in. The most efficient approach is to just chain all the RGB values together. Duh. Still we want to do some work with the individual images so let’s label them.
</p>
<pre class="sourceCode r code-with-copy"><code>rm_list&lt;-list()
for (n in 1:6){
  img&lt;-jpeg::readJPEG(paste0("img/rm",n,".jpg"))
  R&lt;-as.vector(img[,,1])
  G&lt;-as.vector(img[,,2])
  B&lt;-as.vector(img[,,3])
  rm_list&lt;-bind_rows(data_frame(img=n,R,G,B),rm_list) %&gt;% 
    arrange(img)
}

rm_list &lt;- left_join(rm_list,
                     data_frame(
                     img = c(1, 2, 3, 4, 5, 6),
                     name = c("Schwifty","Portal","Cable",
                     "Family", "Outdoor", "Wedding")
                     ))</code></pre>
<section id="show-me-what-you-got" class="level1">
<h1>
Show Me What You Got
</h1>
<p>
I chose the images from Google image search to be representative of varying but typical scenes.
</p>
<p>
<img src="outsiderdata.netlify.app/posts/2019-02-04-rick-and-morty-palettes/img/rm3.jpg" alt="Cable"> Cable
</p>
<p>
<img src="outsiderdata.netlify.app/posts/2019-02-04-rick-and-morty-palettes/img/rm4.jpg" alt="Family"> Family
</p>
<p>
<img src="outsiderdata.netlify.app/posts/2019-02-04-rick-and-morty-palettes/img/rm6.jpg" alt="Wedding"> Wedding
</p>
<p>
<img src="outsiderdata.netlify.app/posts/2019-02-04-rick-and-morty-palettes/img/rm5.jpg" alt="Outdoor"> Outdoor
</p>
<p>
<img src="outsiderdata.netlify.app/posts/2019-02-04-rick-and-morty-palettes/img/rm2.jpg" alt="Portal"> Portal
</p>
<p>
<img src="outsiderdata.netlify.app/posts/2019-02-04-rick-and-morty-palettes/img/rm1.jpg" alt="Schwifty"> Schwifty
</p>
<p>
For fun let’s do some density plots of the color values.
</p>
<pre class="sourceCode r code-with-copy"><code>#make data tidy first
rm_tidy &lt;- rm_list %&gt;% gather("color","level",-img,-name)
ggplot(rm_tidy,aes(x=level,fill=color))+
  geom_density(alpha=0.7) + 
  scale_fill_manual(values=c("blue","green","red")) + 
  theme_void()</code></pre>
<p>
<img src="outsiderdata.netlify.app/posts/2019-02-04-rick-and-morty-palettes/img/unnamed-chunk-3-1.png" width="672">
</p>
<p>
We can see some evidence of bimodality, a preference for very bright and very dark hues. Red is more often cranked to the max, while blue is much more evenly distributed. Perhaps that is typical of the limited palette of cartoons or just a function of the small number of frames I chose.
</p>
<pre class="sourceCode r code-with-copy"><code>ggplot(rm_tidy,aes(x=level,fill=color))+
  geom_density(alpha=0.7) + 
  scale_fill_manual(values=c("blue","green","red")) + 
  facet_wrap(~name)+
  theme_void()</code></pre>
<p>
<img src="outsiderdata.netlify.app/posts/2019-02-04-rick-and-morty-palettes/img/unnamed-chunk-4-1.png" width="672">
</p>
<p>
It’s interesting to compare “Cable” with “Family.” Both images share the same backdrop but “Family” is much darker.
</p>
</section>
<section id="make-the-palettes" class="level1">
<h1>
Make the Palettes
</h1>
<p>
When I was a kid with watercolors I wanted to come up with a name for the filthy color that resulted when I mixed all the colors together. I called it (trigger warning) “Hitler” (but, really, brown). What is the color that results when we average all the RGB values? What named R colors resemble it? It looks to me like it’s between “cornsilk4”" and “darkkhaki.”"
</p>
<pre class="sourceCode r code-with-copy"><code>blend_color&lt;-rm_list %&gt;% 
  summarise(R=mean(R),G=mean(G),B=mean(B)) %&gt;% 
  rgb()

show_col(c("cornsilk4",blend_color,"darkkhaki"))</code></pre>
<p>
<img src="outsiderdata.netlify.app/posts/2019-02-04-rick-and-morty-palettes/img/unnamed-chunk-5-1.png" width="672">
</p>
<p>
Let’s call it “desertkhaki” which, hopefully, is not a trigger word.
</p>
<p>
Now, for the fun part. In the Wes Anderson palette set, each movie get’s a different palette. Let’s make palettes for each of the images, which I chose for their distinctiveness.
</p>
<p>
For me, the good thing about open source is that I can stand on the shoulders of giants in the community. R also makes very muscular analysis trivally simple. On the other hand, it makes “script kiddies” like me potentially dangerous. I can only describe k-means in the most general terms but can run it in a snap.
</p>
<pre class="sourceCode r code-with-copy"><code>num_colors = 16
pal_schwifty &lt;- rm_list %&gt;% 
  filter(name=="Schwifty") %&gt;% 
  select(R,G,B) %&gt;% 
  kmeans(centers = num_colors, iter.max = 30) %&gt;% 
  .$centers %&gt;% 
  rgb()

show_col(pal_schwifty)</code></pre>
<p>
<img src="outsiderdata.netlify.app/posts/2019-02-04-rick-and-morty-palettes/img/unnamed-chunk-6-1.png" width="672">
</p>
<p>
For data plotting the separation between some of these colors is too small. I think 9 colors will suffice.
</p>
<pre class="sourceCode r code-with-copy"><code>num_colors = 9
pal_schwifty &lt;- rm_list %&gt;% 
  filter(name=="Schwifty") %&gt;% 
  select(R,G,B) %&gt;% 
  kmeans(centers = num_colors, iter.max = 30) %&gt;% 
  .$centers %&gt;% 
  as_tibble() %&gt;% 
  {.}</code></pre>
<pre class="sourceCode r code-with-copy"><code>show_col(rgb(pal_schwifty))</code></pre>
<p>
<img src="outsiderdata.netlify.app/posts/2019-02-04-rick-and-morty-palettes/img/unnamed-chunk-7-1.png" width="672">
</p>
<p>
For plotting purposes I would like use these colors in order of intensity. Sorting colors is a <a href="http://www.alanzucconi.com/2015/09/30/colour-sorting/">topic in itself</a> but here we’ll do it quick and simple.
</p>
<pre class="sourceCode r code-with-copy"><code>pal_schwifty %&gt;% 
  mutate(saturation=rowSums(.[1:3])) %&gt;% 
  arrange(saturation) %&gt;% 
  rgb() %&gt;% 
  show_col()</code></pre>
<p>
<img src="outsiderdata.netlify.app/posts/2019-02-04-rick-and-morty-palettes/img/unnamed-chunk-8-1.png" width="672">
</p>
<p>
That’s about right. Let’s put it all together. Go through all the images to create a series of palettes.
</p>

<pre class="sourceCode r code-with-copy"><code>#function to turn a table of RGB values to an ordered list of colors
gen_pal &lt;- function(rgb_table) {
  num_colors = 9
  pal &lt;- rgb_table %&gt;%
  select(R, G, B) %&gt;%
  kmeans(centers = num_colors, iter.max = 30) %&gt;%
  .$centers %&gt;%
  as_tibble() %&gt;%
  mutate(saturation = rowSums(.[1:3])) %&gt;%
  arrange(saturation) %&gt;%
  rgb()
  return(pal)
}</code></pre>
<pre class="sourceCode r code-with-copy"><code>#now make list columns, which are totally awesome, for each palette
palette_rick&lt;-rm_list %&gt;% 
  group_by(name) %&gt;% 
  select(-img) %&gt;% 
  nest(.key="rgb") %&gt;% 
  transmute(name=name,pal= map(rgb,gen_pal))
palette_rick</code></pre>
<pre><code>## # A tibble: 6 x 2
##   name     pal      
##   &lt;chr&gt;    &lt;list&gt;   
## 1 Schwifty &lt;chr [9]&gt;
## 2 Portal   &lt;chr [9]&gt;
## 3 Cable    &lt;chr [9]&gt;
## 4 Family   &lt;chr [9]&gt;
## 5 Outdoor  &lt;chr [9]&gt;
## 6 Wedding  &lt;chr [9]&gt;</code></pre>
<pre class="sourceCode r code-with-copy"><code>#a function to extract the individual palettes, given a name.

extract_pal&lt;-function(palette_list,pal_name){
  pal&lt;-palette_list %&gt;% filter(name==pal_name) %&gt;% 
    select(pal) %&gt;% 
    unlist() %&gt;% 
    as.vector()
  return(pal)
}</code></pre>
<pre class="sourceCode r code-with-copy"><code>plot_one&lt;-function(pal_name){
  tmp &lt;- palette_rick %&gt;% unnest() %&gt;% filter(name==pal_name)
  g&lt;- ggplot(tmp,aes(pal,fill=pal)) + geom_bar() + 
  scale_fill_manual(values=tmp$pal,guide=F) +
  theme_void()+ggtitle(pal_name)
  return (g)
  
}

lapply(palette_rick$name,plot_one) %&gt;% 
  grid.arrange(grobs=.)</code></pre>
<p>
<img src="outsiderdata.netlify.app/posts/2019-02-04-rick-and-morty-palettes/img/unnamed-chunk-11-1.png" width="672">
</p>
<p>
Finally, let’s do what we said we’d do at the beginning, put all these images together and add it to our list column of palettes.
</p>
<pre class="sourceCode r code-with-copy"><code>multi_img_pal &lt;- gen_pal(rm_list)
palette_rick&lt;-data_frame(name="all",pal=list(multi_img_pal)) %&gt;% bind_rows(palette_rick)
show_col(multi_img_pal)</code></pre>
<p>
<img src="outsiderdata.netlify.app/posts/2019-02-04-rick-and-morty-palettes/img/unnamed-chunk-12-1.png" width="672">
</p>
<p>
Not too bad. I’m glad something resembling Rick’s hair makes it into the list. Compare it to the ggsci package Rick and Morty palette. Here we see the weaknesses of an algorithmic approach. ggsci is more interesting since it has more color diversity and vividness. I assume they were hand selected. You can see Rick’s hair and Morty’s shirt color.
</p>
<pre class="sourceCode r code-with-copy"><code>show_col(ggsci::pal_rickandmorty()(9))</code></pre>
<p>
<img src="outsiderdata.netlify.app/posts/2019-02-04-rick-and-morty-palettes/img/unnamed-chunk-13-1.png" width="672">
</p>
<p>
Since the (rather flimsy) point of this excercise is to make palettes for data graphics, let’s make some plots.
</p>
<pre class="sourceCode r code-with-copy"><code>#use the example in help for dplyr::gather
stocks &lt;- data.frame(
  time = as.Date('2009-01-01') + 0:9,
  W = rnorm(10, 0, 1),
  X = rnorm(10, 0, 1),
  Y = rnorm(10, 0, 2),
  Z = rnorm(10, 0, 4)
)
stocksm &lt;- stocks %&gt;% gather(stock, price, -time)

ggplot(stocksm,aes(time,price,color=stock))+geom_line(size=2)+
  scale_color_manual(values = multi_img_pal) + theme_minimal()</code></pre>
<p>
<img src="outsiderdata.netlify.app/posts/2019-02-04-rick-and-morty-palettes/img/unnamed-chunk-14-1.png" width="672">
</p>
<pre class="sourceCode r code-with-copy"><code>ggplot(stocksm,aes(time,price,color=stock))+geom_line(size=2) +
  theme_minimal() +
  scale_color_manual(values = extract_pal(palette_rick,"Wedding"))</code></pre>
<p>
<img src="outsiderdata.netlify.app/posts/2019-02-04-rick-and-morty-palettes/img/unnamed-chunk-15-1.png" width="672"> Arguably, the perceptual differnces among the colors are less than ideal, even if the colors are pleasing. We might take the additional step of hand-selecting colors from a larger generated palette that are more suitable for plots.
</p>
</section>
<section id="one-more-thing" class="level1">
<h1>
One more thing…
</h1>
<p>
Back to the k-means analysis. When we created these palettes we were really assigning colors to the centers of the clusters of near neigbors in the a 2D space. This is a form of principal components analysis (PCA). Let’s visualize those clusters. The <code>ggplot::autoplot()</code> function makes this trivally easy. While we are at it, let’s crank up the number of colors to 20.
</p>
<pre class="sourceCode r code-with-copy"><code>num_colors = 20
#assign each pixel to a cluster
km &lt;-  rm_list[c("R","G","B")] %&gt;% kmeans(centers = num_colors, iter.max = 30)
rm_PCA&lt;-prcomp(rm_list[c("R","G","B")])

rm_list &lt;- rm_list %&gt;% mutate(cluster=as.factor(km$cluster))
autoplot(rm_PCA, x=1,y=2,data = rm_list, colour = "cluster",
         loadings = TRUE, loadings.colour = 'blue',
         loadings.label = TRUE, loadings.label.size = 10) +
  scale_color_manual(values=rgb(km$centers),guide=FALSE)+
  theme_classic()</code></pre>
<p>
<img src="outsiderdata.netlify.app/posts/2019-02-04-rick-and-morty-palettes/img/unnamed-chunk-16-1.png" width="672"> This is every pixel colored by it’s cluster assignment and plotted. It’s clear that the x-dimension, which happens to explain 74% of the color variance, is luminosity, with darker shades on the right. The other dimension seems to be related to hue.
</p>
<p>
We can make it clear by plotting the second and third principal component.
</p>
<pre class="sourceCode r code-with-copy"><code>rm_list &lt;- rm_list %&gt;% mutate(cluster=as.factor(km$cluster))
autoplot(rm_PCA, x=2,y=3,data = rm_list, colour = "cluster",
         loadings = TRUE, loadings.colour = 'blue',
         loadings.label = TRUE, loadings.label.size = 10) +
  scale_color_manual(values=rgb(km$centers),guide=F)+
  theme_classic()</code></pre>
<p>
<img src="outsiderdata.netlify.app/posts/2019-02-04-rick-and-morty-palettes/img/unnamed-chunk-17-1.png" width="672">
</p>
<p>
Now it’s quite clear that the second and third principal components map to the color space even though this explains only about 25% of the variation in the data.
</p>
<p>
Feel free to get schwifty with these palettes!
</p>
</section>



 ]]></description>
  <category>clustering</category>
  <category>rick and morty</category>
  <category>palettes</category>
  <guid>outsiderdata.netlify.app/posts/2019-02-04-rick-and-morty-palettes/2019-02-04-rick-and-morty-palettes.html</guid>
  <pubDate>Mon, 04 Feb 2019 05:00:00 GMT</pubDate>
  <media:content url="outsiderdata.netlify.app/posts/2019-02-04-rick-and-morty-palettes/img/rm2.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Is Free Pre-K in NYC Favoring the Rich?</title>
  <dc:creator>Art Steinmetz</dc:creator>
  <link>outsiderdata.netlify.app/posts/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich.html</link>
  <description><![CDATA[ 



<section id="introduction" class="level1">
<h1>
Introduction
</h1>
<p>
A hallmark of mayoral administration of NYC Mayor Bill DeBlasio has been free pre-K for all New York families. When the program was initially rolled out there were <a href="http://www.theatlantic.com/education/archive/2015/02/a-tale-of-two-pre-ks/385997/">complaints in some quarters that upper-income neighborhoods were getting more slots</a>.
</p>
<p>
This is an exploration comparing income to pre-K seats by neighborhoods. It was done mainly to help me practice with the whole workflow of data gathering, document parsing, and data tidying - plus making cool bi-variate choropleth maps! I had to invent a novel method in R to get a good looking bivariate legend onto the chart.
</p>
<p>
Thanks to Joshua Stevens for the inspiration and color theory of bi-variate maps (<a href="http://www.joshuastevens.net/cartography/make-a-bivariate-choropleth-map/" class="uri">http://www.joshuastevens.net/cartography/make-a-bivariate-choropleth-map/</a>). Thanks to Ari Lamstein for the awesome suite of choropleth packages (<a href="http://www.arilamstein.com/" class="uri">http://www.arilamstein.com/</a>).
</p>
<p>
In my original version I use an outside program, PDFTOTEXT.EXE, to get parseable text out of the PDF documents at the NYC.gov web site. I share the commented out code for this here but skip the step in the notebook to save run time. Instead, I load the raw converted text files to illustrate the parsing.
</p>
<p>
A further complication is to directly grab the income and population data from the census bureau requires an API key. You’ll have to get your own here: <a href="http://api.census.gov/data/key_signup.html"></a>. I comment out the relevant lines but instead provide the raw downloaded data sets to illustrate how they get manipulated.
</p>
<p>
NOTE: This analysis was originally done back in 201. The data is from that time. The URLs for city’s directories have changed and so too have the formats. The web scraping routines need to be modified accordingly.
</p>
<p>
You can find the raw data in CSV format at <a href="https://github.com/apsteinmetz/PreK" class="uri">https://github.com/apsteinmetz/PreK</a>.
</p>
</section>
<section id="load-libraries" class="level1">
<h1>
Load Libraries
</h1>
<pre class="sourceCode r code-with-copy"><code>library(Hmisc) # cut2 for binning
library(choroplethr)
#not on CRAN. Do an install the first time
#devtools::install_github('arilamstein/choroplethrZip@v1.5.0')
library(choroplethrZip)
library(acs)  # retrieve census data
library(tidyverse)
library(stringr)
library(reshape2)
library(cowplot)
library(jpeg)</code></pre>
</section>
<section id="load-income-and-population-data-from-the-federal-census" class="level1">
<h1>
Load income and population data from the federal census
</h1>
<p>
First, we go to the American Community Survey from the US census. The ‘acs’ package lets us directly grab that data. It’s not exactly easy because the breadth of the data is huge and it took a lot of trial and error to get just the desired data. To make it work we need to browse around the ACS to find out the NYC FIPS codes that will map to NYC Zip codes and find the table numbers that hold the income and population data. Start your browsing here: <a href="https://factfinder.census.gov/faces/nav/jsf/pages/searchresults.xhtml?refresh=t" class="uri">https://factfinder.census.gov/faces/nav/jsf/pages/searchresults.xhtml?refresh=t</a>
</p>
<section id="use-the-acs-package-to-construct-the-queries-for-census-api" class="level2">
<h2 class="anchored" data-anchor-id="use-the-acs-package-to-construct-the-queries-for-census-api">
Use the acs package to construct the queries for census api
</h2>
<pre class="sourceCode r code-with-copy"><code># -----------------------------------------------------------------------
# get census data on children and income
#census api key
#see acs package documentation
#api.key.install('your key here')

# NYC county codes
nyc_fips = c(36085,36005, 36047, 36061, 36081)
#get the zips for all nyc counties
data("zip.regions")
nyc_zips&lt;-data.frame(county.fips.numeric=nyc_fips)%&gt;%inner_join(zip.regions)%&gt;%select(region)%&gt;%t
# make an ACS geo set
nycgeo&lt;- acs::geo.make(zip.code = nyc_zips)</code></pre>
<p>
##Connect to census.gov Requires an API key. You can uncomment the lines below if you have a key. Otherwise skip to the next section to load the raw csv files which were prepared for this notebook.
</p>

<pre class="sourceCode r code-with-copy"><code># Household Household income is table 190013, per capita income is 19301
#income&lt;-acs::acs.fetch(endyear=2011,geography=nycgeo,table.number="B19013")
# #get relevant data into a data frame format
#inc&lt;-cbind(acs::geography(income),acs::estimate(income))
# kidsUnder3&lt;-acs::acs.fetch(endyear=2011,geography=nycgeo,table.number="B09001",keyword = "Under 3")
# kids&lt;-cbind(acs::geography(kidsUnder3),acs::estimate(kidsUnder3))
# totalPop&lt;-acs.fetch(endyear=2011,geography=nycgeo,table.number="B01003")
# pop&lt;-cbind(geography(totalPop),estimate(totalPop))</code></pre>
<p>
##Alternatively, load from csv files …the data we would have otherwise gotten from census.gov. Comment this chunk out if you fetch the census data directly.
</p>
<pre class="sourceCode r code-with-copy"><code>#if we can't connect to census.gov
inc&lt;-read_csv('data/NYCincome.csv',col_types = "ccd")
kids&lt;-read_csv('data/NYCkids.csv',col_types = "ccd")
pop&lt;-read_csv('data/NYCpopulation.csv',col_types = "ccd")</code></pre>
<p>
##Massage the census data
</p>
<pre class="sourceCode r code-with-copy"><code>names(inc)&lt;-c("NAME","zip","HouseholdIncome")
#needs some cleanup of dupes. I don't know why
inc&lt;-distinct(select(inc,zip,HouseholdIncome))

#kids under 3 in 2011 should approximate Pre-K kids in 2015
names(kids)&lt;-c("NAME","zip","kidsUnder3")
kids&lt;-distinct(select(kids,zip,kidsUnder3))
kids&lt;-kids %&gt;% select(zip,kidsUnder3) %&gt;% distinct() %&gt;% filter(kidsUnder3!=0 | !is.na(kidsUnder3))

names(pop)&lt;-c("NAME","zip","totPop")
pop&lt;-pop%&gt;%select(zip,totPop)%&gt;%distinct()%&gt;%filter(totPop!=0)

census&lt;-pop%&gt;%inner_join(kids)%&gt;%inner_join(inc)%&gt;%mutate(zip=as.character(zip))</code></pre>
</section>
<section id="look-at-some-preliminary-pictures-from-the-census" class="level2">
<h2 class="anchored" data-anchor-id="look-at-some-preliminary-pictures-from-the-census">
Look at some preliminary pictures from the census
</h2>
<p>
So now we have some census data. We can use the ‘chorplethr’ package to easily create some meaningful maps. Let’s look at where the kids are and what incomes are in NYC Zip codes. Note that the ‘choroplethr’ package requires the inputs to be in a data frame where the geographic identifier is labeled “region” and the data to be displayed is labeled “value.”
</p>
<pre class="sourceCode r code-with-copy"><code>#where are zips with the most rugrats?
kidsChor &lt;- census %&gt;% 
  transmute(region = zip, value = kidsUnder3 / totPop * 100)
zip_choropleth(kidsChor, 
               zip_zoom = nyc_zips, 
               title = "Percentage of Kids Under 3 in 2011")</code></pre>
<p>
<img src="outsiderdata.netlify.app/posts/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich/img/unnamed-chunk-6-1.png" width="672">
</p>
<pre class="sourceCode r code-with-copy"><code>incomeChor &lt;- census %&gt;% 
  transmute(region = zip, 
            value = HouseholdIncome)
zip_choropleth(incomeChor, 
               zip_zoom = nyc_zips, 
               title = "Household Income 2011")</code></pre>
<pre><code>## Warning in self$bind(): The following regions were missing and are being
## set to NA: 10174, 10119, 11371, 10110, 10271, 10171, 10177, 10152, 10279,
## 10115, 11430, 10111, 10112, 10167, 11351, 11359, 11424, 11425, 11451,
## 10169, 10103, 10311, 10153, 10154, 10199, 10165, 10168, 10278, 10020,
## 10173, 10170, 10172</code></pre>
<p>
<img src="outsiderdata.netlify.app/posts/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich/img/unnamed-chunk-7-1.png" width="672">
</p>
</section>
</section>
<section id="load-data-about-location-and-size-of-pre-k-programs-from-nyc" class="level1">
<h1>
Load data about location and size of pre-K programs from NYC
</h1>
<p>
As we did before we have two altenative procedures, one that illustrates downloading the PDF pre-K brochures from NYC.gov and converting them to text, which I comment out, and the second loads the converted text from csv files here in case the PDF’s cease to be available. I don’t want the notebook to break if NYC changes their web site.
</p>
<p>
We then parse it to find the Zip codes of the schools, the number of seats and whether they are full-day or half-day.
</p>
<section id="download-pdfs-from-nyc.gov" class="level2">
<h2 class="anchored" data-anchor-id="download-pdfs-from-nyc.gov">
Download PDFs from NYC.gov
</h2>
<p>
Download the PDFs then convert to text using an outside program, PDFTOTEXT.EXE (<a href="http://www.foolabs.com/xpdf/home.html" class="uri">http://www.foolabs.com/xpdf/home.html</a>).
</p>

<pre class="sourceCode r code-with-copy"><code># # -----------------------------------------------------------------------
# # get NYC data on pre-K programs
# # scan seat directory pdfs and put into a data frame by zip code
# #DOE pre-k directories
# urls&lt;- c("http://schools.nyc.gov/NR/rdonlyres/1F829192-ABE8-4BE6-93B5-1A33A6CCC32E/0/2015PreKDirectoryManhattan.pdf",
#          "http://schools.nyc.gov/NR/rdonlyres/5337838E-EBE8-479A-8AB5-616C135A4B3C/0/2015PreKDirectoryBronx.pdf",
#          "http://schools.nyc.gov/NR/rdonlyres/F2D95BF9-553A-4B92-BEAA-785A2D6C0798/0/2015PreKDirectoryBrooklyn.pdf",
#          "http://schools.nyc.gov/NR/rdonlyres/B9B2080A-0121-4C73-AF4A-45CBC3E28CA3/0/2015PreKDirectoryQueens.pdf",
#          "http://schools.nyc.gov/NR/rdonlyres/4DE31FBF-DA0D-4628-B709-F9A7421F7152/0/2015PreKDirectoryStatenIsland.pdf")
# 
# #assumes pdftotext.exe is in the current directory.  Edit as necessary
# exe &lt;- "pdftotext.exe"
# 
# #regex to parse address line
# pkseattokens &lt;-"(Address: )([.[:alnum:]- ()]+),+ ([0-9]{5})([a-zA-Z .()-:]+) ([0-9]{1,4}) (FD|HD|AM|PM|5H)"
# 
# # each of the PDF directories have 27 pages of intro material. Skip it. This might change for different years. Check PDFs
# firstPage = 28
# 
# dests &lt;- tempfile(str_match(urls,"Directory(\\w.+).pdf")[,2],fileext = ".pdf")
# txt&lt;- NULL
# for (i in 1:length(urls)) {
#   download.file(urls[i],destfile = dests[i],mode = "wb")
#   # pdftotxt.exe is in current directory and convert pdf to text using "table" style at firstpage
#   result&lt;-system(paste(exe, "-table -f", firstPage, dests[i], sep = " "), intern=T)
#   # get txt-file name and open it  
#   filetxt &lt;- sub(".pdf", ".txt", dests[i])
#   txt &lt;- append(txt,readLines(filetxt,warn=FALSE))
# }</code></pre>
</section>
<section id="alternatively-import-and-combine-the-already-converted-text-files." class="level2">
<h2 class="anchored" data-anchor-id="alternatively-import-and-combine-the-already-converted-text-files.">
Alternatively, import and combine the already converted text files.
</h2>
<pre class="sourceCode r code-with-copy"><code>boroughList &lt;- c('Manhattan','Bronx','Brooklyn','Queens','Staten')
txt&lt;-NULL
for (borough in  boroughList){
  # get txt-file name and open it  
  filetxt &lt;- paste("data/",borough, ".txt", sep='')
  txt &lt;- append(txt,readLines(filetxt,warn = FALSE))
}</code></pre>
</section>
<section id="extract-relevant-info-from-text-files" class="level2">
<h2 class="anchored" data-anchor-id="extract-relevant-info-from-text-files">
Extract relevant info from text files
</h2>
<p>
Pull out the Zip, seat count and day length of each school. Note the pretty heroic (for me, anyway) regular expression, “pkseattokens.”"
</p>
<pre class="sourceCode r code-with-copy"><code># find address line which contains zip and seat count
txt2&lt;-txt[grep("Address:",txt)]
# strip chars that will mess up regex
pkseattokens &lt;-"(Address: )([.[:alnum:]- ()]+),+ ([0-9]{5})([a-zA-Z .()-:]+) ([0-9]{1,4}) (FD|HD|AM|PM|5H)"
txt2&lt;-sub("'","",txt2)
schools&lt;-as_data_frame(str_match(txt2,pkseattokens))[,c(4,6,7)]
names(schools)&lt;-c("zip","seats","dayLength")
#have to convert from factor to character THEN to integer.  Don't know why
schools$seats&lt;-as.integer(as.character(schools$seats))

# aggregate seat count by zip code
sumSeats &lt;- schools %&gt;% 
  group_by(zip) %&gt;% 
  summarise(count = n(), 
            numSeats = sum(seats, na.rm = TRUE))
  names(sumSeats)&lt;-c("zip","schools","numSeats")</code></pre>
<p>
So we go from this: <img src="outsiderdata.netlify.app/img/prekbooklet.png">
</p>
<p>
then to this:
</p>
<pre class="sourceCode r code-with-copy"><code>txt[1:3]</code></pre>
<pre><code>## [1] "    District 1: Full-Day Pre-K Programs                                                                                      You may apply to these programs online, over the phone, or at a Family Welcome Center."
## [2] ""                                                                                                                                                                                                                   
## [3] "    Bank Street Head Start (01MATK)                                                                                                            Other School Features         2015   2014 Lowest"</code></pre>
<p>
and then to this:
</p>
<pre class="sourceCode r code-with-copy"><code>txt2[1:3]</code></pre>
<pre><code>## [1] "    Address: 535 East 5th Street, 10009 (East Village)                               Phone:    212-353-2532                                    Breakfast/Lunch/Snack(s)      40 FD  N/A"
## [2] "    Address: 280 Rivington Street, 10002 (Lower East Side)                           Phone:    212-254-3070                                    Breakfast/Lunch/Snack(s)      40 FD  N/A"
## [3] "    Address: 180 Suffolk Street, 10002 (Chinatown)                                   Phone:    212-982-6650                                    Breakfast/Lunch/Snack(s)      29 FD  N/A"</code></pre>
<p>
…and finally to this:
</p>
<pre class="sourceCode r code-with-copy"><code>schools[1:3,]</code></pre>
<pre><code>## # A tibble: 3 x 3
##   zip   seats dayLength
##   &lt;chr&gt; &lt;int&gt; &lt;chr&gt;    
## 1 10009    40 FD       
## 2 10002    40 FD       
## 3 10002    29 FD</code></pre>
<p>
Man, I love when the regex works! Magic!
</p>
</section>
<section id="look-at-some-preliminary-pictures-from-the-pre-k-data" class="level2">
<h2 class="anchored" data-anchor-id="look-at-some-preliminary-pictures-from-the-pre-k-data">
Look at some preliminary pictures from the pre-K data
</h2>
<p>
Not all the programs are full day. Are there a lot of schools offering shorter programs? We won’t use this data further in our analysis, but lets look at how many seats are full day vs.&nbsp;something else. Full day is the overwhelming majority.
</p>
<pre class="sourceCode r code-with-copy"><code>#how do the programs break out in terms of day length?
sumDayLength&lt;-schools%&gt;%group_by(dayLength)%&gt;%summarise(NumSchools=n(),NumSeats=sum(seats,na.rm=TRUE))
ggplot(sumDayLength,aes(x=dayLength,y=NumSeats)) + geom_col() +
  scale_y_continuous(labels = scales::comma)</code></pre>
<p>
<img src="outsiderdata.netlify.app/posts/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich/img/unnamed-chunk-14-1.png" width="672">
</p>
<p>
Where are the most schools? Where are the most seats? We might assume this pictures look the same, and they do.
</p>
<pre class="sourceCode r code-with-copy"><code># some preliminary pictures
sumSeats %&gt;% transmute(region = zip, value = schools) %&gt;%
  zip_choropleth(zip_zoom = nyc_zips, 
                 title = "Number of Schools")</code></pre>
<p>
<img src="outsiderdata.netlify.app/posts/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich/img/unnamed-chunk-15-1.png" width="672">
</p>
<pre class="sourceCode r code-with-copy"><code>sumSeats %&gt;% transmute(region=zip,value=numSeats) %&gt;% 
  zip_choropleth(zip_zoom = nyc_zips,
                 title = "Number of Pre-K Seats")</code></pre>
<pre><code>## Warning in super$initialize(zip.map, user.df): Your data.frame contains the
## following regions which are not mappable: 11249, 11376, NA</code></pre>
<pre><code>## Warning in self$bind(): The following regions were missing and are being
## set to NA: 10464, 11040, 10280, 10174, 10017, 10119, 11371, 10110, 10271,
## 11003, 11370, 10171, 10069, 10162, 10177, 10152, 10279, 10115, 10005,
## 10111, 10112, 10167, 11351, 11359, 11424, 11425, 11451, 10006, 10169,
## 10103, 10311, 10153, 10154, 10199, 10165, 10168, 10278, 10020, 10173,
## 10170, 10172, 11005</code></pre>
<p>
<img src="outsiderdata.netlify.app/posts/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich/img/unnamed-chunk-16-1.png" width="672">
</p>
</section>
</section>
<section id="combine-the-data-from-both-the-federal-census-and-the-city" class="level1">
<h1>
Combine the data from both the federal census and the city
</h1>
<p>
Combine the data and do some per capita and normalization calculations.
</p>
<pre class="sourceCode r code-with-copy"><code># -----------------------------------------------------------------------
#combine the vectors for seats, income and population
allData&lt;-sumSeats %&gt;% 
  left_join(pop) %&gt;% 
  left_join(kids) %&gt;% 
  left_join(inc) %&gt;% 
  na.omit()
#get rid of airports, JFK and LGA
allData&lt;-filter(allData,zip!=11371 &amp; zip!=11430)

# add normalized seats per capita/kid
allData&lt;-allData %&gt;% mutate(seatsPer100Kids = round(numSeats/ kidsUnder3*100,digits=1),
                            seatsPer1000People=round(numSeats/totPop*1000,digits=1))

allData</code></pre>
<pre><code>## # A tibble: 171 x 8
##    zip   schools numSeats totPop kidsUnder3 HouseholdIncome seatsPer100Kids
##    &lt;chr&gt;   &lt;int&gt;    &lt;int&gt;  &lt;dbl&gt;      &lt;dbl&gt;           &lt;dbl&gt;           &lt;dbl&gt;
##  1 10001       7      167  21097        532           67795            31.4
##  2 10002      27      982  81335       1761           32407            55.8
##  3 10003       2       54  55190        937           88601             5.8
##  4 10004       1       31   2604        154          127448            20.1
##  5 10007       1       36   5892        311          191900            11.6
##  6 10009      13      366  62335       1116           55316            32.8
##  7 10010       2       87  28954        532           94242            16.4
##  8 10011       4      123  51064       1187           99700            10.4
##  9 10012       2       56  24342        453           77072            12.4
## 10 10013       4       74  25942        889           64806             8.3
## # ... with 161 more rows, and 1 more variable: seatsPer1000People &lt;dbl&gt;</code></pre>
</section>
<section id="now-lets-do-the-cool-stuff" class="level1">
<h1>
Now let’s do the cool stuff!
</h1>
<p>
First, what is the targeted level of seats available for every 100 kids? We don’t know but do know that funds are finite and not every parent wants to put their child into pre-K. It looks like most neighborhoods have roughly 25 seats for every 100 children.
</p>
<pre class="sourceCode r code-with-copy"><code>allData %&gt;% ggplot(aes(seatsPer100Kids)) + geom_histogram(binwidth = 5) +
  labs(y="Count of Neighborhoods")</code></pre>
<p>
<img src="outsiderdata.netlify.app/posts/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich/img/unnamed-chunk-18-1.png" width="672">
</p>
<p>
Is there an obvious relationship between household income and seats?
</p>
<pre class="sourceCode r code-with-copy"><code>ggplot(allData,aes(y=seatsPer100Kids,x=HouseholdIncome))+geom_point() +
    scale_x_continuous(labels = scales::dollar)</code></pre>
<p>
<img src="outsiderdata.netlify.app/posts/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich/img/unnamed-chunk-19-1.png" width="672">
</p>
<p>
Well, that isn’t a very clear visual because of the outliers on income. Let’s normalize the the seats and income data and look again.
</p>
<pre class="sourceCode r code-with-copy"><code>#lets look at the same data normalized by quantile
#compute income quantiles
fn&lt;-ecdf(allData$HouseholdIncome)
allData&lt;-mutate(allData,incomeQuantile=fn(allData$HouseholdIncome))
#compute seatsPer100Kids quantiles
fn&lt;-ecdf(allData$seatsPer100Kids)
allData&lt;-mutate(allData,seatsQuantile=fn(allData$seatsPer100Kids))

#no obvious relationship
ggplot(allData,aes(y=seatsQuantile,x=incomeQuantile))+geom_point()</code></pre>
<p>
<img src="outsiderdata.netlify.app/posts/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich/img/unnamed-chunk-20-1.png" width="672">
</p>
<p>
It’s pretty clear that there is no bias toward wealthy neighborhoods getting more pre-K seats per child. Further, it looks like a cluster of the wealthiest neighborhoods have very few seats.
</p>
</section>
<section id="finally-lets-create-the-map-that-pulls-everything-together" class="level1">
<h1>
Finally, let’s create the map that pulls everything together!
</h1>
<p>
To set the stage for the bi-variate plot we need to split the data into bins. A 3x3 matrix of income cohorts and pre-K seat quantiles is about all we can handle without over-complicating the visual.
</p>
<pre class="sourceCode r code-with-copy"><code># set bins for bi-variate plot
bins&lt;-3
allData&lt;-mutate(allData,seatsBin=cut2(seatsPer100Kids,g=bins,levels.mean = TRUE))
allData&lt;-mutate(allData,incomeBin=cut2(HouseholdIncome,g=bins,levels.mean = TRUE))

# create a data frame exclusively for use in a chorpleth object
# contains only zips as "region" and income/seats crosstab as "value"
bvc_df&lt;-allData
levels(bvc_df$seatsBin)&lt;-bins:1
levels(bvc_df$incomeBin)&lt;-bins:1
bvc_df&lt;-transmute(bvc_df,region=zip,value=paste(seatsBin,'-',incomeBin,sep=''))
title1&lt;-"NYC Household Income in 2011 vs. Pre-K Seats Per Child 3-5 in 2015"

#create choropleth object
bvc &lt;- ZipChoropleth$new(bvc_df)
bvc$title &lt;-title1
#use color scheme shown here http://www.joshuastevens.net/cartography/make-a-bivariate-choropleth-map/
#assumes 9 levels
bvColors=c("#be64ac","#8c62aa","#3b4994","#dfb0d6","#a5add3","#5698b9","#e8e8e8","#ace4e4","#5ac8c8")
bvc$ggplot_scale = scale_fill_manual(name="", values=bvColors, drop=FALSE)
bvc$set_zoom_zip(county_zoom=nyc_fips,
                 state_zoom = NULL,
                 msa_zoom = NULL,
                 zip_zoom = NULL)
bvc$render()</code></pre>
<p>
<img src="outsiderdata.netlify.app/posts/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich/img/unnamed-chunk-21-1.png" width="672">
</p>
<p>
So there’s the map. We can immediately see the problem with the default legend. The labeling is meaningless. It’s just the index of the bins. That we can fix fairly easily but worse is the uni-dimensional nature of the legend.
</p>
<p>
Here is where the interesting hack comes in. ggplot doesn’t really have a facility for a bivariate legend. The beautiful plots Joshua Stevens shows on his web page use a dedicated graphic composition program. Can we cobble something up in R? Yes! The ‘cowplot’ package allows for creation of a layered plotting canvas where we can overlay multiple plots in arbitary positions and sizes.
</p>
<section id="create-the-custom-legend." class="level2">
<h2 class="anchored" data-anchor-id="create-the-custom-legend.">
Create the custom legend.
</h2>
<p>
To create the legend we ‘simply’ create a heat map of the 3x3 bins in the map and label the axes appropriately. Then, using ‘cowplot’, shove it into a corner of the map. There are other ways we could use, but they don’t look nearly as nice.
</p>
<pre class="sourceCode r code-with-copy"><code>#first create a legend plot
legendGoal = melt(matrix(1:9, nrow = 3))
lg &lt;- ggplot(legendGoal, aes(Var2, Var1, fill = as.factor(value))) + geom_tile()
lg &lt;- lg + scale_fill_manual(name = "", values = bvColors)
lg &lt;- lg + theme(legend.position = "none")
lg &lt;- lg + theme(axis.title.x = element_text(size = rel(1), color = bvColors[3])) + 
  xlab(" More Income --&gt;")
lg &lt;- lg + theme(axis.title.y = element_text(size = rel(1), color = bvColors[3])) + 
  ylab("   More Seats --&gt;")
lg &lt;- lg + theme(axis.text = element_blank())
lg &lt;- lg + theme(line = element_blank())
lg</code></pre>
<p>
<img src="outsiderdata.netlify.app/posts/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich/img/unnamed-chunk-22-1.png" width="672">
</p>
<p>
Above we see the legend as a custom rolled heat map. There is no data in it, just a matrix corresponding to the bin indices in the zip code map. We assign colors to match.
</p>
</section>
<section id="put-both-plots-on-a-grid" class="level2">
<h2 class="anchored" data-anchor-id="put-both-plots-on-a-grid">
Put both plots on a grid
</h2>
<p>
Now we have the map in the ‘gg’ variable and the legend in the ‘lg’ variable. ‘ggdraw()’ and ‘draw_plot()’ are the ‘cowplot’ functions that let us create the canvas. We tweak the location and size parameters for rendering the legend element until it looks nice inset with the map.
</p>
<pre class="sourceCode r code-with-copy"><code># put the legend together with the map
# further annotate plot in the ggplot2 environment
#strip out the ugly legend
gg&lt;-bvc$render()  + theme(legend.position="none")
ggdraw() + draw_plot(lg,0.2,0.5,width=0.2,height=0.35) + 
  draw_plot(gg)</code></pre>
<p>
<img src="outsiderdata.netlify.app/posts/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich/img/unnamed-chunk-23-1.png" width="672">
</p>
<p>
This map shows clearly where the low income, well served areas of the city are and that the swanky manhattan zip codes have the fewest free pre-K seats per child.
</p>
</section>
</section>
<section id="wrap-up" class="level1">
<h1>
Wrap-Up
</h1>
<p>
Finally, we can do a simple cross-tab heatmap to aggregate the map data. It shows clearly that there is a high occurance of wealthy zip codes with a dearth of seats and a similarly high occurance of low income zip codes with a comparitively large number of seats. The mayor’s fans can confidently assert that NYC’s pre-K program is serving the poorest neighborhoods of the city the best.
</p>

<pre class="sourceCode r code-with-copy"><code>#crosstab of number of zip codes in income and seat Bins
xtab&lt;-table(allData$seatsBin,allData$incomeBin)
rownames(xtab)&lt;-c("Few","Average","Many")
colnames(xtab)&lt;-c("Low","Middle","High")
hm &lt;- as_data_frame(xtab)
#  mutate_all(as.numeric) %&gt;%
#  round()
names(hm)&lt;-c("SeatsPer100Kids","HouseholdIncome","Freq")
hm &lt;- hm %&gt;% mutate(SeatsPer100Kids=as_factor(SeatsPer100Kids))
hm &lt;- hm %&gt;% mutate(HouseholdIncome=as_factor(HouseholdIncome))
hm &lt;- hm %&gt;% rename(ZipCount = Freq)
#show heatmap of crosstab
# this suggests that high income zip codes are underserved with pre-K seats
ggplot(hm, aes(SeatsPer100Kids, HouseholdIncome)) + 
  geom_tile(aes(fill = ZipCount),colour = "white")  +
   scale_fill_gradient(low = "lightgrey", 
                       high = "steelblue",
                       breaks=c(13,18,23)) +
    
  NULL</code></pre>
<p>
<img src="outsiderdata.netlify.app/posts/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich/img/unnamed-chunk-24-1.png" width="672">
</p>
<p>
Thanks for reading! All errors are my own and I am not trying to make any political points. I am just a data science dabbler so critiques of the code, methods and conclusions are all welcome!
</p>
<p>
– Art Steinmetz
</p>
</section>



 ]]></description>
  <category>web scraping</category>
  <category>education</category>
  <category>maps</category>
  <guid>outsiderdata.netlify.app/posts/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich.html</guid>
  <pubDate>Thu, 29 Nov 2018 05:00:00 GMT</pubDate>
  <media:content url="outsiderdata.netlify.app/posts/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich/img/unnamed-chunk-23-1.png" medium="image" type="image/png" height="103" width="144"/>
</item>
<item>
  <title>New Winter Sports for New Countries</title>
  <dc:creator>Art Steinmetz</dc:creator>
  <link>outsiderdata.netlify.app/posts/2018-02-26-new-winter-sports-for-new-countries/2018-02-26-new-winter-sports-for-new-countries.html</link>
  <description><![CDATA[ 



<section id="looking-at-winter-olympic-medal-rankings-by-vintage-of-sport-introduction" class="level1">
<h1>
Looking at Winter Olympic Medal Rankings by Vintage of Sport Introduction
</h1>
<p>
Norway is a tiny country that punches way above its weight in the Winter Olympic medal count. We are not surprised as those folks are practically born on skis. At the same time, toussle-haired surfer dudes and dudettes from the US seem to be all over the hill when snowboards are involved. Notably, the sports where the US is most visible are sports which arose fairly recently. Is there a pattern here? Let’s do a quick hit to see if we can visualize the dominance of countries, not by event, but by vintage of a sport’s introduction to the games.
</p>
<pre class="sourceCode r code-with-copy"><code>library(tidyverse)
library(rvest)
library(knitr)</code></pre>
</section>
<section id="scrape-and-clean-the-data-from-the-web" class="level1">
<h1>
Scrape and Clean the Data from the Web
</h1>
<p>
I chose to get the medal total by scraping the NBC site. Doubtless you could find another source for this. Making an error-proof run through the countries was like navigating slalom gates. The code is simple now but, due to variations in the tables from country to country it took a number of iterations to get this right. Using the “Inspect” feature of Google Chrome really helped me here to extract the right pieces (“xpath selectors”).
</p>
<p>
By the time you see this the web site may have changed or disappeared.
</p>

<pre class="sourceCode r code-with-copy"><code>#get top medal countries. Countries with 10 or more medals in total
URL_stub&lt;-"http://www.nbcolympics.com/medals/"

#the order of the countries is grouped by continent, then total medal count
countries&lt;-as_factor(c("Norway","Germany","Netherlands","Austria","Switzerland",
                       "France","Sweden","Italy","olympic-athlete-russia",
                       "south-korea","Japan",
                       "united-states","Canada"))

all_medals&lt;-data_frame()
for (country in countries){
  print(country)
  medals_raw&lt;-read_html(paste0(URL_stub,country))
  medals&lt;-medals_raw %&gt;%
    html_node(xpath="//table[@class='grid-table grid-table-2018']") %&gt;%
    html_table() %&gt;% .[,1:5] %&gt;%
    mutate(Country=country) %&gt;%
    # get rid of special chars
    mutate(Sport=str_extract(Sport,"(\\w|\\-| )+")) %&gt;%
    select(Country,everything()) %&gt;%
    {.}
  all_medals&lt;-bind_rows(medals,all_medals)
}</code></pre>
<pre><code>## [1] "Norway"
## [1] "Germany"
## [1] "Netherlands"
## [1] "Austria"
## [1] "Switzerland"
## [1] "France"
## [1] "Sweden"
## [1] "Italy"
## [1] "olympic-athlete-russia"
## [1] "south-korea"
## [1] "Japan"
## [1] "united-states"
## [1] "Canada"</code></pre>
<p>
Now get the Wikipedia page that describes the first year an event was held. We are limiting ourselves to the broad class of sport. There are many snowboarding events that were introduced in different years. We are lumping all snowboard events together and using just the first year a snowboarding event was introduced.
</p>
<p>
Again, there is no guarantee that the Wikipedia page format won’t change in the future.
</p>

<pre class="sourceCode r code-with-copy"><code>URL&lt;-"https://en.wikipedia.org/wiki/Winter_Olympic_Games"
wiki_page_raw&lt;-read_html(URL)

current_sports&lt;-wiki_page_raw %&gt;%
  html_node(xpath="//*[@id='mw-content-text']/div/table[3]") %&gt;%
  html_table() %&gt;% .[,1:4] %&gt;%
  mutate(Years=str_extract(Years,"\\d{4}")) %&gt;%
  rename(Notes=`Medal events contested in 2014`,Year=Years) %&gt;%
  mutate(Notes=str_replace(Notes,"\\[\\d+\\]",""))</code></pre>
<p>
The names of the sports aren’t exactly the same at both sites so align the names of the sports in both tables.
</p>
<pre class="sourceCode r code-with-copy"><code>#we can get some better alignment by forcing case agreement
all_medals&lt;-all_medals %&gt;% mutate(Sport=str_to_title(Sport))
current_sports&lt;-current_sports %&gt;% mutate(Sport=str_to_title(Sport))

#manually fix the four remaining problems
current_sports&lt;-current_sports %&gt;% mutate(Sport=ifelse(Sport=="Short Track Speed Skating","Short Track",Sport))
current_sports&lt;-current_sports %&gt;% mutate(Sport=ifelse(Sport=="Bobsleigh","Bobsled",Sport))
current_sports&lt;-current_sports %&gt;% mutate(Sport=ifelse(Sport=="Ice Hockey","Hockey",Sport))
current_sports&lt;-current_sports %&gt;% mutate(Sport=ifelse(Sport=="Cross-Country Skiing","Cross-Country",Sport))

#diplay clean results
all_medals %&gt;% select(Sport) %&gt;%
  distinct() %&gt;%
  left_join(current_sports) %&gt;%
  arrange(Year) %&gt;%
  kable()</code></pre>

<table class="table">
<thead>
<tr class="header">
<th align="left">
Sport
</th>
<th align="left">
Year
</th>
<th align="right">
Events
</th>
<th align="left">
Notes
</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">
Figure Skating
</td>
<td align="left">
1924
</td>
<td align="right">
5
</td>
<td align="left">
Men’s and women’s singles; pairs; ice dancing and team event.
</td>
</tr>
<tr class="even">
<td align="left">
Speed Skating
</td>
<td align="left">
1924
</td>
<td align="right">
14
</td>
<td align="left">
Men’s and women’s 500 m, 1000 m, 1500 m, 5000 m, Massstart and team pursuit; women’s 3000 m; men’s 10,000 m.
</td>
</tr>
<tr class="odd">
<td align="left">
Bobsled
</td>
<td align="left">
1924
</td>
<td align="right">
3
</td>
<td align="left">
Four-man race, two-man race and two-woman race.
</td>
</tr>
<tr class="even">
<td align="left">
Hockey
</td>
<td align="left">
1924
</td>
<td align="right">
2
</td>
<td align="left">
Men’s and women’s tournaments.
</td>
</tr>
<tr class="odd">
<td align="left">
Curling
</td>
<td align="left">
1924
</td>
<td align="right">
3
</td>
<td align="left">
Men’s, women’s and mixed doubles. tournaments.
</td>
</tr>
<tr class="even">
<td align="left">
Cross-Country
</td>
<td align="left">
1924
</td>
<td align="right">
12
</td>
<td align="left">
Men’s sprint, team sprint, 30&nbsp;km pursuit, 15&nbsp;km, 50&nbsp;km and 4x10&nbsp;km relay; women’s sprint, team sprint, 15&nbsp;km pursuit, 10&nbsp;km, 30&nbsp;km and 4x5&nbsp;km relay.
</td>
</tr>
<tr class="odd">
<td align="left">
Nordic Combined
</td>
<td align="left">
1924
</td>
<td align="right">
3
</td>
<td align="left">
Men’s 10&nbsp;km individual normal hill, 10&nbsp;km individual large hill and team.
</td>
</tr>
<tr class="even">
<td align="left">
Ski Jumping
</td>
<td align="left">
1924
</td>
<td align="right">
4
</td>
<td align="left">
Men’s individual large hill, team large hill; men’s and women’s individual normal hill.
</td>
</tr>
<tr class="odd">
<td align="left">
Skeleton
</td>
<td align="left">
1928
</td>
<td align="right">
2
</td>
<td align="left">
Men’s and women’s events.
</td>
</tr>
<tr class="even">
<td align="left">
Alpine Skiing
</td>
<td align="left">
1936
</td>
<td align="right">
11
</td>
<td align="left">
Men’s and women’s downhill, super G, giant slalom, slalom, and combined, and parallel slalom.
</td>
</tr>
<tr class="odd">
<td align="left">
Biathlon
</td>
<td align="left">
1960
</td>
<td align="right">
11
</td>
<td align="left">
Sprint (men: 10&nbsp;km; women: 7.5&nbsp;km), the individual (men: 20&nbsp;km; women: 15&nbsp;km), pursuit (men: 12.5&nbsp;km; women: 10&nbsp;km), relay (men: 4x7.5&nbsp;km; women: 4x6&nbsp;km; mixed: 2x7.5&nbsp;km+2x6&nbsp;km), and the mass start (men: 15&nbsp;km; women: 12.5&nbsp;km).
</td>
</tr>
<tr class="even">
<td align="left">
Luge
</td>
<td align="left">
1964
</td>
<td align="right">
4
</td>
<td align="left">
Men’s and women’s singles, men’s doubles, team relay.
</td>
</tr>
<tr class="odd">
<td align="left">
Freestyle Skiing
</td>
<td align="left">
1992
</td>
<td align="right">
10
</td>
<td align="left">
Men’s and women’s moguls, aerials, ski cross, superpipe, and slopestyle.
</td>
</tr>
<tr class="even">
<td align="left">
Short Track
</td>
<td align="left">
1992
</td>
<td align="right">
8
</td>
<td align="left">
Men’s and women’s 500 m, 1000 m, 1500 m; women’s 3000&nbsp;m relay; and men’s 5000&nbsp;m relay.
</td>
</tr>
<tr class="odd">
<td align="left">
Snowboarding
</td>
<td align="left">
1998
</td>
<td align="right">
8
</td>
<td align="left">
Men’s and women’s parallel, half-pipe, snowboard cross, and slopestyle.
</td>
</tr>
</tbody>

</table>
<p>
Good! It must be noted that we are working with tiny data here. If my boss asked me to do this I would just manually create all these tables, in the interest of time. Here at OutsideRdata we do things, not the hard way, but the way that teaches us the most!
</p>
<p>
The cleaning is done. Now lets join the tables, make sure the countries are in the right order and add some observations using the oh-so-useful <code>complete</code> function to put zeros in the missing combinations of year and sport for every country.
</p>

<pre class="sourceCode r code-with-copy"><code>final_table &lt;- all_medals %&gt;%
  select(Country,Sport,Total) %&gt;%
  left_join(current_sports) %&gt;% .[,1:4]

#make sure the order of the countries matches the order we started with
final_table$Country &lt;- as_factor(final_table$Country,levels=levels(countries))

#fill empty cases with zero so there are no blanks in the plot
final_table &lt;- final_table %&gt;% complete(Country,Sport,Year,fill=list(Total=0))

agg_medals&lt;-final_table %&gt;% group_by(Country,Year) %&gt;%
  summarize(Total=sum(Total))
agg_medals[1:10,] %&gt;% kable()</code></pre>
<table class="table">
<thead>
<tr class="header">
<th align="left">
Country
</th>
<th align="left">
Year
</th>
<th align="right">
Total
</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">
Canada
</td>
<td align="left">
1924
</td>
<td align="right">
11
</td>
</tr>
<tr class="even">
<td align="left">
Canada
</td>
<td align="left">
1928
</td>
<td align="right">
0
</td>
</tr>
<tr class="odd">
<td align="left">
Canada
</td>
<td align="left">
1936
</td>
<td align="right">
0
</td>
</tr>
<tr class="even">
<td align="left">
Canada
</td>
<td align="left">
1960
</td>
<td align="right">
0
</td>
</tr>
<tr class="odd">
<td align="left">
Canada
</td>
<td align="left">
1964
</td>
<td align="right">
2
</td>
</tr>
<tr class="even">
<td align="left">
Canada
</td>
<td align="left">
1992
</td>
<td align="right">
12
</td>
</tr>
<tr class="odd">
<td align="left">
Canada
</td>
<td align="left">
1998
</td>
<td align="right">
4
</td>
</tr>
<tr class="even">
<td align="left">
united-states
</td>
<td align="left">
1924
</td>
<td align="right">
7
</td>
</tr>
<tr class="odd">
<td align="left">
united-states
</td>
<td align="left">
1928
</td>
<td align="right">
0
</td>
</tr>
<tr class="even">
<td align="left">
united-states
</td>
<td align="left">
1936
</td>
<td align="right">
3
</td>
</tr>
</tbody>

</table>
<section id="a-digression" class="level2">
<h2 class="anchored" data-anchor-id="a-digression">
A Digression
</h2>
<p>
A best practice with tidy data is to have every observation and every variable in a single data table. Where we want to use the data in a related table we use <code>_join</code> to add the data to the main table. This runs contrary to best practice in the early days of PC databases where “relational” was a big part of data manipulation. The data tables were kept separate and linked by keys. Keys are still how <code>_join</code> works, of course, but we just make one humongous table rather than look up the related fields on the fly. This is faster but uses more memory and/or storage. Back in the day when a couple megabytes of RAM was a lot, we cared about those things, even for small data projects. Now, we use local million-row tables with nary a blink of the eye. You kids don’t know how tough it was!
</p>
</section>
</section>
<section id="visualize" class="level1">
<h1>
Visualize
</h1>
<pre class="sourceCode r code-with-copy"><code>agg_medals %&gt;% ggplot(aes(Year,Country,fill=Total))+geom_tile()</code></pre>
<p>
<img src="outsiderdata.netlify.app/posts/2018-02-26-new-winter-sports-for-new-countries/img/unnamed-chunk-6-1.png" class="img-fluid">
</p>
<p>
We see some patterns emerging but this chart is mostly a reflection of the fact that most of the sports have been around since the beginning of the Winter games. There are more medals to be won in those sports. Let’s normalize the data by the fraction of total medals in each vintage. Also clean up the labels a bit.
</p>
<pre class="sourceCode r code-with-copy"><code>agg_medals&lt;-agg_medals %&gt;%
  group_by(Year) %&gt;%
  mutate(Pct_That_Year=Total/sum(Total))

agg_medals %&gt;%
  ggplot(aes(Year,Country,fill=Pct_That_Year))+geom_tile()+
  labs(title="New Countries Win in New Sports",
       subtitle="Medal Ranking by Vintage of Sport",
       x="Year Sport Introduced",
       y="Country by Continent")+
  geom_hline(yintercept = 2.5,size=2)+
  geom_hline(yintercept = 4.5,size=2)</code></pre>
<p>
<img src="outsiderdata.netlify.app/posts/2018-02-26-new-winter-sports-for-new-countries/img/unnamed-chunk-7-1.png" class="img-fluid">
</p>
<p>
There you have it. We clearly see that the U.S. and Canada come to the fore with the most recently introduced events. The schussing countries of Europe are a bit stronger in the older sports. Note the intersection of Germany and 1964. There was one sport inaugurated in 1964, luge. Likewise, in 1928 the only sport introduced was skeleton. Some events, like skeleton, appeared, dissapeared then reappeared again (in the case of skeleton, 2002). Biathalon is listed as a 1960 vintage sport - newish. Cross-country skiing is half the event and the Norwegians are very strong, of course. This shows the limitation of this analysis.
</p>
<p>
It is strange that skeleton, bobsled and luge are broken out as separate sports while “Alpine Skiing” is just one. For the purposes of this analysis it doesn’t affect the conclusions.
</p>
<p>
If we wanted to take this further, a more robust analysis would use several Olympics to get a larger sample size but I have to move on to March Madness!
</p>
</section>



 ]]></description>
  <category>web scraping</category>
  <category>sports</category>
  <guid>outsiderdata.netlify.app/posts/2018-02-26-new-winter-sports-for-new-countries/2018-02-26-new-winter-sports-for-new-countries.html</guid>
  <pubDate>Mon, 26 Feb 2018 05:00:00 GMT</pubDate>
  <media:content url="outsiderdata.netlify.app/posts/2018-02-26-new-winter-sports-for-new-countries/img/unnamed-chunk-6-1.png" medium="image" type="image/png" height="103" width="144"/>
</item>
<item>
  <title>Live Fast, Die Young, Stay Pretty?</title>
  <dc:creator>Art Steinmetz</dc:creator>
  <link>outsiderdata.netlify.app/posts/2018-02-11-live-fast-die-young-stay-pretty/2018-02-11-live-fast-die-young-stay-pretty.html</link>
  <description><![CDATA[ 



<section id="analyzing-deaths-of-rock-musicians" class="level2">
<h2 class="anchored" data-anchor-id="analyzing-deaths-of-rock-musicians">
Analyzing deaths of rock musicians
</h2>
<p>
Live fast, die young, stay pretty? That’s the stereotype for rockers, or it was. We only need to look at Keith Richards, over 70 and going strong, to find a major counterexample. Do rockers die young? What do they die of? How does that compare to the broader population (in the U.S., anyway). It turns out there are some suprising answers to those questions.
</p>
<p>
Along the way we’ll learn something about web scraping, html parsing and some <code>ggplot2</code> tricks. We use the <code>tidyverse</code> dialect throughout, just so you know.
</p>
</section>
<section id="load-libraries-and-do-some-setup." class="level1">
<h1>
Load libraries and do some setup.
</h1>
<p>
Note the <code>mutate_cond</code> function in the chunk below. We’ll use it later.
</p>
<pre class="sourceCode r code-with-copy"><code>library(ggplot2)
library(stringr)
library(tidyverse)
library(lubridate)
library(rvest)
library(ggrepel)
library(gapminder)
library(knitr)
library(RColorBrewer)

# function to mutate only rows meeting a certain condition
# most useful enhancement to dplyr ever!
# I load this in all my projects.
mutate_cond &lt;- function(.data, condition, ..., envir = parent.frame()) {
  #ref: https://stackoverflow.com/questions/34096162/dplyr-mutate-replace-on-a-subset-of-rows
  condition &lt;- eval(substitute(condition), .data, envir)
  condition[is.na(condition)] = FALSE
  .data[condition, ] &lt;- .data[condition, ] %&gt;% mutate(...)
  .data
}</code></pre>
</section>
<section id="load-tables-from-wikipedia." class="level1">
<h1>
Load tables from Wikipedia.
</h1>
<p>
Wikipedia maintains a table of <a href="https://en.wikipedia.org/wiki/List_of_deaths_in_rock_and_roll">deaths in rock</a> which will be our primary source. All the usual Wikipedia caveats apply plus some more. The people included in this table are whoever the posters want to include. Few are rock “stars.” Many are producers, agents, roadies and the like. But hey, they are all in the life, right? There are errors in spelling and place names. Since this is a recreational excercise, I did not spend time validating the entries.
</p>
<p>
The deaths for 2010 and later are in a separate article so we need to get both. When I started this project they were on one page. By the time you read this the Wikipedia gods may have changed it again. Beware.
</p>
<pre class="sourceCode r code-with-copy"><code>death_page1&lt;-read_html("https://en.wikipedia.org/wiki/List_of_deaths_in_rock_and_roll")
write_html(death_page1,file="death_page1.html")
death_page2&lt;-read_html("https://en.wikipedia.org/wiki/List_of_2010s_deaths_in_rock_and_roll")
write_html(death_page2,file="death_page2.html")</code></pre>
<section id="a-note-on-pointers." class="level2">
<h2 class="anchored" data-anchor-id="a-note-on-pointers.">
A note on “pointers.”
</h2>
<p>
R and the <code>rvest</code> package have some great functions for converting html <code>&lt;table&gt;</code>s into data frames. <code>rvest</code> is a very powerful package but one thing I learned is that it works with pointers to the data rather than the actual data. C programmers and old geezers like me will be familiar with this. I remember pop and push and stacks and all that stuff from the old days. R generally doesn’t pass values “by reference.” It passes “by value.” That’s why when you modify data in the scope of a function it doesn’t affect the value of the data outside unless you assign it with <code>return &lt;data&gt;</code>. Using pointers in <code>rvest</code> functions means modifications to html data happen without an explicit assigment.
</p>
<p>
Consider a trival example:
</p>
<pre class="sourceCode r code-with-copy"><code>#usual R behavior
my_function&lt;- function(x){return (x+1)}
data=3
`#this doesn't change data but it would if data was passed by reference
my_function(data)
`# [1] 4
data
`# [1] 3
`#this does change data in the usual R way
data&lt;-my_function(data)
data
`# [1] 4</code></pre>
<p>
If we were passing values “by reference” <code>my_function(data)</code> would change <code>data</code> without the need to assign it back to <code>data</code>. That’s how <code>rvest</code> works.
</p>
<p>
We use this behavior to combine the tables in the two Wikipedia articles into one html page by extracting the tables in the second wiki article and making them xml siblings of the tables in the first.
</p>
<p>
Alternatively, we could load the two pages, extract the tables separately and combine them later but this is trickier!
</p>
<pre class="sourceCode r code-with-copy"><code>#join pre-2010 to post 2010
death_page&lt;-death_page1
death_page_child&lt;-death_page1 %&gt;% xml_children() %&gt;% .[2]
death_page2_child&lt;-death_page2 %&gt;% xml_children() %&gt;% .[2]
#create one big web page by adding all the first level children from the second
#page to the first.
#This modifies death_page by changing the list of pointers associated with it.
xml_add_sibling(death_page_child,death_page2_child)
write_html(death_page,file="death_page.html")</code></pre>
</section>
</section>
<section id="clean-the-data." class="level1">
<h1>
Clean the data.
</h1>
<p>
Unfortunately, as is often the case, the raw data is not as clean as we might like.
</p>
<p>
Here’s a data wrastlin’ puzzle. The artist and the band name (or names) share a single table cell. This is not a problem for the anonymous summary stats we will present here but I am also doing another analysis where I look at radio station playlist patterns do detect deaths (the subject of a future post). For that we need to flag the event by band or artist, however it appears. Let’s tackle it now.
</p>
<p>
Parsing HTML is a dark art and I would not rise to the level of sorcerer’s apprentice. The clue that will let us separate the two (or more) identifiers is that the band is encased in a <code>&lt;small&gt;&lt;&gt;</code> tag within the <code>&lt;tr&gt;&lt;&gt;</code> tag that <code>html_table</code> looks for. Extracting the just the text from the cell concatenates the two items without a separator. The <code>rvest</code> functions won’t help this noob here. I chose to do old-school raw text editing with search/replace to break the person and the band into separate columns. So instead of <code>&lt;td&gt;Person&lt;small&gt;Band,Band&lt;&gt;&lt;&gt;</code> we want to have <code>&lt;td&gt;Person,Band,band&lt;&gt;</code>. This way we can separate out band names later.
</p>
<pre class="sourceCode r code-with-copy"><code>#Crude hack follows
# read it in as a raw text file and edit the tags
# to break the artist name and band names into comma-separated list
# by replacing '&lt;small&gt;' with ','.
death_page_text&lt;-read_file("death_page.html")
death_page_text&lt;- gsub( "&lt;small&gt;", ", ", death_page_text )
death_page_text&lt;- gsub( "&lt;/small&gt;", "", death_page_text )
write_file(death_page_text,"death_page2.html")</code></pre>
<p>
A further wrinkle is some of the age-at-death numbers are given as a range like “45-46” which causes <code>html_table</code> to create a <code>character</code> column for <code>Age</code>. Tables that don’t have any ranges, just single numbers, will be extracted as <code>numeric</code> columns. We have many tables so We want to use <code>bind_rows</code> to combine all the web page tables into one data frame. Alas, <code>bind_rows</code> will balk at chaining columns of diferent types. To avoid this, let’s arbitrarily take the lowest number in any age range we find before binding. We can do this as part of the table extraction process.
</p>
<p>
Also omit the first five tables which, by inspection, we know are not the mortality tables.
</p>

<pre class="sourceCode r code-with-copy"><code># omit first five tables
deaths_raw&lt;-read_html("death_page2.html") %&gt;% 
  html_nodes("table") %&gt;% 
  .[5:length(.)] %&gt;% 
  html_table(fill=TRUE) %&gt;%
  lapply(function(x){mutate(x,Age=as.integer(str_extract(as.character(Age),"[0-9]+")))}) %&gt;% 
  bind_rows() %&gt;% 
  as_data_frame() %&gt;% 
  {.}

deaths_raw[1:5,] %&gt;% kable(format.args=list(big.mark=","))</code></pre>
<table class="table">
<colgroup>
<col style="width: 22%">
<col style="width: 18%">
<col style="width: 18%">
<col style="width: 18%">
<col style="width: 20%">
</colgroup>
<thead>
<tr class="header">
<th>Name</th>
<th>Age</th>
<th>Date</th>
<th>Location</th>
<th>Cause of death</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Kansas Joe McCoy, Harlem Hamfats</td>
<td>44</td>
<td>January 28, 1950</td>
<td>Chicago, Illinois, USA</td>
<td>Heart disease[1]</td>
</tr>
<tr class="even">
<td>Bertha Hill</td>
<td>45</td>
<td>May 7, 1950</td>
<td>New York City, New York, USA</td>
<td>Hit and run accident[2]</td>
</tr>
<tr class="odd">
<td>Papa Charlie McCoy, Harlem Hamfats</td>
<td>41</td>
<td>July 26, 1950</td>
<td>Chicago, Illinois, USA</td>
<td>Paralytic brain disease[1]</td>
</tr>
<tr class="even">
<td>Cecil Gant</td>
<td>37</td>
<td>February 4, 1951</td>
<td>Nashville, Tennessee, USA</td>
<td>Pneumonia and heart attack[3]</td>
</tr>
<tr class="odd">
<td>Luke Jordan</td>
<td>60</td>
<td>June 25, 1952</td>
<td>Lynchburg, Virginia, USA</td>
<td>[4]</td>
</tr>
</tbody>
</table>
<p>
…and so on.
</p>
<p>
Almost done with the boring bits. Separate out the list of name and bands into, at most, four bands. Turn the date string into a POSIX-style date. Replace the empty causes with “Not Specified.” Finally, strip the footnote numbers out of the file.
</p>
<pre class="sourceCode r code-with-copy"><code>#take at most three band names.
deaths&lt;-deaths_raw %&gt;% 
  separate(Name,into=c("Name","band1","band2","band3","band4"),sep=",",extra="drop") %&gt;% 
  unique()


deaths&lt;-deaths %&gt;% 
  mutate(Date=parse_date(Date,"%B %d, %Y")) %&gt;% 
  filter(!is.na(Date))

#remove footnotes and change empty causes to "Not Specified"

deaths&lt;-deaths %&gt;% mutate(`Cause of death`=str_replace(`Cause of death`,"\\[[0-9,]+\\]",""))
deaths&lt;-deaths %&gt;% mutate_cond(is.na(`Cause of death`),`Cause of death`="Not Specified")
deaths&lt;-deaths %&gt;% mutate_cond(`Cause of death`=="",`Cause of death`="Not Specified")
deaths[1:5,]%&gt;% kable(format.args=list(big.mark=","))</code></pre>
<table class="table">
<colgroup>
<col style="width: 11%">
<col style="width: 11%">
<col style="width: 11%">
<col style="width: 11%">
<col style="width: 11%">
<col style="width: 11%">
<col style="width: 11%">
<col style="width: 11%">
<col style="width: 11%">
</colgroup>
<thead>
<tr class="header">
<th>Name</th>
<th>band1</th>
<th>band2</th>
<th>band3</th>
<th>band4</th>
<th>Age</th>
<th>Date</th>
<th>Location</th>
<th>Cause of death</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Kansas Joe McCoy</td>
<td>Harlem Hamfats</td>
<td>NA</td>
<td>NA</td>
<td>NA</td>
<td>44</td>
<td>1950-01-28</td>
<td>Chicago, Illinois, USA</td>
<td>Heart disease</td>
</tr>
<tr class="even">
<td>Bertha Hill</td>
<td>NA</td>
<td>NA</td>
<td>NA</td>
<td>NA</td>
<td>45</td>
<td>1950-05-07</td>
<td>New York City, New York, USA</td>
<td>Hit and run accident</td>
</tr>
<tr class="odd">
<td>Papa Charlie McCoy</td>
<td>Harlem Hamfats</td>
<td>NA</td>
<td>NA</td>
<td>NA</td>
<td>41</td>
<td>1950-07-26</td>
<td>Chicago, Illinois, USA</td>
<td>Paralytic brain disease</td>
</tr>
<tr class="even">
<td>Cecil Gant</td>
<td>NA</td>
<td>NA</td>
<td>NA</td>
<td>NA</td>
<td>37</td>
<td>1951-02-04</td>
<td>Nashville, Tennessee, USA</td>
<td>Pneumonia and heart attack</td>
</tr>
<tr class="odd">
<td>Luke Jordan</td>
<td>NA</td>
<td>NA</td>
<td>NA</td>
<td>NA</td>
<td>60</td>
<td>1952-06-25</td>
<td>Lynchburg, Virginia, USA</td>
<td>Not Specified</td>
</tr>
</tbody>
</table>
</section>
<section id="another-caveat" class="level1">
<h1>
Another Caveat
</h1>
<p>
Now we have a clean table. Before going further we should note that death entries are growing over time. No great surprises here but but note that the in the early days of rock there were were few old rockers. We should keep this in the back of our mind as we proceed.
</p>
<pre class="sourceCode r code-with-copy"><code>deaths&lt;-deaths %&gt;% mutate(Year=year(Date))

deaths %&gt;%  
  group_by(Year) %&gt;% 
  summarise(Entries=n()) %&gt;%
  ggplot(aes(Year,Entries))+geom_col()</code></pre>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="outsiderdata.netlify.app/posts/2018-02-11-live-fast-die-young-stay-pretty/img/unnamed-chunk-8-1.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">More Years, More Rockers</figcaption><p></p>
</figure>
</div>
<section id="common-causes-of-death" class="level1">
<h1>Common causes of Death</h1>
<p>
Let’s do a quick exploration to see if we are satisfied with the features of our data set.
</p>

<pre class="sourceCode r code-with-copy"><code>#what are common causes of death
cause_table&lt;-deaths %&gt;% 
  group_by(`Cause of death`) %&gt;% 
  summarize(Cause=n()) %&gt;% 
  arrange(desc(Cause))

cause_table[1:11,]%&gt;% kable(format.args=list(big.mark=","))</code></pre>
<table class="table">
<thead>
<tr class="header">
<th>Cause of death</th>
<th>Cause</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Not Specified</td>
<td>599</td>
</tr>
<tr class="even">
<td>Heart attack</td>
<td>240</td>
</tr>
<tr class="odd">
<td>Cancer</td>
<td>165</td>
</tr>
<tr class="even">
<td>Lung cancer</td>
<td>104</td>
</tr>
<tr class="odd">
<td>Traffic accident</td>
<td>78</td>
</tr>
<tr class="even">
<td>Drug overdose</td>
<td>74</td>
</tr>
<tr class="odd">
<td>Heart failure</td>
<td>74</td>
</tr>
<tr class="even">
<td>Stroke</td>
<td>59</td>
</tr>
<tr class="odd">
<td>Natural causes</td>
<td>48</td>
</tr>
<tr class="even">
<td>Pneumonia</td>
<td>48</td>
</tr>
<tr class="odd">
<td>Suicide</td>
<td>41</td>
</tr>
</tbody>
</table>
<p>
Looking through the table we see that many causes can be generalized. Let’s aggregate a bit. We are making some choices here. For instance, I call any cause containing the words “heart” or “cardiac” as “heart disease.” I count alcohol as a drug. Where multiple terms exist, drugs trump accidents. Suicide trumps everthing but cancer. Cancer trumps everthing. Note the use of the very handy <code>mutate_cond</code> function.
</p>
<pre class="sourceCode r code-with-copy"><code>#add a new column "general cause"
deaths&lt;-deaths %&gt;% mutate(General_Cause=`Cause of death`)

deaths&lt;-deaths %&gt;% 
  mutate_cond(str_detect(tolower(`Cause of death`),
                         "accident|fall|hit|crash|fire|poison"),General_Cause="Accident")
deaths&lt;-deaths %&gt;% 
  mutate_cond(str_detect(tolower(`Cause of death`),
                         "drug|overdose|alcohol"),General_Cause="Drugs")
deaths&lt;-deaths %&gt;% 
  mutate_cond(str_detect(tolower(`Cause of death`),"asphyxiation"),
              General_Cause="Asphyxiation")

deaths&lt;-deaths %&gt;% 
  mutate_cond(str_detect(tolower(`Cause of death`),"shot|shoot|murder|stab|gun|knife"),
              General_Cause="Murdered")
deaths&lt;-deaths %&gt;% 
  mutate_cond(str_detect(tolower(`Cause of death`),
                         "heart|cardi|coronary"),General_Cause="Heart Disease")
deaths&lt;-deaths %&gt;% 
  mutate_cond(str_detect(tolower(`Cause of death`),"aids|hiv"),General_Cause="AIDS")
deaths&lt;-deaths %&gt;% 
  mutate_cond(str_detect(tolower(`Cause of death`),
                         "diabetes|diabetic"),General_Cause="Diabetes")
deaths&lt;-deaths %&gt;% 
  mutate_cond(str_detect(tolower(`Cause of death`),
                         "pneumonia|emphysema|respiratory"),General_Cause="Lung Disease")
deaths&lt;-deaths %&gt;% 
  mutate_cond(str_detect(tolower(`Cause of death`),
                         "cirrhosis|neph|liver"),General_Cause="Liver Disease")

deaths&lt;-deaths %&gt;% 
  mutate_cond(str_detect(tolower(`Cause of death`),
                         "stroke"),General_Cause="Stroke")
deaths&lt;-deaths %&gt;% 
  mutate_cond(str_detect(tolower(`Cause of death`),
                         "alzh"),General_Cause="Alzheimers")
deaths&lt;-deaths %&gt;% 
  mutate_cond(str_detect(tolower(`Cause of death`),"suicide"),General_Cause="Suicide")

#Cancer is last because citing other organs and cancer counts as cancer.  We
#override previous  generalizations
#-omas other than "stomach" and "coma" are cancer
deaths&lt;-deaths %&gt;% 
  mutate_cond(str_detect(`Cause of death`,"oma") &amp; 
                !str_detect(`Cause of death`,"tomach|coma"),
              General_Cause="Cancer")
deaths&lt;-deaths %&gt;% 
  mutate_cond(str_detect(tolower(`Cause of death`),"cancer|tumor"),General_Cause="Cancer")</code></pre>
<p>
Further aggregate causes. Arbitrarily, label the general cause of any cause below the top 15 as “Other.”
</p>

<pre class="sourceCode r code-with-copy"><code>cause_table&lt;-deaths %&gt;% 
  count(General_Cause) %&gt;% 
  arrange(desc(n)) %&gt;% 
  rename(Rockers=n)

cause_table&lt;-cause_table %&gt;% 
  mutate_cond(!(General_Cause %in% cause_table$General_Cause[1:15]),General_Cause="Other")
cause_table&lt;-cause_table %&gt;% 
  mutate_cond(General_Cause =="Natural causes",General_Cause="Other")
#re-aggregate
cause_table&lt;-cause_table %&gt;%
  group_by(General_Cause) %&gt;% 
  summarize(Rockers=sum(Rockers)) %&gt;% 
  arrange(desc(Rockers))

cause_table&lt;-cause_table %&gt;% mutate(General_Cause= as_factor(General_Cause))
#map it back to the deaths table
deaths&lt;-deaths %&gt;% mutate_cond(!(General_Cause %in% cause_table$General_Cause[1:15]),General_Cause="Other")
deaths&lt;-deaths %&gt;% mutate_cond(General_Cause =="Natural causes",General_Cause="Other")

#We've done all the messing with deaths we need to do so lets save it for later.
save(deaths,file='deaths.rdata')


gg&lt;-cause_table[1:12,]  %&gt;% 
  ggplot(aes(General_Cause,Rockers))+geom_col()
gg&lt;-gg+coord_flip()
gg</code></pre>
<p></p>
<p><img src="outsiderdata.netlify.app/posts/2018-02-11-live-fast-die-young-stay-pretty/img/unnamed-chunk-11-1.png" class="img-fluid"></p>
<p>
NOW, we are done with all the data wrangling and can get to the (morbid) fun stuff.
</p>
<p>
This gives us a decent picture of how rock stars die. Mostly from the same things that kill all of us. Heart disease and cancer were the leading causes of death in US in 2015, followed by lung disease and accidents according to <a href="https://www.cdc.gov/nchs/fastats/deaths.htm" class="uri">https://www.cdc.gov/nchs/fastats/deaths.htm</a> and shown below.
</p>
<pre class="sourceCode r code-with-copy"><code>us_deaths&lt;-data_frame(General_Cause=as_factor(c('Heart Disease','Cancer',
                                      'Lung Disease','Accident','Stroke',
                                      'Alzheimers','Diabetes','Liver Disease',
                                      'Suicide','Murdered','Other')),
                      US=c(633842,595930,212103,146571,140323,110561,79535,49959,44193,15696,683917))
us_deaths %&gt;% kable(format.args=list(big.mark=","))</code></pre>
<table class="table">
<thead>
<tr class="header">
<th>General_Cause</th>
<th>US</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Heart Disease</td>
<td>633,842</td>
</tr>
<tr class="even">
<td>Cancer</td>
<td>595,930</td>
</tr>
<tr class="odd">
<td>Lung Disease</td>
<td>212,103</td>
</tr>
<tr class="even">
<td>Accident</td>
<td>146,571</td>
</tr>
<tr class="odd">
<td>Stroke</td>
<td>140,323</td>
</tr>
<tr class="even">
<td>Alzheimers</td>
<td>110,561</td>
</tr>
<tr class="odd">
<td>Diabetes</td>
<td>79,535</td>
</tr>
<tr class="even">
<td>Liver Disease</td>
<td>49,959</td>
</tr>
<tr class="odd">
<td>Suicide</td>
<td>44,193</td>
</tr>
<tr class="even">
<td>Murdered</td>
<td>15,696</td>
</tr>
<tr class="odd">
<td>Other</td>
<td>683,917</td>
</tr>
</tbody>
</table>
<p>
We might also note: Murder: 15,696 (<a href="https://ucr.fbi.gov/crime-in-the-u.s/2015/crime-in-the-u.s.-2015/tables/table-4" class="uri">https://ucr.fbi.gov/crime-in-the-u.s/2015/crime-in-the-u.s.-2015/tables/table-4</a>)
</p>
<p>
Drug overdoses: 53,000 (2015,<a href="https://www.drugabuse.gov/related-topics/trends-statistics/overdose-death-rates" class="uri">https://www.drugabuse.gov/related-topics/trends-statistics/overdose-death-rates</a>)
</p>
<p>
Alcohol Poisoning:2,200 (<a href="https://www.cdc.gov/vitalsigns/alcohol-poisoning-deaths/index.html" class="uri">https://www.cdc.gov/vitalsigns/alcohol-poisoning-deaths/index.html</a>)
</p>
<p>
Drug overdoses may count as “accidents” at the CDC. I don’t separate out Pneumonia from chronic lung problems.
</p>
<p>
Alzheimer’s is a big killer, nationally, but is only 21st on the list of rock deaths. Perhaps it is underreported or was recognized as a distinct cause of death only recently. Maybe rock stars die too young to get Alzheimers.
</p>
<p>
I am guilty of some statistical sloppiness from here on since I will be comparing deaths in the U.S. in 2015 to deaths of rock musicians worldwide over all years. As the pinball machines used to say, “For Amusement Only.”
</p>
</section>
</section>
<section id="you-cant-make-this-stuff-up" class="level1">
<h1>
You can’t make this stuff up
</h1>
<p>
There are some interesting causes in this Wikipedia tables. Three electrocutions, two by guitar and one my microphone. They all happened in England in the early 70s. Presumably the electrical codes have been tightned up a bit since. Three people are listed who choked on their own vomit. That’s all? Consider the hapless Steve Took of T-Rex, who died from choking on a cocktail cherry. That’s why I always take my Manhattans with a lemon twist instead. I’d choke on those red glowing balls of wax, too!
</p>
</section>
<section id="live-fast" class="level1">
<h1>
Live fast?
</h1>
<p>
While the big killers are the same for both populations, is there evidence that rockers “live fast?” Yes, it turns out.
</p>

<pre class="sourceCode r code-with-copy"><code># we assume the CDC calls drug overdoses an accident so we will too.
comp_deaths&lt;-cause_table %&gt;% 
  mutate_cond(General_Cause=="Drugs",General_Cause="Accident") %&gt;% 
  group_by(General_Cause) %&gt;% 
  summarise(Rockers=sum(Rockers)) %&gt;% 
  right_join(us_deaths) %&gt;%
  mutate(General_Cause=as_factor(General_Cause)) %&gt;% 
  na.omit() %&gt;% 
  mutate(Rockers=Rockers/sum(Rockers),US=US/sum(US)) %&gt;% 
  {.}

gg&lt;-comp_deaths %&gt;% ggplot(aes(US,Rockers))+geom_point()
gg&lt;-gg+  geom_text_repel(label=comp_deaths$General_Cause)
gg&lt;-gg+geom_abline(slope=1,intercept=0)
gg&lt;-gg+labs(title="Rock Stars are (Almost) Like the Rest of US\nProportion of all Deaths",caption="Sources: Wikipedia and CDC",
y='Rock Music Deaths (1950-Present)',x='US Deaths (2015)')
gg&lt;-gg+scale_x_continuous(labels=scales::percent)
gg&lt;-gg+scale_y_continuous(labels=scales::percent)
gg</code></pre>
<p><img src="outsiderdata.netlify.app/posts/2018-02-11-live-fast-die-young-stay-pretty/img/unnamed-chunk-13-1.png" class="img-fluid"></p>
<p>
This scatter plot shows the preponderance of rock musician deaths relative to the broader US population. Where the points lie above the line it means rock musicians die of that cause more often than the overall US population. We can see that suicide, murder and, especially, accidents account for more relative deaths than they do in the broader population. Cancer seems more prevalent, too. We can’t know, but might suspect, this due to behavioral factors like smoking.
</p>
</section>
<section id="die-young" class="level1">
<h1>
Die Young?
</h1>
<p>
We’ve <code>&lt;grin&gt;</code>scientifically proven<code>&lt;/grin&gt;</code> that rock stars “live fast.” Do they die young? Life expectancies have grown over the years so taking an aggregate average age at death going back to 1950 won’t be comprable to current US life expectancy. To get a comparison let’s pull the data from the <code>gapminder</code> package. This shows life expectancy at birth as opposed to average age at death, which is what our data set has now. We have to get a bit tricky to make the numbers comparable by taking <code>date</code> minus <code>Age</code> to get <code>birth_year</code>. We take the gapminder life expectancy for <code>birth_year</code> and compare that to <code>Age</code> at death. The earliest birth years in our data are much earlier than what the gapminder set covers so we need to extrapolate the data, as well. If we were doing REAL data science we would find a better mortality data set, but this is OUTSIDER data science! We keep it simple.
</p>

<pre class="sourceCode r code-with-copy"><code>#Add birth year column.
deaths&lt;-deaths %&gt;% mutate(birth_year=year(Date)-Age)

#Create extrapolation of gapminder life expectancy using the full range of birth
#years from the deaths table.
#create a row for all years.
life_exp&lt;-gapminder %&gt;% 
  filter(country=="United States") %&gt;% 
  select(year,lifeExp) %&gt;% rename(birth_year=year) %&gt;% 
  complete(birth_year=min(deaths$birth_year):year(Sys.Date())) %&gt;% 
  {.}
#extrapolate
p&lt;-lm(life_exp$lifeExp~life_exp$birth_year)
life_exp$expected_longevity&lt;-predict(p,life_exp)

#how well does this fit the data?
life_exp %&gt;% ggplot(aes(birth_year,expected_longevity))+geom_line()+geom_point(aes(y=lifeExp))+
  labs(title="Gapminder U.S. Life Expectancy at Birth, Extrapolated",y="Life Expectancy",x="Birth Year")</code></pre>
<p>
<img src="outsiderdata.netlify.app/posts/2018-02-11-live-fast-die-young-stay-pretty/img/unnamed-chunk-14-1.png" class="img-fluid">
</p>
<p>
We see that the <code>gapminder</code> data is pretty linear so we should be okay to extrapolate. That said, we are going way beyond the observed range. This is generally bad practice and, <a href="https://www.edwardtufte.com/bboard/q-and-a-fetch-msg?msg_id=0001yB">according to Edward Tufte</a>, was a contributing factor in the Columbia Space Shuttle disaster (that, and Powerpoint). I saw some sources that indicate the line should bend downward in the 1800s. I am not controlling for demographics, either. Our sample sizes are pretty small at the far end of the range, anyway, so any conclusions would have to be taken with a huge grain of salt,regardless. Still, lets plunge on!
</p>
<p>
We need to apply the expected longevity for each musician for the year they were born and compare that to the age they were at death. We then summarize that data for each year of deaths. To put it another way, in any given year, how did the rock musicians who died compare to the life expectancy for all people born in the US the same year each rock musician did?
</p>
<p>
The plot below shows how the average age at death has changed over the years for rock musicians and the population as a whole. The series for rock musicians in the chart is very unstable early on because the sample size is so small. Despite this caveat and the warning about extrapolation
</p>
<p>
Plotting note: I am not using “tidy”” data here so I roll some intersting g-g-jitsu to get the legends nice. The musician data and the U.S. population data are in separate columns so they get plotted with separate <code>geom_line</code> aesthetics. We could use <code>gather</code> to make the data tidy but that would complicate taking the difference to show the <code>gap</code> series. This data formate makes it easy to just put it in a new column.
</p>
<pre class="sourceCode r code-with-copy"><code>#merge deaths with life expectancy for the relevant birth year.  Determine
#whether they died "young"  or outlived expectations.
deaths &lt;- deaths %&gt;% 
  left_join(life_exp) %&gt;% 
  select(-lifeExp) %&gt;% 
  mutate(relative_mortality=Age-expected_longevity) %&gt;% 
  {.}

death_age&lt;-deaths %&gt;% 
  group_by(Year) %&gt;% 
  summarise(rock_mean=as.integer(mean(Age)),
            us_mean=mean(expected_longevity),
            gap=mean(relative_mortality))

gg&lt;-death_age %&gt;% ggplot(aes(Year,rock_mean,color="red"))+geom_line()
gg&lt;-gg+geom_line(aes(y=us_mean,color="black"))
gg&lt;-gg+geom_col(aes(y=gap,fill="darkgrey"),color="lightgrey")
gg&lt;-gg+labs(title="Rock Stars Don't Die Young Anymore\nRocker Death Age vs. U.S. Population",
            y="Average Age At Death",
            x="Year of Death",
            caption="Source:Wikipedia, gapminder.org")
gg&lt;-gg+ scale_colour_manual(name = 'Lines', 
         values =c('black'='black','red'='red'), labels = c('US Pop.(expected)',"Rockers (actual)"))
gg&lt;-gg+ scale_fill_identity(name = 'Bars', guide = 'legend',labels = c('Gap'))
gg</code></pre>
<p>
<img src="outsiderdata.netlify.app/posts/2018-02-11-live-fast-die-young-stay-pretty/img/unnamed-chunk-15-1.png" class="img-fluid">
</p>
<p>
This really surprised me. Rock stars used to die young, but not any more! They have completely closed the gap with the U.S. population at large (We can discount the early years in this chart since there are so few observations). Again, Keith Richards, stick with the whatever you’re doing, dude!
</p>
<p>
Be careful, though. Remember what we observed above. In the early days of rock, there were few old musicians. This chart may simply reflect the arrival of old rockers. To do this analysis the proper way we would need to adjust for that somehow. The same issue might skew the relative accident frequency. Further research is needed.
</p>
<p>
Aww, heck with it. Let’s show the tidyverse way to roll this. Start from the <code>death_age</code> table we created above. Tidyfy it with <code>gather</code>, then plot.
</p>
<pre class="sourceCode r code-with-copy"><code>gg&lt;-death_age %&gt;% gather(key="cohort",value="death_age",-Year) %&gt;% 
  ggplot(aes(Year,death_age,color=cohort))+geom_line()+geom_hline(yintercept = 0)
gg&lt;-gg+labs(title="Rock Stars Don't Die Young Anymore\nRocker Death Age vs. U.S. Population",
            y="Average Age At Death",
            x="Year of Death",
            caption="Source:Wikipedia, gapminder.org")
gg&lt;-gg+ scale_colour_manual(name="",
                            values =c('blue','red',"black"),
                            labels = c('Gap',"Rockers",'US Pop.(expected)'))
gg</code></pre>
<p>
<img src="outsiderdata.netlify.app/posts/2018-02-11-live-fast-die-young-stay-pretty/img/unnamed-chunk-16-1.png" class="img-fluid">
</p>
<p>
This is much simpler, at the expense of losing the bar <code>geom</code> for <code>Gap</code>. The first example is a complicated mix of mixing <code>color</code>, <code>fill</code> and legend labels. The code for the second is far more clear to me. But…if we want to mix <code>geom_line</code> and <code>geom_bar</code> we are back to a more complicated treatment.
</p>
</section>
<section id="stay-pretty-the-27-club." class="level1">
<h1>
Stay pretty? The “27 Club.”
</h1>
<p>
“They” say 27 is particularly risky age to be a rock star. <a href="https://en.wikipedia.org/wiki/27_Club">The “27 club” is a thing</a>. Is it truly standout year for mortality, or is it just a meme without grounds?
</p>
<p>
We’ll plot the number of deaths by age and highlight 27. Create the highlight by making a vector, <code>fills</code>, that specifies the color of each bar in advance. At first glance, creating <code>fills</code> seems overly complicated. It’s slightly tricky since not all ages have deaths so a consecutive series won’t work.
</p>
<pre class="sourceCode r code-with-copy"><code>highlight=27
# the code below creates the list of colors for plot bars, making just one red.
# I suspect I am making this harder than it needs to be.
fills&lt;-deaths %&gt;% 
  select(Age) %&gt;%
  unique() %&gt;% 
  arrange(Age) %&gt;% 
  mutate(color="black") %&gt;% 
  mutate_cond(Age==highlight,color="red") %&gt;% 
  pull(color) %&gt;% 
  {.}

gg&lt;-deaths %&gt;% ggplot(aes(Age))+geom_bar(fill=fills)
gg&lt;-gg+labs(title="Rock Musician Age of Death (1950-2017)",y="Count")
gg&lt;-gg+annotate("text",x=27,y=55,label="Age 27",size=8)
gg</code></pre>
<p>
<img src="outsiderdata.netlify.app/posts/2018-02-11-live-fast-die-young-stay-pretty/img/unnamed-chunk-17-1.png" class="img-fluid">
</p>
<p>
Well, well, well. It turns out that 27 IS a risky age! Optically, it looks like about double the mortality we might otherwise expect. We always have to be wary of carts leading horses, though. Perhaps the folks who are interested in the topic are more than usually dilligent in finding examples and adding them to the Wikipedia page. Still, it’s fascinating. Why do 27-year olds die? You already know the answer.
</p>
<pre class="sourceCode r code-with-copy"><code>deaths %&gt;% filter(Age==highlight) %&gt;% 
  group_by(General_Cause) %&gt;% 
  summarise(Count=n()) %&gt;%
  arrange(desc(Count))%&gt;% 
  kable(format.args=list(big.mark=","))</code></pre>
<table class="table">
<thead>
<tr class="header">
<th>General_Cause</th>
<th>Count</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Accident</td>
<td>9</td>
</tr>
<tr class="even">
<td>Drugs</td>
<td>8</td>
</tr>
<tr class="odd">
<td>Other</td>
<td>6</td>
</tr>
<tr class="even">
<td>Suicide</td>
<td>5</td>
</tr>
<tr class="odd">
<td>Not Specified</td>
<td>4</td>
</tr>
<tr class="even">
<td>Murdered</td>
<td>3</td>
</tr>
<tr class="odd">
<td>Cancer</td>
<td>1</td>
</tr>
<tr class="even">
<td>Diabetes</td>
<td>1</td>
</tr>
<tr class="odd">
<td>Heart Disease</td>
<td>1</td>
</tr>
<tr class="even">
<td>Lung Disease</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>
Tragic, of course. Those of us who have achieved a certain age occasionally reflect upon all the times in our youth when we did stupid stuff that could have killed us but luck favored us at that moment. Did I say “we?” Well maybe just me.
</p>
<p>
How do causes of death change as rock musicians age? For this we’ll use a <code>geom_density</code> plot to get smooth progressions over time in a stacked format. Using <code>position=“fill”</code> is the way to get the stack of causes to reach 100% for all age cohorts. I mention this because I was scratching my head over how to do this. I was confusing the use of the argument <code>“fill”</code> in this context with the use of <code>fill</code> to create a different series for each categories of a variable. We use <code>fill</code> in both senses here: <code>geom_density(aes(fill=General_Cause),color=“grey”,position=“fill”)</code>
</p>
<p>
If we assume “Not Specified” deaths are in proportion to the reported causes, we can filter it it out get a clearer picture.
</p>
<pre class="sourceCode r code-with-copy"><code>#cut off below 21 and above 85 because the data gets pretty thin
gg&lt;-deaths %&gt;%  
  filter(General_Cause != "Not Specified") %&gt;% 
  filter(Age &gt;21,Age&lt;86) %&gt;% 
  ggplot(aes(Age,..count..))+geom_density(aes(fill=General_Cause),color="grey",alpha=0.9,position="fill")
gg&lt;-gg+labs(title="Top Causes of Rock Musician Deaths by Age of Death",
            y="Relative Frequency")
gg&lt;-gg+scale_y_continuous(labels=scales::percent,limits=c(0,1.00))
gg&lt;-gg+theme(panel.background = element_blank(),
             panel.grid.major.x = element_line(color="black"))
#make a palette for 15 categories with good color separation
pal&lt;-c(brewer.pal(12,"Set3"),brewer.pal(3,"Set2"))
gg&lt;-gg+scale_fill_manual(values=pal)
gg</code></pre>
<p>
<img src="outsiderdata.netlify.app/posts/2018-02-11-live-fast-die-young-stay-pretty/img/unnamed-chunk-19-1.png" class="img-fluid">
</p>
<p>
If you make it into your 50s as a rocker you are out of the woods for suicide, murder and accidents. Cancer supplants accidents as the leading cause after age 40. Heart disease is prevalent starting at 40 and onward. “Not Specified” rises substantially with age. I suspect it’s because rock stars who die in their prime are more reported upon.
</p>
</section>
<section id="a-couple-more-questions-suggested-by-our-discoveries." class="level1">
<h1>
A couple more questions suggested by our discoveries.
</h1>
<p>
Note the emergence and disappearance of AIDS as a cause of death from people in their late 20s to 50. Is this a function of just the 80s and 90s when AIDS was a particular scourge? Is it now a chronic, manageable, condition so that it won’t be a frequent cause of death any more? Let’s look at the number of reported AIDS deaths by year.
</p>
<pre class="sourceCode r code-with-copy"><code>deaths %&gt;% filter(General_Cause=="AIDS") %&gt;% ggplot(aes(Year))+geom_bar()</code></pre>
<p>
<img src="outsiderdata.netlify.app/posts/2018-02-11-live-fast-die-young-stay-pretty/img/unnamed-chunk-20-1.png" class="img-fluid">
</p>
<p>
There are so few instances of AIDS deaths that we can’t be confident of our hypothesis. Still, it is consistent with the dissapearance of AIDS as a cause after 2001 for many years - until 2017. The last case is Michael Freidman. I question his inclusion on this list as he was known as a Broadway song writer. Still, his was a tragic case. His disease progressed rapidly but neither he nor his friends believed it was possible that HIV was causing his symptoms. By the time he sought medical attention it was too late. <a href="https://www.nytimes.com/2017/10/11/theater/michael-friedman-aids-death-theater.html">The theatre community was stunned</a> that, in this place and time, AIDS could still kill someone.
</p>
<p>
Finally, let’s use this data to question our “live fast” assertion from above. We know deaths from all causes rise with age. Maybe the asbsolute number of accidental deaths stays the same (or even rises) but just goes down relative to other causes common to all geezers. As a reality check, let’s ask if the absolute number changes with age.
</p>
<pre class="sourceCode r code-with-copy"><code>gg&lt;-deaths  %&gt;% 
  ggplot(aes(Age,fill="grey"))+geom_histogram(bins=10)
gg&lt;-gg+labs(title='Number of Reported Deaths by Accident, by Age',
            x='Age at Death',
            y="Count")
gg&lt;-gg+geom_histogram(data=filter(deaths,General_Cause=="Accident"),
                      aes(Age,fill="black"),
                      bins=10)
gg&lt;-gg+ scale_fill_manual(name='',
                          values =c('grey'='grey','black'='black'),
                          labels = c('Accident','All Causes'))
gg</code></pre>
<p>
<img src="outsiderdata.netlify.app/posts/2018-02-11-live-fast-die-young-stay-pretty/img/unnamed-chunk-21-1.png" class="img-fluid">
</p>
<p>
The absolute number of accidental deaths falls as people age, even as total deaths rise. So, yes, rock musicians, like the rest of us, do get more careful as they age!
</p>
</section>
<section id="conclusion" class="level1">
<h1>
Conclusion
</h1>
<p>
This project started out as part of a separate project but I went down an interesting rabbit hole and realized there was something to say about the Wikipedia data set. Alas, the 80/20 rule applies. Most of the time on this project was spent iterating and tweaking the data cleaning part. The HTML manipulation was a pain. Once I had a clean data set, I breezed through the exploration and discovery part…and that was very fun. Still, I learned some more tricks about web scraping and found some surprising things in the data. Win-win!
</p>
<p>
Stay off the dope, kids!
</p>
</section>



 ]]></description>
  <category>music</category>
  <category>web scraping</category>
  <category>health</category>
  <category>gapminder</category>
  <guid>outsiderdata.netlify.app/posts/2018-02-11-live-fast-die-young-stay-pretty/2018-02-11-live-fast-die-young-stay-pretty.html</guid>
  <pubDate>Sun, 11 Feb 2018 05:00:00 GMT</pubDate>
  <media:content url="outsiderdata.netlify.app/posts/2018-02-11-live-fast-die-young-stay-pretty/img/unnamed-chunk-8-1.png" medium="image" type="image/png" height="103" width="144"/>
</item>
<item>
  <title>Plumbing the Depths of My Soul (in Facebook)</title>
  <dc:creator>Art Steinmetz</dc:creator>
  <link>outsiderdata.netlify.app/posts/2017-11-25-summary-analysis-of-my-facebook-existence/2017-11-25-summary-analysis-of-my-facebook-existence.html</link>
  <description><![CDATA[ 



<p><img src="outsiderdata.netlify.app/posts/2017-11-25-summary-analysis-of-my-facebook-existence/img/moods.gif" class="img-fluid"></p>
<p>First post! Let’s start out nice and easy. No big data machine learning or heavy stats. This post will merely explore the depths of my soul through a meta-analysis of every one of my Facebook posts. Meta-navel gazing, if you will.</p>
<p>Perhaps you are not all that interested in the plumbing the depths of <i>my</i> soul. Still, you may be interested in seeing how you can do an analyis of your own Facebook life in the comfort of your own home. If so, read on!</p>
<p>We will (lightly) cover web scraping, sentiment analysis, tests of significance and visualize it with a generous helping of <code>ggplot</code>. Note I use the <code>tidyverse</code>/<code>dplyr</code> vernacular. This is fast becoming a dialect of R. I quite like it but its syntax is different than traditional R. It produces sometimes slower, but much more readable, code. Basically, you “pipe” data tables through action verbs using the pipe operator (“%&gt;%”).</p>
<p>Let’s go do some outsider data science!</p>
<p>Start by loading needed packages.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><span class="fu" style="color: #4758AB;">library</span>(rvest)</span>
<span id="cb1-2"><span class="fu" style="color: #4758AB;">library</span>(stringr)</span>
<span id="cb1-3"><span class="fu" style="color: #4758AB;">library</span>(dplyr)</span>
<span id="cb1-4"><span class="fu" style="color: #4758AB;">library</span>(tidyverse)</span>
<span id="cb1-5"><span class="fu" style="color: #4758AB;">library</span>(tidytext)</span>
<span id="cb1-6"><span class="fu" style="color: #4758AB;">library</span>(wordcloud)</span>
<span id="cb1-7"><span class="fu" style="color: #4758AB;">library</span>(knitr)</span>
<span id="cb1-8"><span class="fu" style="color: #4758AB;">library</span>(kableExtra)</span>
<span id="cb1-9"><span class="fu" style="color: #4758AB;">library</span>(ggplot2)</span>
<span id="cb1-10"><span class="fu" style="color: #4758AB;">library</span>(zoo)</span>
<span id="cb1-11"><span class="fu" style="color: #4758AB;">library</span>(reshape2)</span>
<span id="cb1-12"><span class="fu" style="color: #4758AB;">library</span>(lubridate)</span>
<span id="cb1-13"></span>
<span id="cb1-14"></span>
<span id="cb1-15"><span class="co" style="color: #5E5E5E;">#make explicit so kableExtra doesn't complain later</span></span>
<span id="cb1-16"><span class="fu" style="color: #4758AB;">options</span>(<span class="at" style="color: #657422;">knitr.table.format =</span> <span class="st" style="color: #20794D;">"html"</span>) </span></code></pre></div>
</div>
<section id="fetch-and-clean-all-the-words-in-my-facebook-posts" class="level1">
<h1>Fetch and clean all the words in my Facebook posts</h1>
<p>Facebook lets you download a log of all your activity at https://Facebook.com/settings. Look toward the bottom of the page for the download link. You will get an email with a link to a zipped set of html files. These are what I’ll be using for the analysis.</p>
<p>First let’s get all my comments since the dawn of my Facebook existence.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1">path<span class="ot" style="color: #003B4F;">=</span><span class="st" style="color: #20794D;">'data/'</span></span>
<span id="cb2-2">raw_timeline<span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">read_html</span>(<span class="fu" style="color: #4758AB;">paste0</span>(path,<span class="st" style="color: #20794D;">"timeline.htm"</span>),<span class="at" style="color: #657422;">encoding=</span><span class="st" style="color: #20794D;">"UTC-8"</span>)</span></code></pre></div>
</div>
<p>Now that we have the raw data we need to extract the just the text of the comments. Visually inspecting the raw html file reveals that all of the comments I wrote have the tag <code>&lt;div class="comment"&gt;</code> so I construct an xpath selector to grab those nodes then get the text in them. This is what the raw html looks like:</p>
<p><code>&lt;/p&gt;&lt;p&gt;&lt;div class="meta"&gt;Thursday, November 16, 2017 at 1:17pm EST&lt;/div&gt;</code><br> <code>&lt;div class="comment"&gt;I’m sure you are all thinking “what does this mean for Al Franken?”&lt;/div&gt;</code><br> <code>&lt;/p&gt;&lt;p&gt;</code><br> <code>&lt;div class="meta"&gt;Thursday, November 16, 2017 at 10:44am EST&lt;/div&gt;</code><br> <code>Art Steinmetz shared a link.</code><br> <code>&lt;/p&gt;&lt;p&gt;</code><br></p>
<p>The challenge here is that we want to get the date also which appears BEFORE the comment and has the tag <code>&lt;div class="meta"&gt;</code>. Unfortunately, as we see above, merely sharing a link generates this tag without any comment or a different tag class so there are more <code>meta</code> classes than <code>comment</code> classes. Facebook should create a separate XML record for each log activity, but they don’t.</p>
<p>The code below seems inelegant to me. <code>for</code> loops in R are non-idiomatic and indicate somebody was steeped in a non vectorized language (like me). I tried without success to craft an xpath expression that would walk backwards when it sees a <code>comment</code> class to get the date. In the end I resorted to the devil I know, a loop.</p>
<div class="cell" data-hash="2017-11-25-summary-analysis-of-my-facebook-existence_cache/html/unnamed-chunk-3_e6e4f850cccf439ad33be058b992de4f">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1">timeline_post_nodes <span class="ot" style="color: #003B4F;">&lt;-</span> raw_timeline <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb3-2">  <span class="fu" style="color: #4758AB;">html_nodes</span>(<span class="at" style="color: #657422;">xpath=</span><span class="st" style="color: #20794D;">"//div[@class ='comment'] | //div[@class='meta']"</span>)</span>
<span id="cb3-3"></span>
<span id="cb3-4">timeline_posts1<span class="ot" style="color: #003B4F;">&lt;-</span><span class="cn" style="color: #8f5902;">NULL</span></span>
<span id="cb3-5"><span class="co" style="color: #5E5E5E;">#the bit below is the slowest part of our project. </span></span>
<span id="cb3-6"><span class="co" style="color: #5E5E5E;">#If you post multiple times a day over years it could take a while.</span></span>
<span id="cb3-7"><span class="cf" style="color: #003B4F;">for</span> (n <span class="cf" style="color: #003B4F;">in</span> <span class="dv" style="color: #AD0000;">1</span><span class="sc" style="color: #5E5E5E;">:</span><span class="fu" style="color: #4758AB;">length</span>(timeline_post_nodes)){</span>
<span id="cb3-8">  <span class="cf" style="color: #003B4F;">if</span> ( <span class="fu" style="color: #4758AB;">html_attr</span>(timeline_post_nodes[n],<span class="st" style="color: #20794D;">"class"</span>)<span class="sc" style="color: #5E5E5E;">==</span><span class="st" style="color: #20794D;">"comment"</span>){</span>
<span id="cb3-9">    post<span class="ot" style="color: #003B4F;">=</span> <span class="fu" style="color: #4758AB;">html_text</span>(timeline_post_nodes[n])</span>
<span id="cb3-10">    date<span class="ot" style="color: #003B4F;">=</span> <span class="fu" style="color: #4758AB;">html_text</span>(timeline_post_nodes[n<span class="dv" style="color: #AD0000;">-1</span>])</span>
<span id="cb3-11">    timeline_posts1<span class="ot" style="color: #003B4F;">&lt;-</span>timeline_posts1 <span class="sc" style="color: #5E5E5E;">%&gt;%</span> <span class="fu" style="color: #4758AB;">bind_rows</span>(<span class="fu" style="color: #4758AB;">tibble</span>(date,post))</span>
<span id="cb3-12">  }</span>
<span id="cb3-13">}</span></code></pre></div>
</div>
<p>The time stamps we extracted are just character strings with no quantitative meaning. Let’s convert the dates in the form of “Saturday November 18 2017 11:12am EST” to a day of the week and a POSIX date/time format that other R functions will understand. First we pull out the day of the week using the comma as a separator but this also separates the month and day from the year, which we don’t want, so we put those back together.</p>
<p>This begs the question of whether we should have used a tricker “regular expression” to accomplish this in one step. RegExes are a dark art that I have a lot of admiration for, even if I am a rank neophyte. In this exercise I didn’t think it was worth the time to figure out a “proper” solution when a “simple” one sufficed. Other times I like the puzzle challenge of coming up with a powerful RegEx. There are web sites that are a great help in building them. Try http://regex101.com, for one.</p>
<p>With a good date string in hand we can use <code>parse_date()</code> to convert it. Notice the <code>format</code> string we use to accomplish this.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1">timeline_posts<span class="ot" style="color: #003B4F;">&lt;-</span>timeline_posts1 <span class="sc" style="color: #5E5E5E;">%&gt;%</span></span>
<span id="cb4-2">  <span class="fu" style="color: #4758AB;">mutate</span>(<span class="at" style="color: #657422;">date=</span><span class="fu" style="color: #4758AB;">sub</span>(<span class="st" style="color: #20794D;">"at "</span>,<span class="st" style="color: #20794D;">""</span>,date)) <span class="sc" style="color: #5E5E5E;">%&gt;%</span></span>
<span id="cb4-3">  <span class="fu" style="color: #4758AB;">separate</span>(date,<span class="fu" style="color: #4758AB;">c</span>(<span class="st" style="color: #20794D;">"doy"</span>,<span class="st" style="color: #20794D;">"date"</span>,<span class="st" style="color: #20794D;">"yeartime"</span>),<span class="at" style="color: #657422;">sep=</span><span class="st" style="color: #20794D;">", "</span>) <span class="sc" style="color: #5E5E5E;">%&gt;%</span></span>
<span id="cb4-4">  <span class="fu" style="color: #4758AB;">transmute</span>(<span class="at" style="color: #657422;">doy=</span>doy,<span class="at" style="color: #657422;">date=</span><span class="fu" style="color: #4758AB;">paste</span>(date,yeartime),<span class="at" style="color: #657422;">post=</span>post)</span>
<span id="cb4-5"></span>
<span id="cb4-6"><span class="co" style="color: #5E5E5E;"># Now that we've pulled out the day of the week, let's make sure they show in order in plots</span></span>
<span id="cb4-7"><span class="co" style="color: #5E5E5E;"># by making doy and ordered factor.</span></span>
<span id="cb4-8">day_order<span class="ot" style="color: #003B4F;">&lt;-</span><span class="fu" style="color: #4758AB;">c</span>(<span class="st" style="color: #20794D;">"Monday"</span>,<span class="st" style="color: #20794D;">"Tuesday"</span>,<span class="st" style="color: #20794D;">"Wednesday"</span>,</span>
<span id="cb4-9">            <span class="st" style="color: #20794D;">"Thursday"</span>,<span class="st" style="color: #20794D;">"Friday"</span>,<span class="st" style="color: #20794D;">"Saturday"</span>, </span>
<span id="cb4-10">            <span class="st" style="color: #20794D;">"Sunday"</span>)</span>
<span id="cb4-11"></span>
<span id="cb4-12">timeline_posts<span class="sc" style="color: #5E5E5E;">$</span>doy<span class="ot" style="color: #003B4F;">&lt;-</span><span class="fu" style="color: #4758AB;">factor</span>(timeline_posts<span class="sc" style="color: #5E5E5E;">$</span>doy,<span class="at" style="color: #657422;">levels =</span> day_order)</span>
<span id="cb4-13"></span>
<span id="cb4-14">timeline_posts<span class="ot" style="color: #003B4F;">&lt;-</span>timeline_posts <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb4-15">  <span class="fu" style="color: #4758AB;">mutate</span>(<span class="at" style="color: #657422;">date =</span> <span class="fu" style="color: #4758AB;">str_remove</span>(date,<span class="st" style="color: #20794D;">" EST| EDT"</span>)) <span class="sc" style="color: #5E5E5E;">|&gt;</span> </span>
<span id="cb4-16">  <span class="fu" style="color: #4758AB;">mutate</span>(<span class="at" style="color: #657422;">date =</span> <span class="fu" style="color: #4758AB;">parse_datetime</span>(date,</span>
<span id="cb4-17">                               <span class="at" style="color: #657422;">format=</span><span class="st" style="color: #20794D;">"%B %d %Y %I:%M%p"</span>,</span>
<span id="cb4-18">                               <span class="at" style="color: #657422;">locale =</span> <span class="fu" style="color: #4758AB;">locale</span>(<span class="at" style="color: #657422;">tz =</span> <span class="st" style="color: #20794D;">"US/Eastern"</span>)))</span>
<span id="cb4-19"></span>
<span id="cb4-20"><span class="fu" style="color: #4758AB;">kable</span>(<span class="fu" style="color: #4758AB;">head</span>(timeline_posts[<span class="dv" style="color: #AD0000;">1</span><span class="sc" style="color: #5E5E5E;">:</span><span class="dv" style="color: #AD0000;">2</span>,])) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb4-21">  <span class="fu" style="color: #4758AB;">kable_styling</span>(<span class="at" style="color: #657422;">bootstrap_options =</span> <span class="st" style="color: #20794D;">"striped"</span>, <span class="at" style="color: #657422;">full_width =</span> <span class="cn" style="color: #8f5902;">FALSE</span>, <span class="at" style="color: #657422;">position=</span><span class="st" style="color: #20794D;">'left'</span>)</span></code></pre></div>
<div class="cell-output-display">

<table class="table table-striped" style="width: auto !important; ">
 <thead>
  <tr>
   <th style="text-align:left;"> doy </th>
   <th style="text-align:left;"> date </th>
   <th style="text-align:left;"> post </th>
  </tr>
 </thead>
<tbody>
  <tr>
   <td style="text-align:left;"> Saturday </td>
   <td style="text-align:left;"> 2017-11-18 11:12:00 </td>
   <td style="text-align:left;"> I feel cheated. When I read the fine print I see these guys haven't won the "Uniformity of Granulation" award since 1894.  I want the oatmeal that won last year! </td>
  </tr>
  <tr>
   <td style="text-align:left;"> Saturday </td>
   <td style="text-align:left;"> 2017-11-18 10:41:00 </td>
   <td style="text-align:left;"> I had a chance to visit Shenzhen this year.  The hardware scene is reminiscent of Blade Runner as you'll see.  This guy prowls the markets to make his own iPhone from scratch. </td>
  </tr>
</tbody>
</table>

</div>
</div>
<p>We now have over 2000 text strings, each representing one post. Since we are working at the word level we need to break up each post into its constituent words.</p>
<p>For much of this analysis I am following the example shown at https://cran.r-project.org/web/packages/tidytext/vignettes/tidytext.html.</p>
<p>The unnest_tokens function from the ‘tidytext’ package lets us convert a dataframe with a text column to be one-word-per-row dataframe. How many words are there?</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1">my_post_words<span class="ot" style="color: #003B4F;">&lt;-</span>  timeline_posts <span class="sc" style="color: #5E5E5E;">%&gt;%</span></span>
<span id="cb5-2">  <span class="fu" style="color: #4758AB;">unnest_tokens</span>(word, post)</span>
<span id="cb5-3"></span>
<span id="cb5-4"><span class="fu" style="color: #4758AB;">nrow</span>(my_post_words)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 51347</code></pre>
</div>
</div>
<p>So we have over fifty thousand words. A lot of them are going to be uninteresting. Although, given that Facebook posts are an excercise in narcissim, you might say all of them are uninteresting to anybody but me.</p>
<p>Anyway, lets press on. We can use the <code>stop_words</code> data set included with tidytext to to strip out the superfluous words. Note this includes words like ‘accordingly’ which convey little meaning but might be useful in revealing idiosyncratic writting patterns, much like people punctuate their speech with vocal pauses like “like” and “right.” How many words are left after that?</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><span class="fu" style="color: #4758AB;">data</span>(<span class="st" style="color: #20794D;">"stop_words"</span>)</span>
<span id="cb7-2">cleaned_post_words <span class="ot" style="color: #003B4F;">&lt;-</span> my_post_words <span class="sc" style="color: #5E5E5E;">%&gt;%</span></span>
<span id="cb7-3">  <span class="fu" style="color: #4758AB;">anti_join</span>(stop_words,<span class="at" style="color: #657422;">by=</span><span class="st" style="color: #20794D;">'word'</span>)</span>
<span id="cb7-4"></span>
<span id="cb7-5"><span class="fu" style="color: #4758AB;">nrow</span>(cleaned_post_words)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 22465</code></pre>
</div>
</div>
</section>
<section id="look-at-the-most-common-words" class="level1">
<h1>Look at the most common words</h1>
<p>So now our data set is clean and tidy. Let’s answer some questions. What are the most common words I use in posts.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1">popular_words<span class="ot" style="color: #003B4F;">&lt;-</span>cleaned_post_words <span class="sc" style="color: #5E5E5E;">%&gt;%</span></span>
<span id="cb9-2">  <span class="fu" style="color: #4758AB;">count</span>(word, <span class="at" style="color: #657422;">sort =</span> <span class="cn" style="color: #8f5902;">TRUE</span>)</span>
<span id="cb9-3"><span class="fu" style="color: #4758AB;">kable</span>(popular_words[<span class="dv" style="color: #AD0000;">1</span><span class="sc" style="color: #5E5E5E;">:</span><span class="dv" style="color: #AD0000;">10</span>,]) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb9-4">  <span class="fu" style="color: #4758AB;">kable_styling</span>(<span class="at" style="color: #657422;">bootstrap_options =</span> <span class="st" style="color: #20794D;">"striped"</span>, <span class="at" style="color: #657422;">full_width =</span> <span class="cn" style="color: #8f5902;">FALSE</span>, <span class="at" style="color: #657422;">position=</span><span class="st" style="color: #20794D;">'left'</span>)</span></code></pre></div>
<div class="cell-output-display">

<table class="table table-striped" style="width: auto !important; ">
 <thead>
  <tr>
   <th style="text-align:left;"> word </th>
   <th style="text-align:right;"> n </th>
  </tr>
 </thead>
<tbody>
  <tr>
   <td style="text-align:left;"> day </td>
   <td style="text-align:right;"> 111 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> â </td>
   <td style="text-align:right;"> 101 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> people </td>
   <td style="text-align:right;"> 97 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> time </td>
   <td style="text-align:right;"> 92 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> kids </td>
   <td style="text-align:right;"> 84 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> love </td>
   <td style="text-align:right;"> 69 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> carrie </td>
   <td style="text-align:right;"> 60 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> guy </td>
   <td style="text-align:right;"> 59 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> friends </td>
   <td style="text-align:right;"> 56 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> art </td>
   <td style="text-align:right;"> 48 </td>
  </tr>
</tbody>
</table>

</div>
</div>
<p>I don’t know where that “a-hat” character comes from but let’s get rid of it.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1">cleaned_post_words<span class="ot" style="color: #003B4F;">&lt;-</span> cleaned_post_words<span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb10-2">  <span class="fu" style="color: #4758AB;">mutate</span>(<span class="at" style="color: #657422;">word=</span><span class="fu" style="color: #4758AB;">str_replace</span>(word,<span class="st" style="color: #20794D;">"â"</span>,<span class="st" style="color: #20794D;">""</span>)) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb10-3">  <span class="fu" style="color: #4758AB;">filter</span>(<span class="fu" style="color: #4758AB;">str_length</span>(word)<span class="sc" style="color: #5E5E5E;">&gt;</span><span class="dv" style="color: #AD0000;">0</span>)</span>
<span id="cb10-4">popular_words<span class="ot" style="color: #003B4F;">&lt;-</span>cleaned_post_words <span class="sc" style="color: #5E5E5E;">%&gt;%</span></span>
<span id="cb10-5">  <span class="fu" style="color: #4758AB;">count</span>(word, <span class="at" style="color: #657422;">sort =</span> <span class="cn" style="color: #8f5902;">TRUE</span>)</span></code></pre></div>
</div>
<p>After we strip out stop words we have less then 10,000 “real” words left.</p>
<p>Good to see that my wife’s name is one of my most used words. “Kids,” “friends,” and “love” are no surprise. What’s a good way to visualize this? Word cloud!</p>
<p>I love word clouds! We can easily display the most used words this way using the <code>wordcloud</code> package.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><span class="co" style="color: #5E5E5E;"># We love wordclouds!</span></span>
<span id="cb11-2"><span class="co" style="color: #5E5E5E;">#scalefactor magnifies differences for wordcloud</span></span>
<span id="cb11-3">scaleFactor<span class="ot" style="color: #003B4F;">=</span><span class="fl" style="color: #AD0000;">1.3</span></span>
<span id="cb11-4">maxWords <span class="ot" style="color: #003B4F;">=</span> <span class="dv" style="color: #AD0000;">200</span></span>
<span id="cb11-5"></span>
<span id="cb11-6"></span>
<span id="cb11-7"><span class="fu" style="color: #4758AB;">wordcloud</span>(<span class="at" style="color: #657422;">words =</span> popular_words<span class="sc" style="color: #5E5E5E;">$</span>word, </span>
<span id="cb11-8">          <span class="at" style="color: #657422;">freq =</span> popular_words<span class="sc" style="color: #5E5E5E;">$</span>n<span class="sc" style="color: #5E5E5E;">^</span>scaleFactor,</span>
<span id="cb11-9">          <span class="at" style="color: #657422;">max.words=</span>maxWords, </span>
<span id="cb11-10">          <span class="at" style="color: #657422;">random.order=</span><span class="cn" style="color: #8f5902;">FALSE</span>,<span class="at" style="color: #657422;">rot.per=</span><span class="fl" style="color: #AD0000;">0.35</span>, </span>
<span id="cb11-11">          <span class="at" style="color: #657422;">colors=</span><span class="fu" style="color: #4758AB;">brewer.pal</span>(<span class="dv" style="color: #AD0000;">8</span>, <span class="st" style="color: #20794D;">"Dark2"</span>),</span>
<span id="cb11-12">          <span class="at" style="color: #657422;">scale =</span> <span class="fu" style="color: #4758AB;">c</span>(<span class="dv" style="color: #AD0000;">3</span>,.<span class="dv" style="color: #AD0000;">3</span>))</span></code></pre></div>
<div class="cell-output-display">
<p><img src="outsiderdata.netlify.app/posts/2017-11-25-summary-analysis-of-my-facebook-existence/2017-11-25-summary-analysis-of-my-facebook-existence_files/figure-html/unnamed-chunk-9-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>I mentioned “Obama” about as often as I mentioned “beer.”</p>
</section>
<section id="do-some-sentiment-analysis" class="level1">
<h1>Do some sentiment analysis</h1>
<p>I used to be jerk. But, given my age, I am entitled to call myself a curmudgeon instead. That sounds nicer somehow, and excuses my negative reaction to everything. However, given how internet discourse easily sinks into a tit-for-tat of profane hatred, I try to go against type, accentuate the positive and say nothing if I can’t say something nice. That’s the idea. How does my sour nature interact with my better intentions? We can use sentiment analysis to find out. The tidytext package also has serveral lexicons with thousands of words coded by their sentiment. Refer to http://tidytextmining.com for an excellent tutorial on this. Obviously, the isolated word approach has limitations. Context matters and by taking one word at a time we don’t capture that. So, with that caveat, how much of a downer am I?</p>
<p>First, let’s look at the sentiment of my posts on a binary basis. Is the word positive or negative? The “bing” lexicon scores thousands of words that way. Obviously, not all the words we used are in the data set. About a third are, though.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1">cleaned_post_words <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb12-2">  <span class="fu" style="color: #4758AB;">inner_join</span>(<span class="fu" style="color: #4758AB;">get_sentiments</span>(<span class="st" style="color: #20794D;">'bing'</span>),<span class="at" style="color: #657422;">by=</span><span class="st" style="color: #20794D;">"word"</span>) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb12-3">  <span class="fu" style="color: #4758AB;">group_by</span>(sentiment) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb12-4">  <span class="fu" style="color: #4758AB;">summarize</span>(<span class="at" style="color: #657422;">count=</span><span class="fu" style="color: #4758AB;">n</span>()) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb12-5">  <span class="fu" style="color: #4758AB;">kable</span>() <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb12-6">  <span class="fu" style="color: #4758AB;">kable_styling</span>(<span class="at" style="color: #657422;">bootstrap_options =</span> <span class="st" style="color: #20794D;">"striped"</span>, <span class="at" style="color: #657422;">full_width =</span> <span class="cn" style="color: #8f5902;">FALSE</span>, <span class="at" style="color: #657422;">position=</span><span class="st" style="color: #20794D;">'left'</span>)</span></code></pre></div>
<div class="cell-output-display">

<table class="table table-striped" style="width: auto !important; ">
 <thead>
  <tr>
   <th style="text-align:left;"> sentiment </th>
   <th style="text-align:right;"> count </th>
  </tr>
 </thead>
<tbody>
  <tr>
   <td style="text-align:left;"> negative </td>
   <td style="text-align:right;"> 1678 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> positive </td>
   <td style="text-align:right;"> 1412 </td>
  </tr>
</tbody>
</table>

</div>
</div>
<p>Well, then. So I am a downer, on a net basis, but not terribly so.</p>
<p>We can make this into a word cloud, too! Here are the words I used divided by sentiment.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1">cleaned_post_words <span class="sc" style="color: #5E5E5E;">%&gt;%</span></span>
<span id="cb13-2">  <span class="fu" style="color: #4758AB;">inner_join</span>(<span class="fu" style="color: #4758AB;">get_sentiments</span>(<span class="st" style="color: #20794D;">'bing'</span>),<span class="at" style="color: #657422;">by=</span><span class="st" style="color: #20794D;">"word"</span>) <span class="sc" style="color: #5E5E5E;">%&gt;%</span></span>
<span id="cb13-3">  <span class="fu" style="color: #4758AB;">count</span>(word, sentiment, <span class="at" style="color: #657422;">sort =</span> <span class="cn" style="color: #8f5902;">TRUE</span>) <span class="sc" style="color: #5E5E5E;">%&gt;%</span></span>
<span id="cb13-4">  <span class="fu" style="color: #4758AB;">acast</span>(word <span class="sc" style="color: #5E5E5E;">~</span> sentiment, <span class="at" style="color: #657422;">value.var =</span> <span class="st" style="color: #20794D;">"n"</span>, <span class="at" style="color: #657422;">fill =</span> <span class="dv" style="color: #AD0000;">0</span>) <span class="sc" style="color: #5E5E5E;">%&gt;%</span></span>
<span id="cb13-5">  <span class="fu" style="color: #4758AB;">comparison.cloud</span>(<span class="at" style="color: #657422;">colors =</span> <span class="fu" style="color: #4758AB;">c</span>(<span class="st" style="color: #20794D;">"#F8766D"</span>, <span class="st" style="color: #20794D;">"#00BFC4"</span>),</span>
<span id="cb13-6">                   <span class="at" style="color: #657422;">max.words =</span> <span class="dv" style="color: #AD0000;">100</span>)</span></code></pre></div>
<div class="cell-output-display">
<p><img src="outsiderdata.netlify.app/posts/2017-11-25-summary-analysis-of-my-facebook-existence/2017-11-25-summary-analysis-of-my-facebook-existence_files/figure-html/unnamed-chunk-11-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Wait a minute! “Trump” is scored as a positive sentiment word! Is this a hidden statement by the author of the lexicon?! Doubtful. It’s “trump,” as in “spades trumps clubs,” not as a proper name. And why is “funny” a negative word? I guess it’s “funny strange,” not “funny ha-ha.” It shows the limitations of this kind of thing.</p>
<p>A different lexicon scores each word’s sentiment on a scale of minus to positive five. This seems pretty subjective to me but has the benefit of letting us add up the numbers to get a net score. What is my sentiment score over all words I’ve ever written on Facebook (not all, the log doesn’t include comments to other’s posts).</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1">sentiment_score<span class="ot" style="color: #003B4F;">&lt;-</span>cleaned_post_words <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb14-2">  <span class="fu" style="color: #4758AB;">inner_join</span>(<span class="fu" style="color: #4758AB;">get_sentiments</span>(<span class="st" style="color: #20794D;">'afinn'</span>),<span class="at" style="color: #657422;">by=</span><span class="st" style="color: #20794D;">"word"</span>) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb14-3">  <span class="fu" style="color: #4758AB;">pull</span>(value) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb14-4">  <span class="fu" style="color: #4758AB;">mean</span>()</span>
<span id="cb14-5">sentiment_score</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.1610233</code></pre>
</div>
</div>
<p>Well, this draws an slightly different conclusion. The net score of my sentiment is +0.16 out of range of -5 to +5. Just barely happy. While I may use more negative than positive words, my positive words are more positive. I suspect the word “love” which we already saw is frequently used (though it is “only” a “3”) accounts for this.</p>
<p>What were my most negative words?</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1">word_scores<span class="ot" style="color: #003B4F;">&lt;-</span>cleaned_post_words <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb16-2">  <span class="fu" style="color: #4758AB;">inner_join</span>(<span class="fu" style="color: #4758AB;">get_sentiments</span>(<span class="st" style="color: #20794D;">'afinn'</span>),<span class="at" style="color: #657422;">by=</span><span class="st" style="color: #20794D;">"word"</span>) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb16-3">  <span class="fu" style="color: #4758AB;">group_by</span>(word,value) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> <span class="fu" style="color: #4758AB;">summarise</span>(<span class="at" style="color: #657422;">count=</span><span class="fu" style="color: #4758AB;">n</span>())</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>`summarise()` has grouped output by 'word'. You can override using the
`.groups` argument.</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1">word_scores <span class="sc" style="color: #5E5E5E;">%&gt;%</span></span>
<span id="cb18-2">        <span class="fu" style="color: #4758AB;">arrange</span>((value)) <span class="sc" style="color: #5E5E5E;">%&gt;%</span></span>
<span id="cb18-3">        <span class="fu" style="color: #4758AB;">ungroup</span>() <span class="sc" style="color: #5E5E5E;">%&gt;%</span></span>
<span id="cb18-4">        .[<span class="dv" style="color: #AD0000;">1</span><span class="sc" style="color: #5E5E5E;">:</span><span class="dv" style="color: #AD0000;">10</span>,] <span class="sc" style="color: #5E5E5E;">%&gt;%</span></span>
<span id="cb18-5">        <span class="fu" style="color: #4758AB;">kable</span>() <span class="sc" style="color: #5E5E5E;">%&gt;%</span></span>
<span id="cb18-6">  <span class="fu" style="color: #4758AB;">kable_styling</span>(<span class="at" style="color: #657422;">bootstrap_options =</span> <span class="st" style="color: #20794D;">"striped"</span>, <span class="at" style="color: #657422;">full_width =</span> <span class="cn" style="color: #8f5902;">FALSE</span>, <span class="at" style="color: #657422;">position=</span><span class="st" style="color: #20794D;">'left'</span>)</span></code></pre></div>
</div>
<p>This is a family blog so I comment out the code that displays the worst words. Suffice it to say, they are the usual curse words and forms thereof. I am cringing right now. Did I say those things? Yes, well not often, at least, once or twice is typical for each.</p>
<p>As I mentioned above, the limitation of this analysis is that it lacks context. For instance, did I call someone a slut? I was briefly horrified when I saw that word. Here is the word in context from 2014: “Less slut-shaming and more perp-jailing.”</p>
<p>All these negative words carry more power for me, an old-geezer, than for kids today (kids today!) who let f-bombs roll off their tongues with uncomfortable (to me) ease. Get off my lawn!</p>
<p>What were my positive words?</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb19" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1">word_scores <span class="sc" style="color: #5E5E5E;">%&gt;%</span> <span class="fu" style="color: #4758AB;">arrange</span>(<span class="fu" style="color: #4758AB;">desc</span>(value)) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb19-2">  <span class="fu" style="color: #4758AB;">ungroup</span>() <span class="sc" style="color: #5E5E5E;">%&gt;%</span></span>
<span id="cb19-3">  .[<span class="dv" style="color: #AD0000;">1</span><span class="sc" style="color: #5E5E5E;">:</span><span class="dv" style="color: #AD0000;">10</span>,] <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb19-4">  <span class="fu" style="color: #4758AB;">kable</span>() <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb19-5">  <span class="fu" style="color: #4758AB;">kable_styling</span>(<span class="at" style="color: #657422;">bootstrap_options =</span> <span class="st" style="color: #20794D;">"striped"</span>, <span class="at" style="color: #657422;">full_width =</span> <span class="cn" style="color: #8f5902;">FALSE</span>, <span class="at" style="color: #657422;">position=</span><span class="st" style="color: #20794D;">'left'</span>)</span></code></pre></div>
<div class="cell-output-display">

<table class="table table-striped" style="width: auto !important; ">
 <thead>
  <tr>
   <th style="text-align:left;"> word </th>
   <th style="text-align:right;"> value </th>
   <th style="text-align:right;"> count </th>
  </tr>
 </thead>
<tbody>
  <tr>
   <td style="text-align:left;"> breathtaking </td>
   <td style="text-align:right;"> 5 </td>
   <td style="text-align:right;"> 1 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> outstanding </td>
   <td style="text-align:right;"> 5 </td>
   <td style="text-align:right;"> 1 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> thrilled </td>
   <td style="text-align:right;"> 5 </td>
   <td style="text-align:right;"> 3 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> amazing </td>
   <td style="text-align:right;"> 4 </td>
   <td style="text-align:right;"> 19 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> awesome </td>
   <td style="text-align:right;"> 4 </td>
   <td style="text-align:right;"> 19 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> brilliant </td>
   <td style="text-align:right;"> 4 </td>
   <td style="text-align:right;"> 5 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> fabulous </td>
   <td style="text-align:right;"> 4 </td>
   <td style="text-align:right;"> 1 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> fantastic </td>
   <td style="text-align:right;"> 4 </td>
   <td style="text-align:right;"> 2 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> fun </td>
   <td style="text-align:right;"> 4 </td>
   <td style="text-align:right;"> 45 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> funnier </td>
   <td style="text-align:right;"> 4 </td>
   <td style="text-align:right;"> 1 </td>
  </tr>
</tbody>
</table>

</div>
</div>
<p>Whew! I feel better now. Everything is awesome!</p>
<p>Did I get happier or sadder over time? We’ll answer that question in a minute.</p>
</section>
<section id="time-patterns" class="level1">
<h1>Time Patterns</h1>
<p>The foregoing analysis just includes posts on my timeline where I made a comment. If we want to know things like when I’m active on Facebook we need to look at all activity. Again, Facebook doesn’t separately tag different activities. Let’s go back over all the activity to pull out just the timestamps, but all of them this time.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb20" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb20-1">activity_times <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">tibble</span>(<span class="at" style="color: #657422;">date =</span> raw_timeline <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb20-2">                             <span class="fu" style="color: #4758AB;">html_nodes</span>(<span class="at" style="color: #657422;">xpath=</span><span class="st" style="color: #20794D;">"//div[@class='meta']"</span>) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb20-3">                             <span class="fu" style="color: #4758AB;">html_text</span>()</span>
<span id="cb20-4">                              ) <span class="sc" style="color: #5E5E5E;">%&gt;%</span></span>
<span id="cb20-5">  <span class="fu" style="color: #4758AB;">mutate</span>(<span class="at" style="color: #657422;">date=</span><span class="fu" style="color: #4758AB;">sub</span>(<span class="st" style="color: #20794D;">"at "</span>,<span class="st" style="color: #20794D;">""</span>,date)) <span class="sc" style="color: #5E5E5E;">%&gt;%</span></span>
<span id="cb20-6">  <span class="fu" style="color: #4758AB;">separate</span>(date,<span class="fu" style="color: #4758AB;">c</span>(<span class="st" style="color: #20794D;">"doy"</span>,<span class="st" style="color: #20794D;">"date"</span>,<span class="st" style="color: #20794D;">"yeartime"</span>),<span class="at" style="color: #657422;">sep=</span><span class="st" style="color: #20794D;">", "</span>) <span class="sc" style="color: #5E5E5E;">%&gt;%</span></span>
<span id="cb20-7">  <span class="fu" style="color: #4758AB;">transmute</span>(<span class="at" style="color: #657422;">doy=</span>doy,<span class="at" style="color: #657422;">date=</span><span class="fu" style="color: #4758AB;">paste</span>(date,yeartime)) <span class="sc" style="color: #5E5E5E;">%&gt;%</span></span>
<span id="cb20-8">  <span class="fu" style="color: #4758AB;">mutate</span>(<span class="at" style="color: #657422;">date =</span> <span class="fu" style="color: #4758AB;">str_remove</span>(date,<span class="st" style="color: #20794D;">" EST| EDT"</span>)) <span class="sc" style="color: #5E5E5E;">|&gt;</span> </span>
<span id="cb20-9">  <span class="fu" style="color: #4758AB;">mutate</span>(<span class="at" style="color: #657422;">date =</span> <span class="fu" style="color: #4758AB;">parse_datetime</span>(date,</span>
<span id="cb20-10">                               <span class="at" style="color: #657422;">format=</span><span class="st" style="color: #20794D;">"%B %d %Y %I:%M%p"</span>,</span>
<span id="cb20-11">                               <span class="at" style="color: #657422;">locale =</span> <span class="fu" style="color: #4758AB;">locale</span>(<span class="at" style="color: #657422;">tz =</span> <span class="st" style="color: #20794D;">"US/Eastern"</span>)))</span>
<span id="cb20-12"></span>
<span id="cb20-13">activity_times<span class="sc" style="color: #5E5E5E;">$</span>doy<span class="ot" style="color: #003B4F;">&lt;-</span><span class="fu" style="color: #4758AB;">factor</span>(activity_times<span class="sc" style="color: #5E5E5E;">$</span>doy,<span class="at" style="color: #657422;">levels =</span> day_order)</span></code></pre></div>
</div>
<p>Let’s ask a couple questions. What day of the week am I most active on Facebook?</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb21" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><span class="co" style="color: #5E5E5E;">#make sure days of week are in sequential order. Monday first</span></span>
<span id="cb21-2"></span>
<span id="cb21-3">activity_times <span class="sc" style="color: #5E5E5E;">%&gt;%</span> <span class="fu" style="color: #4758AB;">ggplot</span>(<span class="fu" style="color: #4758AB;">aes</span>(doy))<span class="sc" style="color: #5E5E5E;">+</span><span class="fu" style="color: #4758AB;">geom_bar</span>()<span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb21-4">  <span class="fu" style="color: #4758AB;">labs</span>(<span class="at" style="color: #657422;">title=</span><span class="st" style="color: #20794D;">'Facebook Activity'</span>, <span class="at" style="color: #657422;">x=</span><span class="st" style="color: #20794D;">'Weekday'</span>,<span class="at" style="color: #657422;">y=</span><span class="st" style="color: #20794D;">'Posts'</span>)</span></code></pre></div>
<div class="cell-output-display">
<p><img src="outsiderdata.netlify.app/posts/2017-11-25-summary-analysis-of-my-facebook-existence/2017-11-25-summary-analysis-of-my-facebook-existence_files/figure-html/unnamed-chunk-17-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Monday stands out. I didn’t realize this. Perhaps I come to work Monday morning and catch up with the news which prompts me to post.</p>
<p>Am I more cranky on different days?</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb22" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb22-1"><span class="co" style="color: #5E5E5E;">#cleaned_post_words$doy&lt;-factor(cleaned_post_words$doy,levels = day_order)</span></span>
<span id="cb22-2"></span>
<span id="cb22-3">word_scores_by_weekday<span class="ot" style="color: #003B4F;">&lt;-</span>cleaned_post_words <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb22-4">  <span class="fu" style="color: #4758AB;">inner_join</span>(<span class="fu" style="color: #4758AB;">get_sentiments</span>(<span class="st" style="color: #20794D;">'afinn'</span>),<span class="at" style="color: #657422;">by=</span><span class="st" style="color: #20794D;">"word"</span>) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb22-5">  <span class="fu" style="color: #4758AB;">group_by</span>(doy)</span>
<span id="cb22-6"></span>
<span id="cb22-7">word_scores_by_weekday <span class="sc" style="color: #5E5E5E;">%&gt;%</span></span>
<span id="cb22-8">  <span class="fu" style="color: #4758AB;">summarise</span>(<span class="at" style="color: #657422;">mood=</span><span class="fu" style="color: #4758AB;">mean</span>(value)) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb22-9">  <span class="fu" style="color: #4758AB;">ggplot</span>(<span class="fu" style="color: #4758AB;">aes</span>(<span class="at" style="color: #657422;">x=</span>doy,<span class="at" style="color: #657422;">y=</span>mood))<span class="sc" style="color: #5E5E5E;">+</span><span class="fu" style="color: #4758AB;">geom_col</span>()<span class="sc" style="color: #5E5E5E;">+</span><span class="fu" style="color: #4758AB;">labs</span>(<span class="at" style="color: #657422;">x=</span><span class="st" style="color: #20794D;">"Weekday"</span>,<span class="at" style="color: #657422;">y=</span><span class="st" style="color: #20794D;">"Mood Score"</span>)</span></code></pre></div>
<div class="cell-output-display">
<p><img src="outsiderdata.netlify.app/posts/2017-11-25-summary-analysis-of-my-facebook-existence/2017-11-25-summary-analysis-of-my-facebook-existence_files/figure-html/unnamed-chunk-18-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>This is interesting! I am in a relatively good mood on Monday! It’s the middle of the week when I tend to use more negative words. Then I pick up going into the weekend.</p>
<p>Remember though, these are numbers of small magnitude. Are the variations statistically significant? Let’s compare Tuesday to Sunday and (which have the most extreme differences). First visually then with a t-test to see if the differences are significant. For our hypothesis we assume the the true difference in the average mood on Monday is no different than the average mood on Sunday. Based on the differences we see, can we reject this hypothesis?</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb23" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb23-1">sunday_moods<span class="ot" style="color: #003B4F;">&lt;-</span>word_scores_by_weekday <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb23-2">  <span class="fu" style="color: #4758AB;">filter</span>(doy<span class="sc" style="color: #5E5E5E;">==</span><span class="st" style="color: #20794D;">"Sunday"</span>) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb23-3">  <span class="fu" style="color: #4758AB;">group_by</span>(doy,date) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb23-4">  <span class="fu" style="color: #4758AB;">summarise</span>(<span class="at" style="color: #657422;">mood=</span><span class="fu" style="color: #4758AB;">mean</span>(value)) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb23-5">  <span class="fu" style="color: #4758AB;">select</span>(doy,mood)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>`summarise()` has grouped output by 'doy'. You can override using the `.groups`
argument.</code></pre>
</div>
<div class="sourceCode cell-code" id="cb25" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb25-1">tuesday_moods<span class="ot" style="color: #003B4F;">&lt;-</span>word_scores_by_weekday <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb25-2">  <span class="fu" style="color: #4758AB;">filter</span>(doy<span class="sc" style="color: #5E5E5E;">==</span><span class="st" style="color: #20794D;">"Tuesday"</span>) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb25-3">  <span class="fu" style="color: #4758AB;">group_by</span>(doy,date) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb25-4">  <span class="fu" style="color: #4758AB;">summarise</span>(<span class="at" style="color: #657422;">mood=</span><span class="fu" style="color: #4758AB;">mean</span>(value)) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb25-5">  <span class="fu" style="color: #4758AB;">select</span>(doy,mood)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>`summarise()` has grouped output by 'doy'. You can override using the `.groups`
argument.</code></pre>
</div>
<div class="sourceCode cell-code" id="cb27" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb27-1"><span class="fu" style="color: #4758AB;">bind_rows</span>(tuesday_moods,sunday_moods) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> <span class="fu" style="color: #4758AB;">ggplot</span>(<span class="fu" style="color: #4758AB;">aes</span>(mood,<span class="at" style="color: #657422;">fill=</span>doy))<span class="sc" style="color: #5E5E5E;">+</span><span class="fu" style="color: #4758AB;">geom_density</span>(<span class="at" style="color: #657422;">alpha=</span><span class="fl" style="color: #AD0000;">0.7</span>)</span></code></pre></div>
<div class="cell-output-display">
<p><img src="outsiderdata.netlify.app/posts/2017-11-25-summary-analysis-of-my-facebook-existence/2017-11-25-summary-analysis-of-my-facebook-existence_files/figure-html/unnamed-chunk-19-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb28" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb28-1"><span class="fu" style="color: #4758AB;">t.test</span>(tuesday_moods<span class="sc" style="color: #5E5E5E;">$</span>mood,sunday_moods<span class="sc" style="color: #5E5E5E;">$</span>mood)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
    Welch Two Sample t-test

data:  tuesday_moods$mood and sunday_moods$mood
t = -0.97824, df = 332.11, p-value = 0.3287
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -0.6292549  0.2112694
sample estimates:
 mean of x  mean of y 
0.06088435 0.26987711 </code></pre>
</div>
</div>
<p>Rats! It looks like our “interesting” observation is not interesting. The p-value of 0.32 is below 2, so we can’t reject our hypothesis. The difference in mean sentiment for Sunday and Tuesday would have to be beyond the confidence interval to give us acceptable certainty that I am most cranky on Tuesday.</p>
<p>We can’t get too excited by the density plot, either. My posts are bi-modally distributed but, given the relatively short length of my posts, chances are I use just one sentiment-loaded word and that skews the distribution. Again, small sample sizes are the problem. Pretty picture, though!</p>
<p>What times am I most active?</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb30" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb30-1">hours<span class="ot" style="color: #003B4F;">&lt;-</span>cleaned_post_words <span class="sc" style="color: #5E5E5E;">%&gt;%</span> <span class="fu" style="color: #4758AB;">mutate</span>(<span class="at" style="color: #657422;">hour=</span><span class="fu" style="color: #4758AB;">hour</span>(date))</span>
<span id="cb30-2"></span>
<span id="cb30-3">hours <span class="sc" style="color: #5E5E5E;">%&gt;%</span> <span class="fu" style="color: #4758AB;">ggplot</span>(<span class="fu" style="color: #4758AB;">aes</span>(hour))<span class="sc" style="color: #5E5E5E;">+</span><span class="fu" style="color: #4758AB;">geom_bar</span>()<span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb30-4">  <span class="fu" style="color: #4758AB;">labs</span>(<span class="at" style="color: #657422;">title=</span><span class="st" style="color: #20794D;">'Facebook Activity'</span>, <span class="at" style="color: #657422;">x=</span><span class="st" style="color: #20794D;">'Hour'</span>,<span class="at" style="color: #657422;">y=</span><span class="st" style="color: #20794D;">'Posts'</span>)</span></code></pre></div>
<div class="cell-output-display">
<p><img src="outsiderdata.netlify.app/posts/2017-11-25-summary-analysis-of-my-facebook-existence/2017-11-25-summary-analysis-of-my-facebook-existence_files/figure-html/unnamed-chunk-21-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>#Trends over Time</p>
<p>Is there any trend to my Facebook activity over time? Let’s bucket the posts by month and look for a pattern.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb31" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb31-1">activity_times <span class="ot" style="color: #003B4F;">&lt;-</span> activity_times <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb31-2">  <span class="co" style="color: #5E5E5E;">#filter dates before I joined as bad data</span></span>
<span id="cb31-3">  <span class="fu" style="color: #4758AB;">filter</span>(date<span class="sc" style="color: #5E5E5E;">&gt;</span><span class="fu" style="color: #4758AB;">as.Date</span>(<span class="st" style="color: #20794D;">"2008-01-01"</span>)) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb31-4">  <span class="fu" style="color: #4758AB;">mutate</span>(<span class="at" style="color: #657422;">month=</span><span class="fu" style="color: #4758AB;">as.yearmon</span>(date))</span>
<span id="cb31-5"></span>
<span id="cb31-6">activity_times <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb31-7">  <span class="fu" style="color: #4758AB;">ggplot</span>(<span class="fu" style="color: #4758AB;">aes</span>(<span class="fu" style="color: #4758AB;">as.Date</span>(month))) <span class="sc" style="color: #5E5E5E;">+</span> <span class="fu" style="color: #4758AB;">geom_bar</span>() <span class="sc" style="color: #5E5E5E;">+</span><span class="fu" style="color: #4758AB;">labs</span>(<span class="at" style="color: #657422;">x=</span><span class="st" style="color: #20794D;">"Month"</span>,<span class="at" style="color: #657422;">y=</span><span class="st" style="color: #20794D;">"Posts"</span>)</span></code></pre></div>
<div class="cell-output-display">
<p><img src="outsiderdata.netlify.app/posts/2017-11-25-summary-analysis-of-my-facebook-existence/2017-11-25-summary-analysis-of-my-facebook-existence_files/figure-html/unnamed-chunk-22-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>What’s up with the beginning of 2013 and December in 2015? Looking at the raw activity log I see that I briefly let Spotify tell you what I was listening to via Facebook. That generated a lot of activity. I turned it off after a couple weeks. In late 2016 around the election we also see an uptick in activity. Otherwise there have been pretty mild ebbs and flows, averaging about 30 posts per month.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb32" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb32-1">activity_times <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb32-2">  <span class="fu" style="color: #4758AB;">group_by</span>(month) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb32-3">  <span class="fu" style="color: #4758AB;">summarise</span>(<span class="at" style="color: #657422;">n=</span><span class="fu" style="color: #4758AB;">n</span>()) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb32-4">  <span class="fu" style="color: #4758AB;">summarise</span>(<span class="at" style="color: #657422;">avg_per_month=</span><span class="fu" style="color: #4758AB;">mean</span>(n)) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb32-5">  <span class="fu" style="color: #4758AB;">kable</span>() <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb32-6">  <span class="fu" style="color: #4758AB;">kable_styling</span>(<span class="at" style="color: #657422;">bootstrap_options =</span> <span class="st" style="color: #20794D;">"striped"</span>, <span class="at" style="color: #657422;">full_width =</span> <span class="cn" style="color: #8f5902;">FALSE</span>, <span class="at" style="color: #657422;">position=</span><span class="st" style="color: #20794D;">'left'</span>)</span></code></pre></div>
<div class="cell-output-display">

<table class="table table-striped" style="width: auto !important; ">
 <thead>
  <tr>
   <th style="text-align:right;"> avg_per_month </th>
  </tr>
 </thead>
<tbody>
  <tr>
   <td style="text-align:right;"> 29.42105 </td>
  </tr>
</tbody>
</table>

</div>
</div>
</section>
<section id="does-my-mood-change-over-time" class="level1">
<h1>Does my mood change over time?</h1>
<p>We can repeat the sentiment analysis from above but bucket it by month.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb33" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb33-1">word_scores_by_month<span class="ot" style="color: #003B4F;">&lt;-</span>cleaned_post_words <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb33-2">  <span class="fu" style="color: #4758AB;">inner_join</span>(<span class="fu" style="color: #4758AB;">get_sentiments</span>(<span class="st" style="color: #20794D;">'afinn'</span>),<span class="at" style="color: #657422;">by=</span><span class="st" style="color: #20794D;">"word"</span>) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb33-3">  <span class="fu" style="color: #4758AB;">select</span>(date, word,value) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb33-4">  <span class="fu" style="color: #4758AB;">mutate</span>(<span class="at" style="color: #657422;">yearmonth=</span><span class="fu" style="color: #4758AB;">as.yearmon</span>(date)) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb33-5">  <span class="fu" style="color: #4758AB;">group_by</span>(yearmonth) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> <span class="fu" style="color: #4758AB;">summarise</span>(<span class="at" style="color: #657422;">mood=</span><span class="fu" style="color: #4758AB;">sum</span>(value))</span>
<span id="cb33-6"></span>
<span id="cb33-7">word_scores_by_month <span class="sc" style="color: #5E5E5E;">%&gt;%</span>  </span>
<span id="cb33-8">  <span class="fu" style="color: #4758AB;">ggplot</span>(<span class="fu" style="color: #4758AB;">aes</span>(<span class="at" style="color: #657422;">x=</span><span class="fu" style="color: #4758AB;">as.Date</span>(yearmonth),<span class="at" style="color: #657422;">y=</span>mood))<span class="sc" style="color: #5E5E5E;">+</span><span class="fu" style="color: #4758AB;">geom_col</span>()<span class="sc" style="color: #5E5E5E;">+</span><span class="fu" style="color: #4758AB;">geom_smooth</span>()<span class="sc" style="color: #5E5E5E;">+</span><span class="fu" style="color: #4758AB;">labs</span>(<span class="at" style="color: #657422;">x=</span><span class="st" style="color: #20794D;">"Month"</span>,<span class="at" style="color: #657422;">y=</span><span class="st" style="color: #20794D;">"Mood Score"</span>)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>`geom_smooth()` using method = 'loess' and formula = 'y ~ x'</code></pre>
</div>
<div class="cell-output-display">
<p><img src="outsiderdata.netlify.app/posts/2017-11-25-summary-analysis-of-my-facebook-existence/2017-11-25-summary-analysis-of-my-facebook-existence_files/figure-html/unnamed-chunk-24-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>A trend is not very evident. Month-to-month variation is very high. Is it that we don’t have a good sample size or do my moods swing wildly? The most extreme gyration is around the 2016 presidential election. Optimism followed by despair? Perhaps.</p>
</section>
<section id="politics-rears-its-ugly-head" class="level1">
<h1>Politics Rears Its Ugly Head</h1>
<p>I try to avoid too much talk about politics on Facebook but, like most of us, it was tough in an election year. This gives us an opportunity to dig into a specific topic within the posting corpus.</p>
<p>Let’s start by seeing how often I mentioned politicians names.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb35" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb35-1">politicos<span class="ot" style="color: #003B4F;">=</span><span class="fu" style="color: #4758AB;">c</span>(<span class="st" style="color: #20794D;">"obama"</span>,<span class="st" style="color: #20794D;">"trump"</span>,<span class="st" style="color: #20794D;">"hillary"</span>,<span class="st" style="color: #20794D;">"clinton"</span>,<span class="st" style="color: #20794D;">"johnson"</span>,<span class="st" style="color: #20794D;">"kasich"</span>,<span class="st" style="color: #20794D;">"bush"</span>,<span class="st" style="color: #20794D;">"sanders"</span>,<span class="st" style="color: #20794D;">"romney"</span>,<span class="st" style="color: #20794D;">"mccain"</span>,<span class="st" style="color: #20794D;">"palin"</span>)</span>
<span id="cb35-2"></span>
<span id="cb35-3">gg<span class="ot" style="color: #003B4F;">&lt;-</span>cleaned_post_words <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb35-4">  <span class="fu" style="color: #4758AB;">filter</span>(word <span class="sc" style="color: #5E5E5E;">%in%</span> politicos) <span class="sc" style="color: #5E5E5E;">%&gt;%</span></span>
<span id="cb35-5">  <span class="fu" style="color: #4758AB;">mutate</span>(<span class="at" style="color: #657422;">word=</span><span class="fu" style="color: #4758AB;">str_to_title</span>(word)) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb35-6">  <span class="fu" style="color: #4758AB;">count</span>(word, <span class="at" style="color: #657422;">sort =</span> <span class="cn" style="color: #8f5902;">TRUE</span>) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb35-7">  </span>
<span id="cb35-8">  <span class="fu" style="color: #4758AB;">ggplot</span>(<span class="fu" style="color: #4758AB;">aes</span>(<span class="at" style="color: #657422;">x=</span><span class="fu" style="color: #4758AB;">reorder</span>(word,n),<span class="at" style="color: #657422;">y=</span>n))<span class="sc" style="color: #5E5E5E;">+</span><span class="fu" style="color: #4758AB;">geom_col</span>()<span class="sc" style="color: #5E5E5E;">+</span><span class="fu" style="color: #4758AB;">coord_flip</span>()<span class="sc" style="color: #5E5E5E;">+</span><span class="fu" style="color: #4758AB;">labs</span>(<span class="at" style="color: #657422;">y=</span><span class="st" style="color: #20794D;">"Mentions"</span>,<span class="at" style="color: #657422;">x=</span><span class="st" style="color: #20794D;">""</span>)</span>
<span id="cb35-9"></span>
<span id="cb35-10">gg</span></code></pre></div>
<div class="cell-output-display">
<p><img src="outsiderdata.netlify.app/posts/2017-11-25-summary-analysis-of-my-facebook-existence/2017-11-25-summary-analysis-of-my-facebook-existence_files/figure-html/unnamed-chunk-25-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>“Johnson”” is Gary Johnson, in case you forgot. Unlike the the “lamestream media,” I gave much more attention to the Libertarian candidate in my posts. Oddly, that didn’t help his success in the election.</p>
<p>“Hillary”” is the only first name in the list. Using a woman’s first name when equally familiar men are referred to with their last name is often sexist. In my defense, during the election it was important to distinguish between her and husband Bill so that’s why I used “Hillary.” We are not on a first name basis. On the other hand, “Bill” is a common enough name (and noun) so it’s likely that many posts using it don’t refer to the politician. Just to get an idea let’s look at the words bracketing “bill”:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb36" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb36-1">cleaned_post_words <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb36-2">  <span class="fu" style="color: #4758AB;">mutate</span>(<span class="at" style="color: #657422;">phrase=</span><span class="fu" style="color: #4758AB;">paste</span>(<span class="fu" style="color: #4758AB;">lag</span>(word),word,<span class="fu" style="color: #4758AB;">lead</span>(word))) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb36-3">  <span class="fu" style="color: #4758AB;">filter</span>(word<span class="sc" style="color: #5E5E5E;">==</span><span class="st" style="color: #20794D;">'bill'</span>) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb36-4">  <span class="fu" style="color: #4758AB;">select</span>(date,phrase) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb36-5">  <span class="fu" style="color: #4758AB;">kable</span>() <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb36-6">  <span class="fu" style="color: #4758AB;">kable_styling</span>(<span class="at" style="color: #657422;">bootstrap_options =</span> <span class="st" style="color: #20794D;">"striped"</span>, <span class="at" style="color: #657422;">full_width =</span> <span class="cn" style="color: #8f5902;">FALSE</span>, <span class="at" style="color: #657422;">position=</span><span class="st" style="color: #20794D;">'left'</span>)</span></code></pre></div>
<div class="cell-output-display">

<table class="table table-striped" style="width: auto !important; ">
 <thead>
  <tr>
   <th style="text-align:left;"> date </th>
   <th style="text-align:left;"> phrase </th>
  </tr>
 </thead>
<tbody>
  <tr>
   <td style="text-align:left;"> 2017-09-20 08:53:00 </td>
   <td style="text-align:left;"> morning bill protection </td>
  </tr>
  <tr>
   <td style="text-align:left;"> 2017-07-23 12:27:00 </td>
   <td style="text-align:left;"> water bill extra </td>
  </tr>
  <tr>
   <td style="text-align:left;"> 2017-03-11 14:15:00 </td>
   <td style="text-align:left;"> stitch bill doctor </td>
  </tr>
  <tr>
   <td style="text-align:left;"> 2017-03-11 13:51:00 </td>
   <td style="text-align:left;"> pass bill nancy </td>
  </tr>
  <tr>
   <td style="text-align:left;"> 2017-03-05 10:45:00 </td>
   <td style="text-align:left;"> front bill maudlin </td>
  </tr>
  <tr>
   <td style="text-align:left;"> 2016-10-18 13:13:00 </td>
   <td style="text-align:left;"> friends bill fast </td>
  </tr>
  <tr>
   <td style="text-align:left;"> 2016-09-28 21:46:00 </td>
   <td style="text-align:left;"> johnson bill weld </td>
  </tr>
  <tr>
   <td style="text-align:left;"> 2016-09-28 21:46:00 </td>
   <td style="text-align:left;"> idealist bill policy </td>
  </tr>
  <tr>
   <td style="text-align:left;"> 2016-07-28 14:28:00 </td>
   <td style="text-align:left;"> delegates bill clinton </td>
  </tr>
  <tr>
   <td style="text-align:left;"> 2012-08-02 12:59:00 </td>
   <td style="text-align:left;"> governor bill haslam </td>
  </tr>
  <tr>
   <td style="text-align:left;"> 2012-05-03 18:43:00 </td>
   <td style="text-align:left;"> jobs bill money </td>
  </tr>
  <tr>
   <td style="text-align:left;"> 2012-05-03 18:43:00 </td>
   <td style="text-align:left;"> talking bill drummond </td>
  </tr>
  <tr>
   <td style="text-align:left;"> 2012-04-21 17:45:00 </td>
   <td style="text-align:left;"> xtc bill lot </td>
  </tr>
  <tr>
   <td style="text-align:left;"> 2010-10-20 21:12:00 </td>
   <td style="text-align:left;"> betty bill bartlett </td>
  </tr>
  <tr>
   <td style="text-align:left;"> 2009-07-02 16:57:00 </td>
   <td style="text-align:left;"> disco bill nelson </td>
  </tr>
  <tr>
   <td style="text-align:left;"> 2009-03-09 08:36:00 </td>
   <td style="text-align:left;"> pay bill 20 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> 2009-02-27 18:17:00 </td>
   <td style="text-align:left;"> kill bill banks </td>
  </tr>
</tbody>
</table>

</div>
</div>
<p>It looks like just one of the mentions of “Bill” is Bill Clinton, so we can ignore him (for this project, anyway).</p>
<p>Was the uptick we saw above in posting activity around the election due to political activty? We asssume, but let’s look just the posts containing the names of the politcians I identified earlier. This does not include links I shared containing names but did not comment upon, as Facebook won’t tell me what the link was.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb37" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb37-1">cleaned_post_words <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb37-2">  <span class="fu" style="color: #4758AB;">mutate</span>(<span class="at" style="color: #657422;">month=</span><span class="fu" style="color: #4758AB;">as.yearmon</span>(date)) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb37-3">  <span class="fu" style="color: #4758AB;">filter</span>(word <span class="sc" style="color: #5E5E5E;">%in%</span> politicos) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb37-4">  <span class="fu" style="color: #4758AB;">mutate</span>(<span class="at" style="color: #657422;">word=</span><span class="fu" style="color: #4758AB;">str_to_title</span>(word)) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb37-5">  <span class="fu" style="color: #4758AB;">group_by</span>(month,word) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb37-6">  <span class="fu" style="color: #4758AB;">summarize</span>(<span class="at" style="color: #657422;">n=</span><span class="fu" style="color: #4758AB;">n</span>()) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb37-7">  <span class="fu" style="color: #4758AB;">ggplot</span>(<span class="fu" style="color: #4758AB;">aes</span>(<span class="fu" style="color: #4758AB;">as.Date</span>(<span class="at" style="color: #657422;">x=</span>month),<span class="at" style="color: #657422;">y=</span>n,<span class="at" style="color: #657422;">fill=</span>word)) <span class="sc" style="color: #5E5E5E;">+</span> <span class="fu" style="color: #4758AB;">geom_col</span>() <span class="sc" style="color: #5E5E5E;">+</span><span class="fu" style="color: #4758AB;">labs</span>(<span class="at" style="color: #657422;">x=</span><span class="st" style="color: #20794D;">"Month"</span>,<span class="at" style="color: #657422;">y=</span><span class="st" style="color: #20794D;">"Mentions"</span>)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>`summarise()` has grouped output by 'month'. You can override using the
`.groups` argument.</code></pre>
</div>
<div class="cell-output-display">
<p><img src="outsiderdata.netlify.app/posts/2017-11-25-summary-analysis-of-my-facebook-existence/2017-11-25-summary-analysis-of-my-facebook-existence_files/figure-html/unnamed-chunk-27-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Yup, I guess so. Unlike some of my wonderful Facebook friends I have been able to let go of the emotions around the election, as my posts naming politicians have dropped back to their baseline levels. Naturally, the names have changed!</p>
<p>Can you tell my political leanings from this data? Well, duh! You could just read the posts but where’s the fun in that? More practically, if we are analyzing big data sets it would be tough to glean the overall sentiment of many people from reading a feasible sample of posts. Let’s try running the sentiment analysis on all of the posts containing the name of a politician to see of anything emerges.</p>
<p>Note, I often mention more than one name in a single post and this method won’t distinguish which words apply to which name.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb39" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb39-1">pol_sentiment<span class="ot" style="color: #003B4F;">&lt;-</span> <span class="cn" style="color: #8f5902;">NULL</span></span>
<span id="cb39-2">pol_sent<span class="ot" style="color: #003B4F;">&lt;-</span> <span class="cn" style="color: #8f5902;">NULL</span></span>
<span id="cb39-3"><span class="cf" style="color: #003B4F;">for</span> (pol <span class="cf" style="color: #003B4F;">in</span> politicos) {</span>
<span id="cb39-4">  sent_words<span class="ot" style="color: #003B4F;">&lt;-</span>cleaned_post_words <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb39-5">    <span class="fu" style="color: #4758AB;">filter</span>(word <span class="sc" style="color: #5E5E5E;">==</span> pol) <span class="sc" style="color: #5E5E5E;">%&gt;%</span></span>
<span id="cb39-6">    <span class="fu" style="color: #4758AB;">select</span>(date) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb39-7">    <span class="fu" style="color: #4758AB;">unique</span>() <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb39-8">    <span class="fu" style="color: #4758AB;">inner_join</span>(cleaned_post_words,<span class="at" style="color: #657422;">by=</span><span class="st" style="color: #20794D;">'date'</span>) <span class="sc" style="color: #5E5E5E;">%&gt;%</span></span>
<span id="cb39-9">    <span class="fu" style="color: #4758AB;">select</span>(word) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb39-10">    <span class="fu" style="color: #4758AB;">inner_join</span>(<span class="fu" style="color: #4758AB;">get_sentiments</span>(<span class="st" style="color: #20794D;">'afinn'</span>),<span class="at" style="color: #657422;">by=</span><span class="st" style="color: #20794D;">"word"</span>)</span>
<span id="cb39-11">  <span class="co" style="color: #5E5E5E;">#did we get anything?</span></span>
<span id="cb39-12">  <span class="cf" style="color: #003B4F;">if</span>(<span class="fu" style="color: #4758AB;">nrow</span>(sent_words)<span class="sc" style="color: #5E5E5E;">&gt;</span><span class="dv" style="color: #AD0000;">0</span>){</span>
<span id="cb39-13">    avg_score<span class="ot" style="color: #003B4F;">&lt;-</span><span class="fu" style="color: #4758AB;">summarise</span>(sent_words,<span class="at" style="color: #657422;">opinion=</span><span class="fu" style="color: #4758AB;">mean</span>(value),<span class="at" style="color: #657422;">post_count=</span><span class="fu" style="color: #4758AB;">n</span>())</span>
<span id="cb39-14">    pol_sentiment<span class="ot" style="color: #003B4F;">&lt;-</span><span class="fu" style="color: #4758AB;">bind_rows</span>(<span class="fu" style="color: #4758AB;">bind_cols</span>(<span class="at" style="color: #657422;">Politician=</span><span class="fu" style="color: #4758AB;">str_to_title</span>(pol),avg_score),pol_sentiment)</span>
<span id="cb39-15">    pol_sent<span class="ot" style="color: #003B4F;">&lt;-</span><span class="fu" style="color: #4758AB;">bind_rows</span>(<span class="fu" style="color: #4758AB;">tibble</span>(<span class="at" style="color: #657422;">Politician=</span><span class="fu" style="color: #4758AB;">str_to_title</span>(pol),</span>
<span id="cb39-16">                                   <span class="at" style="color: #657422;">opinion=</span>avg_score<span class="sc" style="color: #5E5E5E;">$</span>opinion,</span>
<span id="cb39-17">                                   <span class="at" style="color: #657422;">post_count=</span>avg_score<span class="sc" style="color: #5E5E5E;">$</span>post_count,</span>
<span id="cb39-18">                                   <span class="at" style="color: #657422;">sent_words=</span><span class="fu" style="color: #4758AB;">list</span>(sent_words<span class="sc" style="color: #5E5E5E;">$</span>value)</span>
<span id="cb39-19">                                   ),</span>
<span id="cb39-20">    pol_sent)</span>
<span id="cb39-21">    </span>
<span id="cb39-22">  }</span>
<span id="cb39-23"></span>
<span id="cb39-24">}</span>
<span id="cb39-25">pol_sent[,<span class="dv" style="color: #AD0000;">1</span><span class="sc" style="color: #5E5E5E;">:</span><span class="dv" style="color: #AD0000;">3</span>]</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 8 × 3
  Politician opinion post_count
  &lt;chr&gt;        &lt;dbl&gt;      &lt;int&gt;
1 Romney     -1.75            4
2 Sanders     0.0333         30
3 Bush       -0.410          39
4 Johnson     0.226          84
5 Clinton     0.0345         29
6 Hillary     0.0108         93
7 Trump       0.171          82
8 Obama      -0.112          98</code></pre>
</div>
</div>
<p>First off, Palin and McCain don’t appear in this list beacuse I apparently didn’t use any words from the “afinn” lexicon in my posts mentioning them. Second, Romney is only mentioned in four posts so I don’t think we have a valid sample size. Notice that we store the list of sentiment scores for each politician in <code>sent_words</code>. We’ll use that in a minute.</p>
<p>Taking what’s left, let’s view a chart.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb41" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb41-1">pol_sent <span class="sc" style="color: #5E5E5E;">%&gt;%</span> <span class="fu" style="color: #4758AB;">filter</span>(Politician <span class="sc" style="color: #5E5E5E;">!=</span> <span class="st" style="color: #20794D;">"Romney"</span>) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb41-2">  <span class="fu" style="color: #4758AB;">ggplot</span>(<span class="fu" style="color: #4758AB;">aes</span>(<span class="at" style="color: #657422;">x=</span><span class="fu" style="color: #4758AB;">reorder</span>(Politician,opinion),<span class="at" style="color: #657422;">y=</span>opinion))<span class="sc" style="color: #5E5E5E;">+</span><span class="fu" style="color: #4758AB;">geom_col</span>()<span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb41-3">  <span class="fu" style="color: #4758AB;">coord_flip</span>()<span class="sc" style="color: #5E5E5E;">+</span><span class="fu" style="color: #4758AB;">labs</span>(<span class="at" style="color: #657422;">y=</span><span class="st" style="color: #20794D;">"Sentiment"</span>,<span class="at" style="color: #657422;">x=</span><span class="st" style="color: #20794D;">""</span>)</span></code></pre></div>
<div class="cell-output-display">
<p><img src="outsiderdata.netlify.app/posts/2017-11-25-summary-analysis-of-my-facebook-existence/2017-11-25-summary-analysis-of-my-facebook-existence_files/figure-html/unnamed-chunk-29-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>I’m sad to say this is far from how I would rank order my feelings about each of these names. I was a Gary Johnson supporter but, beyond that, this list might as well be random. Sample size is an issue. I am not that prolific a poster and the words I have in common with the sentiment lexicon is fewer still. Also, remember the sentiment ranges run from -5 to +5. We are talking small magnitudes here. Here is the same chart with the maximum allowable range shown.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb42" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb42-1">pol_sent <span class="sc" style="color: #5E5E5E;">%&gt;%</span> <span class="fu" style="color: #4758AB;">filter</span>(Politician <span class="sc" style="color: #5E5E5E;">!=</span> <span class="st" style="color: #20794D;">"Romney"</span>) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb42-2">  <span class="fu" style="color: #4758AB;">ggplot</span>(<span class="fu" style="color: #4758AB;">aes</span>(<span class="at" style="color: #657422;">x=</span><span class="fu" style="color: #4758AB;">reorder</span>(Politician,opinion),<span class="at" style="color: #657422;">y=</span>opinion))<span class="sc" style="color: #5E5E5E;">+</span><span class="fu" style="color: #4758AB;">geom_col</span>()<span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb42-3">  <span class="fu" style="color: #4758AB;">coord_flip</span>()<span class="sc" style="color: #5E5E5E;">+</span><span class="fu" style="color: #4758AB;">labs</span>(<span class="at" style="color: #657422;">y=</span><span class="st" style="color: #20794D;">"Sentiment"</span>,<span class="at" style="color: #657422;">x=</span><span class="st" style="color: #20794D;">""</span>)<span class="sc" style="color: #5E5E5E;">+</span><span class="fu" style="color: #4758AB;">ylim</span>(<span class="fu" style="color: #4758AB;">c</span>(<span class="sc" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">5</span>,<span class="dv" style="color: #AD0000;">5</span>))</span></code></pre></div>
<div class="cell-output-display">
<p><img src="outsiderdata.netlify.app/posts/2017-11-25-summary-analysis-of-my-facebook-existence/2017-11-25-summary-analysis-of-my-facebook-existence_files/figure-html/unnamed-chunk-30-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Let’s subject this to a test of significance. The “null hypothesis” we want to test is that the mean sentiment score expressed for “Hillary” is not statistically different from the “Trump” score.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb43" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb43-1">trump_opinion<span class="ot" style="color: #003B4F;">&lt;-</span>pol_sent <span class="sc" style="color: #5E5E5E;">%&gt;%</span> <span class="fu" style="color: #4758AB;">filter</span>(Politician<span class="sc" style="color: #5E5E5E;">==</span><span class="st" style="color: #20794D;">"Trump"</span>) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> <span class="fu" style="color: #4758AB;">pull</span>(sent_words) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> <span class="fu" style="color: #4758AB;">unlist</span>()</span>
<span id="cb43-2">hillary_opinion<span class="ot" style="color: #003B4F;">&lt;-</span>pol_sent <span class="sc" style="color: #5E5E5E;">%&gt;%</span> <span class="fu" style="color: #4758AB;">filter</span>(Politician<span class="sc" style="color: #5E5E5E;">==</span><span class="st" style="color: #20794D;">"Hillary"</span>) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> <span class="fu" style="color: #4758AB;">pull</span>(sent_words) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> <span class="fu" style="color: #4758AB;">unlist</span>()</span>
<span id="cb43-3"><span class="fu" style="color: #4758AB;">t.test</span>(trump_opinion,hillary_opinion)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
    Welch Two Sample t-test

data:  trump_opinion and hillary_opinion
t = 0.46581, df = 170.76, p-value = 0.6419
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -0.5179631  0.8379211
sample estimates:
 mean of x  mean of y 
0.17073171 0.01075269 </code></pre>
</div>
</div>
<p>Like before, we can’t reject our hypothesis, I liked Donald Trump as much as I liked Hillary Clinton or, rather, you can’t prove a thing!</p>
<p>This analysis shows I didn’t really express strong opinions. It was deliberate. During the election things got pretty heated, as you may recall. I have Facebook friends on all sides of the the political divides. Out of respect, I tried very hard not to trash anybody’s candidate and instead tried to accentuate the positive.</p>
<p>#Bonus Friend Analysis!</p>
<p>One more thing. We’ve only looked at the ‘timeline.htm’ file but Facebook’s data dump includes a lot of other stuff including a list of your friends and the date they were added. We can look at a timeline and summary statistics for for this too.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb45" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb45-1">raw_friends<span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">read_html</span>(<span class="fu" style="color: #4758AB;">paste0</span>(path,<span class="st" style="color: #20794D;">"friends.htm"</span>),<span class="at" style="color: #657422;">encoding=</span><span class="st" style="color: #20794D;">"UTC-8"</span>)</span></code></pre></div>
</div>
<p>Here the <code>&lt;h2&gt;</code> tag separates the labels for the different friend interactions. What are the categories of interactions Facebook gives us?</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb46" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb46-1">friend_nodes <span class="ot" style="color: #003B4F;">&lt;-</span> raw_friends <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb46-2">  <span class="fu" style="color: #4758AB;">html_nodes</span>(<span class="st" style="color: #20794D;">"h2"</span>) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> <span class="fu" style="color: #4758AB;">html_text</span>()</span>
<span id="cb46-3"></span>
<span id="cb46-4">friend_nodes</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Friends"                  "Sent Friend Requests"    
[3] "Received Friend Requests" "Deleted Friend Requests" 
[5] "Removed Friends"          "Friend Peer Group"       </code></pre>
</div>
</div>
<p>The actual items are in lists separated by the <code>&lt;ul&gt;</code> tag. Let’s traverse the list, extracting the category, name and date for each. The first and the last lists do not contain relevant info so we’ll take just the middle five. We’ll also rename the “Friends” category to the more descriptive “Friends Added.”</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb48" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb48-1">friend_nodes[<span class="dv" style="color: #AD0000;">1</span>]<span class="ot" style="color: #003B4F;">&lt;-</span><span class="st" style="color: #20794D;">"Friends Added"</span></span>
<span id="cb48-2">list_nodes <span class="ot" style="color: #003B4F;">&lt;-</span> raw_friends <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb48-3">  <span class="fu" style="color: #4758AB;">html_nodes</span>(<span class="st" style="color: #20794D;">"ul"</span>) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb48-4">  .[<span class="dv" style="color: #AD0000;">2</span><span class="sc" style="color: #5E5E5E;">:</span><span class="dv" style="color: #AD0000;">6</span>]</span>
<span id="cb48-5"></span>
<span id="cb48-6">friend_table<span class="ot" style="color: #003B4F;">&lt;-</span> <span class="cn" style="color: #8f5902;">NULL</span></span>
<span id="cb48-7"><span class="cf" style="color: #003B4F;">for</span> (node <span class="cf" style="color: #003B4F;">in</span> <span class="dv" style="color: #AD0000;">1</span><span class="sc" style="color: #5E5E5E;">:</span><span class="fu" style="color: #4758AB;">length</span>(list_nodes)){</span>
<span id="cb48-8">  category<span class="ot" style="color: #003B4F;">&lt;-</span>friend_nodes[node]</span>
<span id="cb48-9">  items<span class="ot" style="color: #003B4F;">&lt;-</span> list_nodes[node] <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb48-10">    <span class="fu" style="color: #4758AB;">html_nodes</span>(<span class="st" style="color: #20794D;">"li"</span>) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb48-11">    <span class="fu" style="color: #4758AB;">html_text</span>() <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb48-12">    <span class="fu" style="color: #4758AB;">tibble</span>(<span class="at" style="color: #657422;">item=</span>.) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb48-13">    <span class="fu" style="color: #4758AB;">separate</span>(item,<span class="fu" style="color: #4758AB;">c</span>(<span class="st" style="color: #20794D;">"Name"</span>,<span class="st" style="color: #20794D;">"Date"</span>),<span class="at" style="color: #657422;">sep=</span><span class="st" style="color: #20794D;">"</span><span class="sc" style="color: #5E5E5E;">\\</span><span class="st" style="color: #20794D;">("</span>,<span class="at" style="color: #657422;">remove=</span><span class="cn" style="color: #8f5902;">TRUE</span>) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb48-14">    <span class="fu" style="color: #4758AB;">mutate</span>(<span class="at" style="color: #657422;">Date=</span><span class="fu" style="color: #4758AB;">str_replace</span>(Date,<span class="st" style="color: #20794D;">"</span><span class="sc" style="color: #5E5E5E;">\\</span><span class="st" style="color: #20794D;">)"</span>,<span class="st" style="color: #20794D;">""</span>))</span>
<span id="cb48-15">  </span>
<span id="cb48-16">    friend_table<span class="ot" style="color: #003B4F;">&lt;-</span><span class="fu" style="color: #4758AB;">cbind</span>(<span class="at" style="color: #657422;">Category=</span>category,items,<span class="at" style="color: #657422;">stringsAsFactors=</span><span class="cn" style="color: #8f5902;">FALSE</span>) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb48-17">      <span class="fu" style="color: #4758AB;">bind_rows</span>(friend_table) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb48-18">      <span class="fu" style="color: #4758AB;">as_tibble</span>()</span>
<span id="cb48-19">}</span></code></pre></div>
</div>
<p>How many Facebook friends do I have? Not many, by Facebook standards. This is, of course, the question that measures our entire self-worth. I’m very discriminating about who I friend <harumph>. Well, that plus I’m not a very likeable person. Only four people have become so annoying that I unfriended them (both political extremes). There are more that I unfollowed (too many cat videos) but Facebook doesn’t include them in the log for some reason. Facebook also won’t tell me who unfriended me. But I’m sure that no one would do THAT.</harumph></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb49" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb49-1">friend_table <span class="sc" style="color: #5E5E5E;">%&gt;%</span> <span class="fu" style="color: #4758AB;">group_by</span>(Category) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> <span class="fu" style="color: #4758AB;">summarize</span>(<span class="at" style="color: #657422;">Number=</span><span class="fu" style="color: #4758AB;">n</span>())</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 5 × 2
  Category                 Number
  &lt;chr&gt;                     &lt;int&gt;
1 Deleted Friend Requests      67
2 Friends Added               167
3 Received Friend Requests      7
4 Removed Friends               4
5 Sent Friend Requests          3</code></pre>
</div>
</div>
<p>Once again we have a character string for the date which we need to turn into a proper date. The wrinkle here is dates in the current year don’t specify the year in the log. We have to manually add it. The data is at a daily resolution, which is too granular for a clear picture at my level of activity. Let’s make it quarterly.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb51" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb51-1">friend_table2 <span class="ot" style="color: #003B4F;">&lt;-</span> friend_table <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb51-2">  <span class="fu" style="color: #4758AB;">mutate</span>(<span class="at" style="color: #657422;">Date=</span><span class="fu" style="color: #4758AB;">if_else</span>(<span class="fu" style="color: #4758AB;">str_length</span>(Date)<span class="sc" style="color: #5E5E5E;">&lt;</span><span class="dv" style="color: #AD0000;">7</span>,<span class="fu" style="color: #4758AB;">paste0</span>(Date,<span class="st" style="color: #20794D;">", "</span>,<span class="fu" style="color: #4758AB;">year</span>(<span class="fu" style="color: #4758AB;">Sys.Date</span>())),Date)) <span class="sc" style="color: #5E5E5E;">%&gt;%</span></span>
<span id="cb51-3">  <span class="fu" style="color: #4758AB;">mutate</span>(<span class="at" style="color: #657422;">Date=</span><span class="fu" style="color: #4758AB;">parse_date</span>(Date,<span class="at" style="color: #657422;">format=</span><span class="st" style="color: #20794D;">"%b %d, %Y"</span>)) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb51-4">  <span class="fu" style="color: #4758AB;">select</span>(Category,Date) <span class="sc" style="color: #5E5E5E;">%&gt;%</span></span>
<span id="cb51-5">  <span class="fu" style="color: #4758AB;">mutate</span>(<span class="at" style="color: #657422;">yearquarter=</span><span class="fu" style="color: #4758AB;">as.yearqtr</span>(Date)) <span class="sc" style="color: #5E5E5E;">%&gt;%</span></span>
<span id="cb51-6">  <span class="fu" style="color: #4758AB;">group_by</span>(yearquarter)</span>
<span id="cb51-7">gg<span class="ot" style="color: #003B4F;">&lt;-</span>friend_table2 <span class="sc" style="color: #5E5E5E;">%&gt;%</span> <span class="fu" style="color: #4758AB;">ggplot</span>(<span class="fu" style="color: #4758AB;">aes</span>(<span class="at" style="color: #657422;">x=</span><span class="fu" style="color: #4758AB;">as.Date</span>(yearquarter),<span class="at" style="color: #657422;">fill=</span>Category))<span class="sc" style="color: #5E5E5E;">+</span> <span class="fu" style="color: #4758AB;">geom_bar</span>(<span class="at" style="color: #657422;">width=</span><span class="dv" style="color: #AD0000;">70</span>)</span>
<span id="cb51-8">gg<span class="ot" style="color: #003B4F;">&lt;-</span>gg <span class="sc" style="color: #5E5E5E;">+</span><span class="fu" style="color: #4758AB;">labs</span>(<span class="at" style="color: #657422;">title=</span><span class="st" style="color: #20794D;">"Facebook Friending Activity"</span>,<span class="at" style="color: #657422;">y=</span><span class="st" style="color: #20794D;">"Count"</span>,<span class="at" style="color: #657422;">x=</span><span class="st" style="color: #20794D;">"Quarterly"</span>)</span>
<span id="cb51-9">gg</span></code></pre></div>
<div class="cell-output-display">
<p><img src="outsiderdata.netlify.app/posts/2017-11-25-summary-analysis-of-my-facebook-existence/2017-11-25-summary-analysis-of-my-facebook-existence_files/figure-html/unnamed-chunk-36-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Not too surprising. There was a lot of friending happening when I first joined Facebook. Perhaps a bit more curious is the recent renewed uptick in friending. There has been an influx of renewed high school aquaintances.</p>
<p>Sometimes it is useful too look at the balance of opposites. For example, we can see the balance of the number of friendings vs.&nbsp;the number of delete friend requests by assigning a negative number to deletions. There is no simple way to do this with native dplyr functions, though there should be. Base R is actually better at transforming just certain elements in a column based on some condition. Fortunately, I found a super-useful bit of code on Stack Overflow, <code>mutate_cond()</code>, that does exactly what we need.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb52" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb52-1">mutate_cond <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="cf" style="color: #003B4F;">function</span>(.data, condition, ..., <span class="at" style="color: #657422;">envir =</span> <span class="fu" style="color: #4758AB;">parent.frame</span>()) {</span>
<span id="cb52-2">  <span class="co" style="color: #5E5E5E;">#change elements of a column based on a condition</span></span>
<span id="cb52-3">  <span class="co" style="color: #5E5E5E;">#https://stackoverflow.com/questions/34096162/dplyr-mutate-replace-on-a-subset-of-rows/34096575#34096575</span></span>
<span id="cb52-4">  condition <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">eval</span>(<span class="fu" style="color: #4758AB;">substitute</span>(condition), .data, envir)</span>
<span id="cb52-5">    condition[<span class="fu" style="color: #4758AB;">is.na</span>(condition)] <span class="ot" style="color: #003B4F;">=</span> <span class="cn" style="color: #8f5902;">FALSE</span></span>
<span id="cb52-6">    .data[condition, ] <span class="ot" style="color: #003B4F;">&lt;-</span> .data[condition, ] <span class="sc" style="color: #5E5E5E;">%&gt;%</span> <span class="fu" style="color: #4758AB;">mutate</span>(...)</span>
<span id="cb52-7">    .data</span>
<span id="cb52-8">}</span>
<span id="cb52-9"></span>
<span id="cb52-10"><span class="co" style="color: #5E5E5E;"># tabulate sums of categories by quarter</span></span>
<span id="cb52-11">friend_table3 <span class="ot" style="color: #003B4F;">&lt;-</span> friend_table <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb52-12">  <span class="fu" style="color: #4758AB;">mutate</span>(<span class="at" style="color: #657422;">Date=</span><span class="fu" style="color: #4758AB;">if_else</span>(<span class="fu" style="color: #4758AB;">str_length</span>(Date)<span class="sc" style="color: #5E5E5E;">&lt;</span><span class="dv" style="color: #AD0000;">7</span>,<span class="fu" style="color: #4758AB;">paste0</span>(Date,<span class="st" style="color: #20794D;">", "</span>,<span class="fu" style="color: #4758AB;">year</span>(<span class="fu" style="color: #4758AB;">Sys.Date</span>())),Date)) <span class="sc" style="color: #5E5E5E;">%&gt;%</span></span>
<span id="cb52-13">  <span class="fu" style="color: #4758AB;">mutate</span>(<span class="at" style="color: #657422;">Date=</span><span class="fu" style="color: #4758AB;">parse_date</span>(Date,<span class="at" style="color: #657422;">format=</span><span class="st" style="color: #20794D;">"%b %d, %Y"</span>)) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb52-14">  <span class="fu" style="color: #4758AB;">mutate</span>(<span class="at" style="color: #657422;">yearquarter=</span><span class="fu" style="color: #4758AB;">as.yearqtr</span>(Date)) <span class="sc" style="color: #5E5E5E;">%&gt;%</span></span>
<span id="cb52-15">  <span class="fu" style="color: #4758AB;">select</span>(Category,yearquarter) <span class="sc" style="color: #5E5E5E;">%&gt;%</span></span>
<span id="cb52-16">  <span class="fu" style="color: #4758AB;">group_by</span>(Category,yearquarter) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb52-17">  <span class="fu" style="color: #4758AB;">summarise</span>(<span class="at" style="color: #657422;">count=</span><span class="fu" style="color: #4758AB;">n</span>())</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>`summarise()` has grouped output by 'Category'. You can override using the
`.groups` argument.</code></pre>
</div>
<div class="sourceCode cell-code" id="cb54" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb54-1"><span class="co" style="color: #5E5E5E;">#make deleted requests negative</span></span>
<span id="cb54-2">gg<span class="ot" style="color: #003B4F;">&lt;-</span>friend_table3 <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb54-3">  <span class="fu" style="color: #4758AB;">mutate_cond</span>(Category<span class="sc" style="color: #5E5E5E;">==</span><span class="st" style="color: #20794D;">"Deleted Friend Requests"</span>,<span class="at" style="color: #657422;">count=</span><span class="sc" style="color: #5E5E5E;">-</span>count) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb54-4">  <span class="fu" style="color: #4758AB;">filter</span>(Category<span class="sc" style="color: #5E5E5E;">==</span><span class="st" style="color: #20794D;">"Deleted Friend Requests"</span> <span class="sc" style="color: #5E5E5E;">|</span> Category<span class="sc" style="color: #5E5E5E;">==</span><span class="st" style="color: #20794D;">"Friends Added"</span>) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb54-5">  <span class="fu" style="color: #4758AB;">ggplot</span>(<span class="fu" style="color: #4758AB;">aes</span>(<span class="at" style="color: #657422;">x=</span><span class="fu" style="color: #4758AB;">as.Date</span>(yearquarter),<span class="at" style="color: #657422;">y=</span>count,<span class="at" style="color: #657422;">fill=</span>Category))<span class="sc" style="color: #5E5E5E;">+</span> <span class="fu" style="color: #4758AB;">geom_col</span>() </span>
<span id="cb54-6"></span>
<span id="cb54-7">gg<span class="ot" style="color: #003B4F;">&lt;-</span>gg <span class="sc" style="color: #5E5E5E;">+</span><span class="fu" style="color: #4758AB;">labs</span>(<span class="at" style="color: #657422;">title=</span><span class="st" style="color: #20794D;">"Facebook Friending Activity"</span>,<span class="at" style="color: #657422;">y=</span><span class="st" style="color: #20794D;">"Count"</span>,<span class="at" style="color: #657422;">x=</span><span class="st" style="color: #20794D;">"Quarterly"</span>)</span>
<span id="cb54-8">gg</span></code></pre></div>
<div class="cell-output-display">
<p><img src="outsiderdata.netlify.app/posts/2017-11-25-summary-analysis-of-my-facebook-existence/2017-11-25-summary-analysis-of-my-facebook-existence_files/figure-html/unnamed-chunk-37-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>It seems that adding friends is associated with deleted requests. I’ll surmise that when I show up in a new friend’s network that will spark some friend requests from their network. Some, maybe most, will be from people I don’t actually know and I will reject. There are spikes in rejections because I let them stack up before I notice them.</p>
</section>
<section id="wrapping-up" class="level1">
<h1>Wrapping Up</h1>
<p>Well that was cool. We got to try a lot of things. HTML parsing, date functions, sentiment analysis, text mining and lots of dplyr manipulations. Like a lot of projects, once I got going I thought of many things to try beyond the initial scope. That’s where the fun is when you’re not on deadline and deliverables. Thanks for making it all the way through. Hopefully this gives you some ideas for your own explorations. Now you try!</p>
<p>#Double Bonus! Making the cool speedometer at the top of this post</p>
<p>Because we love cool visualizations, let’s show my mood on a silly gauge. I won’t show the low-level code I used to generate it because it’s basically a copy of what you can find here: http://www.gastonsanchez.com/. Gaston is a visualization guru extrordinaire. This shows how far you can take base R graphics.</p>
<p>What do you think your gauge would look like? If it’s in the red call the suicide prevention hotline.</p>
<p>The animation is created using the <code>magick</code> package. I am thrilled that this recent release brings the image magick program inboard to R so we no longer have to run an external program to render animated files like GIFs (which I insist on pronouncing with a hard ‘G’, BTW.)</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb55" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb55-1"><span class="co" style="color: #5E5E5E;"># create animated mood gif for top of notebook.</span></span>
<span id="cb55-2"><span class="fu" style="color: #4758AB;">library</span>(magick)</span>
<span id="cb55-3">my_days_moods<span class="ot" style="color: #003B4F;">&lt;-</span>word_scores_by_weekday <span class="sc" style="color: #5E5E5E;">%&gt;%</span></span>
<span id="cb55-4">  <span class="fu" style="color: #4758AB;">summarise</span>(<span class="at" style="color: #657422;">mood=</span><span class="fu" style="color: #4758AB;">mean</span>(value))</span>
<span id="cb55-5"></span>
<span id="cb55-6"><span class="co" style="color: #5E5E5E;">#interpolate to create more points for a smooth animation.</span></span>
<span id="cb55-7"><span class="co" style="color: #5E5E5E;"># the trick is to create a series where the mood stays constant for a number of frames</span></span>
<span id="cb55-8"><span class="co" style="color: #5E5E5E;"># then transitions smoothly to the next mood value. Examine interp_moods to see how.</span></span>
<span id="cb55-9">interp_moods<span class="ot" style="color: #003B4F;">&lt;-</span><span class="fu" style="color: #4758AB;">tibble</span>(<span class="at" style="color: #657422;">doy=</span><span class="fu" style="color: #4758AB;">unlist</span>(<span class="fu" style="color: #4758AB;">lapply</span>(<span class="fu" style="color: #4758AB;">levels</span>(my_days_moods<span class="sc" style="color: #5E5E5E;">$</span>doy),rep,<span class="dv" style="color: #AD0000;">10</span>)),</span>
<span id="cb55-10">                         <span class="at" style="color: #657422;">mood_label=</span><span class="fu" style="color: #4758AB;">round</span>(<span class="fu" style="color: #4758AB;">unlist</span>(<span class="fu" style="color: #4758AB;">lapply</span>(my_days_moods<span class="sc" style="color: #5E5E5E;">$</span>mood,rep,<span class="dv" style="color: #AD0000;">10</span>)),<span class="dv" style="color: #AD0000;">2</span>),</span>
<span id="cb55-11">                         <span class="at" style="color: #657422;">mood=</span><span class="fu" style="color: #4758AB;">approx</span>(<span class="at" style="color: #657422;">x=</span><span class="dv" style="color: #AD0000;">1</span><span class="sc" style="color: #5E5E5E;">:</span><span class="dv" style="color: #AD0000;">14</span>,<span class="fu" style="color: #4758AB;">unlist</span>(<span class="fu" style="color: #4758AB;">lapply</span>(my_days_moods<span class="sc" style="color: #5E5E5E;">$</span>mood,rep,<span class="dv" style="color: #AD0000;">2</span>)),<span class="at" style="color: #657422;">n=</span><span class="dv" style="color: #AD0000;">70</span>)<span class="sc" style="color: #5E5E5E;">$</span>y)</span>
<span id="cb55-12"></span>
<span id="cb55-13"></span>
<span id="cb55-14">interp_moods<span class="sc" style="color: #5E5E5E;">$</span>mood_label<span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">paste</span>(<span class="fu" style="color: #4758AB;">ifelse</span>(interp_moods<span class="sc" style="color: #5E5E5E;">$</span>mood_label<span class="sc" style="color: #5E5E5E;">&gt;</span><span class="dv" style="color: #AD0000;">0</span>,<span class="st" style="color: #20794D;">"+"</span>,<span class="st" style="color: #20794D;">""</span>),interp_moods<span class="sc" style="color: #5E5E5E;">$</span>mood_label)</span>
<span id="cb55-15"></span>
<span id="cb55-16"><span class="co" style="color: #5E5E5E;"># I'll spare you the details of the low-level code.</span></span>
<span id="cb55-17"><span class="co" style="color: #5E5E5E;"># see it at http://www.gastonsanchez.com/.</span></span>
<span id="cb55-18"><span class="fu" style="color: #4758AB;">source</span>(<span class="st" style="color: #20794D;">"g_speedometer.r"</span>)</span>
<span id="cb55-19"></span>
<span id="cb55-20">img <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">image_graph</span>(<span class="dv" style="color: #AD0000;">600</span>, <span class="dv" style="color: #AD0000;">400</span>, <span class="at" style="color: #657422;">res =</span> <span class="dv" style="color: #AD0000;">96</span>)</span>
<span id="cb55-21"><span class="cf" style="color: #003B4F;">for</span>(n <span class="cf" style="color: #003B4F;">in</span> <span class="dv" style="color: #AD0000;">1</span><span class="sc" style="color: #5E5E5E;">:</span><span class="fu" style="color: #4758AB;">nrow</span>(interp_moods)){</span>
<span id="cb55-22">  <span class="fu" style="color: #4758AB;">plot_speedometer</span>(<span class="at" style="color: #657422;">label=</span>interp_moods<span class="sc" style="color: #5E5E5E;">$</span>doy[n],</span>
<span id="cb55-23">                   <span class="at" style="color: #657422;">value=</span><span class="fu" style="color: #4758AB;">round</span>(interp_moods<span class="sc" style="color: #5E5E5E;">$</span>mood[n],<span class="dv" style="color: #AD0000;">2</span>),</span>
<span id="cb55-24">                   <span class="at" style="color: #657422;">bottom_label=</span>interp_moods<span class="sc" style="color: #5E5E5E;">$</span>mood_label[n],</span>
<span id="cb55-25">                   <span class="at" style="color: #657422;">min=</span><span class="sc" style="color: #5E5E5E;">-</span><span class="fl" style="color: #AD0000;">0.5</span>,</span>
<span id="cb55-26">                   <span class="at" style="color: #657422;">max=</span><span class="fl" style="color: #AD0000;">0.5</span>)</span>
<span id="cb55-27">  <span class="fu" style="color: #4758AB;">text</span>(<span class="sc" style="color: #5E5E5E;">-</span><span class="fl" style="color: #AD0000;">0.1</span>,<span class="fl" style="color: #AD0000;">1.0</span>,<span class="st" style="color: #20794D;">"Faceboook Mood-o-Meter"</span>,<span class="at" style="color: #657422;">cex=</span><span class="fl" style="color: #AD0000;">1.3</span>)</span>
<span id="cb55-28">}</span>
<span id="cb55-29"><span class="fu" style="color: #4758AB;">dev.off</span>()</span>
<span id="cb55-30">img <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">image_background</span>(<span class="fu" style="color: #4758AB;">image_trim</span>(img), <span class="st" style="color: #20794D;">'white'</span>)</span>
<span id="cb55-31">animation <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">image_animate</span>(img, <span class="at" style="color: #657422;">fps =</span> <span class="dv" style="color: #AD0000;">10</span>)</span>
<span id="cb55-32"><span class="fu" style="color: #4758AB;">image_write</span>(animation, <span class="at" style="color: #657422;">path =</span> <span class="st" style="color: #20794D;">"moods.gif"</span>, <span class="at" style="color: #657422;">format =</span> <span class="st" style="color: #20794D;">"gif"</span>)</span></code></pre></div>
</div>
<p><img src="outsiderdata.netlify.app/posts/2017-11-25-summary-analysis-of-my-facebook-existence/img/moods.gif" class="img-fluid"></p>


</section>

 ]]></description>
  <category>text mining</category>
  <category>facebook</category>
  <guid>outsiderdata.netlify.app/posts/2017-11-25-summary-analysis-of-my-facebook-existence/2017-11-25-summary-analysis-of-my-facebook-existence.html</guid>
  <pubDate>Sat, 25 Nov 2017 05:00:00 GMT</pubDate>
  <media:content url="outsiderdata.netlify.app/posts/2017-11-25-summary-analysis-of-my-facebook-existence/img/moods.gif" medium="image" type="image/gif"/>
</item>
</channel>
</rss>
