[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Art Steinmetz",
    "section": "",
    "text": "July Fourth by Grandma Moses\n\n\nI am not a data scientist. Others have coined good terms like “Data Nerd” and “Citizen Data Scientist.” I’ll coin another: “Outsider Data Scientist.” I would style myself in the likeness of an “outsider” artist, Grandma Moses. She was an American artist who didn’t pick up a brush between childhood and old age, and had no formal training. The works she produced would never be mistaken for the old masters’ but they had a certain charm. Perhaps I might strive for that. I am also getting on in years.\nI play around with R for fun. I enjoy thinking up ways to present complex information in a simple, compelling way. I attended an R conference where an axiom was presented by Dave Robinson that the value of information “still” on your computer is approximately zero and the value of information out in the world is infinitely more. Even if it is small that’s infinitely more than zero, right? That emboldened me to put stuff “out there.” Perhaps someone else might find it interesting. At a minimum this blog forces more rigor in my own thinking.\n\n\n\n“Outsider” is a bit of a misnomer. I’m sure Grandma Moses saw an another painting or two in her life. Painting was pretty mature before Ms. Moses picked up her brush, but this whole data science thing has exploded in the last few years. It is very exciting to even be on the periphery of the event horizon. I am indebted to the R community for all the examples they have shared through R-Bloggers, Stack Overflow and Twitter. The tools provided by Posit (formerly RStudio) are the bomb!\nThis is a personal side project in no way associated with any organization I am affiliated with. My opinions here are mine alone and any data I present here is neither proprietary nor is it warranted to be correct or accurate. Nothing I say here should be construed as investment advice.\n\n\n\nI used to work at OppenheimerFunds Inc. before it was acquired by Invesco, first as a portfolio manager of global macro fixed income and ultimately as CEO. I am the former board chair at the National Museum of Mathematics, MoMath.org, where I rubbed shoulders (though not in creepy way) with people who are really, really smart. Visit the museum when you are in NYC. We are making math cool!\nI am also on the board of “Rock the Street, Wall Street” which brings financial literacy programs into high school classrooms to encourage girls to become interested in finance. We need more diversity in finance.\n\n\n\nI am a trustee of my alma mater, Denison University. Quite coincidentally, my passion for data science has grown along with Denison’s. The college already had strong computer science and applied mathematics programs. A few years ago they launched a Data Analytics major and it has been one of fastest growing disciplines in the school’s history. Data science skills in a liberal arts context give kids a powerful tool set that set them up well for career success. Check it out!\nFinally, I support WFMU. Freeform, listener-supported, radio. Music to code by…sometimes. Want to support the station?\n\n\n\n\n\nArt Steinmetz"
  },
  {
    "objectID": "blogroll.html",
    "href": "blogroll.html",
    "title": "Blogroll",
    "section": "",
    "text": "R Bloggers - Meta site for R blog posts\nQuarto - Publishing Tool for this Blog\nPosit - The Open Source Data Science Company"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Outsider Data Science",
    "section": "",
    "text": "Putting what’s in there, out there. With R!\n\n\n\n\n\n\nNote\n\n\n\nThis blog is for showing my experiments in R coding. If you are interested in my musings on investments, my LinkedIn posts are the place to find them. You can also visit artsteinmetz.com for more info.\n\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n\n\n\n\n  \n\n\n\n\nKakhovka Dam Disaster\n\n\n\n\n\nAnimate Flood Inundation Along the Dnipro River\n\n\n\n\n\n\nJun 9, 2023\n\n\nArt Steinmetz\n\n\n\n\n\n\n  \n\n\n\n\nAnimating a Heatmap with Rayshader\n\n\n\n\n\n\n\n\n\n\n\n\nMay 3, 2023\n\n\nArt Steinmetz\n\n\n\n\n\n\n  \n\n\n\n\nSentiment Analysis Using Google Translate (Pt. 4 - A Painful Sight)\n\n\n\n\n\n\n\ntwitter\n\n\ntidytext\n\n\ntext mining\n\n\n\n\n\n\n\n\n\n\n\nApr 30, 2023\n\n\nArt Steinmetz\n\n\n\n\n\n\n  \n\n\n\n\nSentiment Analysis Using Google Translate (Pt. 3 - Machine Learning)\n\n\n\n\n\n\n\ntidytext\n\n\ntwitter\n\n\nmachine learning\n\n\nxgboost\n\n\n\n\n\n\n\n\n\n\n\nApr 22, 2023\n\n\nArt Steinmetz\n\n\n\n\n\n\n  \n\n\n\n\nSentiment Analysis Using Google Translate (Pt. 2 - Word Valence)\n\n\n\n\n\n\n\ntidytext\n\n\ntwitter\n\n\n\n\n\n\n\n\n\n\n\nApr 16, 2023\n\n\nArt Steinmetz\n\n\n\n\n\n\n  \n\n\n\n\nSentiment Analysis Using Google Translate (Pt. 1 - Translation)\n\n\n\n\n\n\n\ntidyverse\n\n\ngoogle translate\n\n\ntwitter\n\n\n\n\n\n\n\n\n\n\n\nApr 15, 2023\n\n\nArt Steinmetz\n\n\n\n\n\n\n  \n\n\n\n\nSwitching to Quarto from Blogdown\n\n\n\n\n\n\n\nquarto\n\n\nblogging\n\n\n\n\n\n\n\n\n\n\n\nJan 24, 2022\n\n\nArt Steinmetz\n\n\n\n\n\n\n  \n\n\n\n\nCovid Cases vs. Deaths\n\n\n\n\n\n\n\nggplot2\n\n\ntidymodels\n\n\ntidyverse\n\n\nCOVID\n\n\n\n\nEstimate the average lag between a positive COVID-19 case and a death.\n\n\n\n\n\n\nDec 6, 2020\n\n\nArt Steinmetz\n\n\n\n\n\n\n  \n\n\n\n\nWhat Do The Ramones Want?\n\n\n\n\n\n\n\nggplot2\n\n\ntidytext\n\n\nmusic\n\n\n\n\n\n\n\n\n\n\n\nJan 15, 2020\n\n\nArt Steinmetz\n\n\n\n\n\n\n  \n\n\n\n\nState Taxes: It’s not just about Income\n\n\n\n\n\n\n\ntax\n\n\nplotly\n\n\n\n\n\n\n\n\n\n\n\nDec 8, 2019\n\n\nArt Steinmetz\n\n\n\n\n\n\n  \n\n\n\n\nGender Diversity in R and Python Package Contributors\n\n\n\n\n\n\n\ngithub\n\n\ngender\n\n\n\n\n\n\n\n\n\n\n\nJul 16, 2019\n\n\nArt Steinmetz\n\n\n\n\n\n\n  \n\n\n\n\nWhy I migrated from Excel to R\n\n\n\n\n\n\n\nR\n\n\nExcel\n\n\n\n\n\n\n\n\n\n\n\nJun 12, 2019\n\n\nArt Steinmetz\n\n\n\n\n\n\n  \n\n\n\n\nSolving the Letterboxed Puzzle in the New York Times\n\n\n\n\n\n\n\npuzzle\n\n\nrecursion\n\n\n\n\n\n\n\n\n\n\n\nApr 16, 2019\n\n\nArthur Steinmetz\n\n\n\n\n\n\n  \n\n\n\n\nWhere Are The Libertarians?\n\n\n\n\n\n\n\npolitics\n\n\nggplot2\n\n\n\n\n\n\n\n\n\n\n\nFeb 7, 2019\n\n\nArt Steinmetz\n\n\n\n\n\n\n  \n\n\n\n\nRick and Morty Palettes\n\n\n\n\n\n\n\nclustering\n\n\nrick and morty\n\n\npalettes\n\n\n\n\n\n\n\n\n\n\n\nFeb 4, 2019\n\n\nArt Steinmetz\n\n\n\n\n\n\n  \n\n\n\n\nIs Free Pre-K in NYC Favoring the Rich?\n\n\n\n\n\n\n\nweb scraping\n\n\neducation\n\n\nmaps\n\n\n\n\n\n\n\n\n\n\n\nNov 29, 2018\n\n\nArt Steinmetz\n\n\n\n\n\n\n  \n\n\n\n\nNew Winter Sports for New Countries\n\n\n\n\n\n\n\nweb scraping\n\n\nsports\n\n\n\n\n\n\n\n\n\n\n\nFeb 26, 2018\n\n\nArt Steinmetz\n\n\n\n\n\n\n  \n\n\n\n\nLive Fast, Die Young, Stay Pretty?\n\n\n\n\n\n\n\nmusic\n\n\nweb scraping\n\n\nhealth\n\n\ngapminder\n\n\n\n\n\n\n\n\n\n\n\nFeb 11, 2018\n\n\nArt Steinmetz\n\n\n\n\n\n\n  \n\n\n\n\nPlumbing the Depths of My Soul (in Facebook)\n\n\n\n\n\n\n\ntext mining\n\n\nfacebook\n\n\n\n\n\n\n\n\n\n\n\nNov 25, 2017\n\n\nArt Steinmetz\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2017-11-25-summary-analysis-of-my-facebook-existence/2017-11-25-summary-analysis-of-my-facebook-existence.html",
    "href": "posts/2017-11-25-summary-analysis-of-my-facebook-existence/2017-11-25-summary-analysis-of-my-facebook-existence.html",
    "title": "Plumbing the Depths of My Soul (in Facebook)",
    "section": "",
    "text": "First post! Let’s start out nice and easy. No big data machine learning or heavy stats. This post will merely explore the depths of my soul through a meta-analysis of every one of my Facebook posts. Meta-navel gazing, if you will.\nPerhaps you are not all that interested in the plumbing the depths of my soul. Still, you may be interested in seeing how you can do an analyis of your own Facebook life in the comfort of your own home. If so, read on!\nWe will (lightly) cover web scraping, sentiment analysis, tests of significance and visualize it with a generous helping of ggplot. Note I use the tidyverse/dplyr vernacular. This is fast becoming a dialect of R. I quite like it but its syntax is different than traditional R. It produces sometimes slower, but much more readable, code. Basically, you “pipe” data tables through action verbs using the pipe operator (“%&gt;%”).\nLet’s go do some outsider data science!\nStart by loading needed packages.\n\nlibrary(rvest)\nlibrary(stringr)\nlibrary(dplyr)\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(wordcloud)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(ggplot2)\nlibrary(zoo)\nlibrary(reshape2)\nlibrary(lubridate)\n\n\n#make explicit so kableExtra doesn't complain later\noptions(knitr.table.format = \"html\") \n\n\nFetch and clean all the words in my Facebook posts\nFacebook lets you download a log of all your activity at https://Facebook.com/settings. Look toward the bottom of the page for the download link. You will get an email with a link to a zipped set of html files. These are what I’ll be using for the analysis.\nFirst let’s get all my comments since the dawn of my Facebook existence.\n\npath='data/'\nraw_timeline&lt;- read_html(paste0(path,\"timeline.htm\"),encoding=\"UTC-8\")\n\nNow that we have the raw data we need to extract the just the text of the comments. Visually inspecting the raw html file reveals that all of the comments I wrote have the tag &lt;div class=\"comment\"&gt; so I construct an xpath selector to grab those nodes then get the text in them. This is what the raw html looks like:\n&lt;/p&gt;&lt;p&gt;&lt;div class=\"meta\"&gt;Thursday, November 16, 2017 at 1:17pm EST&lt;/div&gt; &lt;div class=\"comment\"&gt;I’m sure you are all thinking “what does this mean for Al Franken?”&lt;/div&gt; &lt;/p&gt;&lt;p&gt; &lt;div class=\"meta\"&gt;Thursday, November 16, 2017 at 10:44am EST&lt;/div&gt; Art Steinmetz shared a link. &lt;/p&gt;&lt;p&gt;\nThe challenge here is that we want to get the date also which appears BEFORE the comment and has the tag &lt;div class=\"meta\"&gt;. Unfortunately, as we see above, merely sharing a link generates this tag without any comment or a different tag class so there are more meta classes than comment classes. Facebook should create a separate XML record for each log activity, but they don’t.\nThe code below seems inelegant to me. for loops in R are non-idiomatic and indicate somebody was steeped in a non vectorized language (like me). I tried without success to craft an xpath expression that would walk backwards when it sees a comment class to get the date. In the end I resorted to the devil I know, a loop.\n\ntimeline_post_nodes &lt;- raw_timeline %&gt;% \n  html_nodes(xpath=\"//div[@class ='comment'] | //div[@class='meta']\")\n\ntimeline_posts1&lt;-NULL\n#the bit below is the slowest part of our project. \n#If you post multiple times a day over years it could take a while.\nfor (n in 1:length(timeline_post_nodes)){\n  if ( html_attr(timeline_post_nodes[n],\"class\")==\"comment\"){\n    post= html_text(timeline_post_nodes[n])\n    date= html_text(timeline_post_nodes[n-1])\n    timeline_posts1&lt;-timeline_posts1 %&gt;% bind_rows(tibble(date,post))\n  }\n}\n\nThe time stamps we extracted are just character strings with no quantitative meaning. Let’s convert the dates in the form of “Saturday November 18 2017 11:12am EST” to a day of the week and a POSIX date/time format that other R functions will understand. First we pull out the day of the week using the comma as a separator but this also separates the month and day from the year, which we don’t want, so we put those back together.\nThis begs the question of whether we should have used a tricker “regular expression” to accomplish this in one step. RegExes are a dark art that I have a lot of admiration for, even if I am a rank neophyte. In this exercise I didn’t think it was worth the time to figure out a “proper” solution when a “simple” one sufficed. Other times I like the puzzle challenge of coming up with a powerful RegEx. There are web sites that are a great help in building them. Try http://regex101.com, for one.\nWith a good date string in hand we can use parse_date() to convert it. Notice the format string we use to accomplish this.\n\ntimeline_posts&lt;-timeline_posts1 %&gt;%\n  mutate(date=sub(\"at \",\"\",date)) %&gt;%\n  separate(date,c(\"doy\",\"date\",\"yeartime\"),sep=\", \") %&gt;%\n  transmute(doy=doy,date=paste(date,yeartime),post=post)\n\n# Now that we've pulled out the day of the week, let's make sure they show in order in plots\n# by making doy and ordered factor.\nday_order&lt;-c(\"Monday\",\"Tuesday\",\"Wednesday\",\n            \"Thursday\",\"Friday\",\"Saturday\", \n            \"Sunday\")\n\ntimeline_posts$doy&lt;-factor(timeline_posts$doy,levels = day_order)\n\ntimeline_posts&lt;-timeline_posts %&gt;% \n  mutate(date = str_remove(date,\" EST| EDT\")) |&gt; \n  mutate(date = parse_datetime(date,\n                               format=\"%B %d %Y %I:%M%p\",\n                               locale = locale(tz = \"US/Eastern\")))\n\nkable(head(timeline_posts[1:2,])) %&gt;% \n  kable_styling(bootstrap_options = \"striped\", full_width = FALSE, position='left')\n\n\n\n\ndoy\ndate\npost\n\n\n\n\nSaturday\n2017-11-18 11:12:00\nI feel cheated. When I read the fine print I see these guys haven't won the \"Uniformity of Granulation\" award since 1894. I want the oatmeal that won last year!\n\n\nSaturday\n2017-11-18 10:41:00\nI had a chance to visit Shenzhen this year. The hardware scene is reminiscent of Blade Runner as you'll see. This guy prowls the markets to make his own iPhone from scratch.\n\n\n\n\n\n\n\nWe now have over 2000 text strings, each representing one post. Since we are working at the word level we need to break up each post into its constituent words.\nFor much of this analysis I am following the example shown at https://cran.r-project.org/web/packages/tidytext/vignettes/tidytext.html.\nThe unnest_tokens function from the ‘tidytext’ package lets us convert a dataframe with a text column to be one-word-per-row dataframe. How many words are there?\n\nmy_post_words&lt;-  timeline_posts %&gt;%\n  unnest_tokens(word, post)\n\nnrow(my_post_words)\n\n[1] 51347\n\n\nSo we have over fifty thousand words. A lot of them are going to be uninteresting. Although, given that Facebook posts are an excercise in narcissim, you might say all of them are uninteresting to anybody but me.\nAnyway, lets press on. We can use the stop_words data set included with tidytext to to strip out the superfluous words. Note this includes words like ‘accordingly’ which convey little meaning but might be useful in revealing idiosyncratic writting patterns, much like people punctuate their speech with vocal pauses like “like” and “right.” How many words are left after that?\n\ndata(\"stop_words\")\ncleaned_post_words &lt;- my_post_words %&gt;%\n  anti_join(stop_words,by='word')\n\nnrow(cleaned_post_words)\n\n[1] 22465\n\n\n\n\nLook at the most common words\nSo now our data set is clean and tidy. Let’s answer some questions. What are the most common words I use in posts.\n\npopular_words&lt;-cleaned_post_words %&gt;%\n  count(word, sort = TRUE)\nkable(popular_words[1:10,]) %&gt;% \n  kable_styling(bootstrap_options = \"striped\", full_width = FALSE, position='left')\n\n\n\n\nword\nn\n\n\n\n\nday\n111\n\n\nâ\n101\n\n\npeople\n97\n\n\ntime\n92\n\n\nkids\n84\n\n\nlove\n69\n\n\ncarrie\n60\n\n\nguy\n59\n\n\nfriends\n56\n\n\nart\n48\n\n\n\n\n\n\n\nI don’t know where that “a-hat” character comes from but let’s get rid of it.\n\ncleaned_post_words&lt;- cleaned_post_words%&gt;% \n  mutate(word=str_replace(word,\"â\",\"\")) %&gt;% \n  filter(str_length(word)&gt;0)\npopular_words&lt;-cleaned_post_words %&gt;%\n  count(word, sort = TRUE)\n\nAfter we strip out stop words we have less then 10,000 “real” words left.\nGood to see that my wife’s name is one of my most used words. “Kids,” “friends,” and “love” are no surprise. What’s a good way to visualize this? Word cloud!\nI love word clouds! We can easily display the most used words this way using the wordcloud package.\n\n# We love wordclouds!\n#scalefactor magnifies differences for wordcloud\nscaleFactor=1.3\nmaxWords = 200\n\n\nwordcloud(words = popular_words$word, \n          freq = popular_words$n^scaleFactor,\n          max.words=maxWords, \n          random.order=FALSE,rot.per=0.35, \n          colors=brewer.pal(8, \"Dark2\"),\n          scale = c(3,.3))\n\n\n\n\nI mentioned “Obama” about as often as I mentioned “beer.”\n\n\nDo some sentiment analysis\nI used to be jerk. But, given my age, I am entitled to call myself a curmudgeon instead. That sounds nicer somehow, and excuses my negative reaction to everything. However, given how internet discourse easily sinks into a tit-for-tat of profane hatred, I try to go against type, accentuate the positive and say nothing if I can’t say something nice. That’s the idea. How does my sour nature interact with my better intentions? We can use sentiment analysis to find out. The tidytext package also has serveral lexicons with thousands of words coded by their sentiment. Refer to http://tidytextmining.com for an excellent tutorial on this. Obviously, the isolated word approach has limitations. Context matters and by taking one word at a time we don’t capture that. So, with that caveat, how much of a downer am I?\nFirst, let’s look at the sentiment of my posts on a binary basis. Is the word positive or negative? The “bing” lexicon scores thousands of words that way. Obviously, not all the words we used are in the data set. About a third are, though.\n\ncleaned_post_words %&gt;% \n  inner_join(get_sentiments('bing'),by=\"word\") %&gt;% \n  group_by(sentiment) %&gt;% \n  summarize(count=n()) %&gt;% \n  kable() %&gt;% \n  kable_styling(bootstrap_options = \"striped\", full_width = FALSE, position='left')\n\n\n\n\nsentiment\ncount\n\n\n\n\nnegative\n1678\n\n\npositive\n1412\n\n\n\n\n\n\n\nWell, then. So I am a downer, on a net basis, but not terribly so.\nWe can make this into a word cloud, too! Here are the words I used divided by sentiment.\n\ncleaned_post_words %&gt;%\n  inner_join(get_sentiments('bing'),by=\"word\") %&gt;%\n  count(word, sentiment, sort = TRUE) %&gt;%\n  acast(word ~ sentiment, value.var = \"n\", fill = 0) %&gt;%\n  comparison.cloud(colors = c(\"#F8766D\", \"#00BFC4\"),\n                   max.words = 100)\n\n\n\n\nWait a minute! “Trump” is scored as a positive sentiment word! Is this a hidden statement by the author of the lexicon?! Doubtful. It’s “trump,” as in “spades trumps clubs,” not as a proper name. And why is “funny” a negative word? I guess it’s “funny strange,” not “funny ha-ha.” It shows the limitations of this kind of thing.\nA different lexicon scores each word’s sentiment on a scale of minus to positive five. This seems pretty subjective to me but has the benefit of letting us add up the numbers to get a net score. What is my sentiment score over all words I’ve ever written on Facebook (not all, the log doesn’t include comments to other’s posts).\n\nsentiment_score&lt;-cleaned_post_words %&gt;% \n  inner_join(get_sentiments('afinn'),by=\"word\") %&gt;% \n  pull(value) %&gt;% \n  mean()\nsentiment_score\n\n[1] 0.1610233\n\n\nWell, this draws an slightly different conclusion. The net score of my sentiment is +0.16 out of range of -5 to +5. Just barely happy. While I may use more negative than positive words, my positive words are more positive. I suspect the word “love” which we already saw is frequently used (though it is “only” a “3”) accounts for this.\nWhat were my most negative words?\n\nword_scores&lt;-cleaned_post_words %&gt;% \n  inner_join(get_sentiments('afinn'),by=\"word\") %&gt;% \n  group_by(word,value) %&gt;% summarise(count=n())\n\n`summarise()` has grouped output by 'word'. You can override using the\n`.groups` argument.\n\n\n\nword_scores %&gt;%\n        arrange((value)) %&gt;%\n        ungroup() %&gt;%\n        .[1:10,] %&gt;%\n        kable() %&gt;%\n  kable_styling(bootstrap_options = \"striped\", full_width = FALSE, position='left')\n\nThis is a family blog so I comment out the code that displays the worst words. Suffice it to say, they are the usual curse words and forms thereof. I am cringing right now. Did I say those things? Yes, well not often, at least, once or twice is typical for each.\nAs I mentioned above, the limitation of this analysis is that it lacks context. For instance, did I call someone a slut? I was briefly horrified when I saw that word. Here is the word in context from 2014: “Less slut-shaming and more perp-jailing.”\nAll these negative words carry more power for me, an old-geezer, than for kids today (kids today!) who let f-bombs roll off their tongues with uncomfortable (to me) ease. Get off my lawn!\nWhat were my positive words?\n\nword_scores %&gt;% arrange(desc(value)) %&gt;% \n  ungroup() %&gt;%\n  .[1:10,] %&gt;% \n  kable() %&gt;% \n  kable_styling(bootstrap_options = \"striped\", full_width = FALSE, position='left')\n\n\n\n\nword\nvalue\ncount\n\n\n\n\nbreathtaking\n5\n1\n\n\noutstanding\n5\n1\n\n\nthrilled\n5\n3\n\n\namazing\n4\n19\n\n\nawesome\n4\n19\n\n\nbrilliant\n4\n5\n\n\nfabulous\n4\n1\n\n\nfantastic\n4\n2\n\n\nfun\n4\n45\n\n\nfunnier\n4\n1\n\n\n\n\n\n\n\nWhew! I feel better now. Everything is awesome!\nDid I get happier or sadder over time? We’ll answer that question in a minute.\n\n\nTime Patterns\nThe foregoing analysis just includes posts on my timeline where I made a comment. If we want to know things like when I’m active on Facebook we need to look at all activity. Again, Facebook doesn’t separately tag different activities. Let’s go back over all the activity to pull out just the timestamps, but all of them this time.\n\nactivity_times &lt;- tibble(date = raw_timeline %&gt;% \n                             html_nodes(xpath=\"//div[@class='meta']\") %&gt;% \n                             html_text()\n                              ) %&gt;%\n  mutate(date=sub(\"at \",\"\",date)) %&gt;%\n  separate(date,c(\"doy\",\"date\",\"yeartime\"),sep=\", \") %&gt;%\n  transmute(doy=doy,date=paste(date,yeartime)) %&gt;%\n  mutate(date = str_remove(date,\" EST| EDT\")) |&gt; \n  mutate(date = parse_datetime(date,\n                               format=\"%B %d %Y %I:%M%p\",\n                               locale = locale(tz = \"US/Eastern\")))\n\nactivity_times$doy&lt;-factor(activity_times$doy,levels = day_order)\n\nLet’s ask a couple questions. What day of the week am I most active on Facebook?\n\n#make sure days of week are in sequential order. Monday first\n\nactivity_times %&gt;% ggplot(aes(doy))+geom_bar()+\n  labs(title='Facebook Activity', x='Weekday',y='Posts')\n\n\n\n\nMonday stands out. I didn’t realize this. Perhaps I come to work Monday morning and catch up with the news which prompts me to post.\nAm I more cranky on different days?\n\n#cleaned_post_words$doy&lt;-factor(cleaned_post_words$doy,levels = day_order)\n\nword_scores_by_weekday&lt;-cleaned_post_words %&gt;% \n  inner_join(get_sentiments('afinn'),by=\"word\") %&gt;% \n  group_by(doy)\n\nword_scores_by_weekday %&gt;%\n  summarise(mood=mean(value)) %&gt;% \n  ggplot(aes(x=doy,y=mood))+geom_col()+labs(x=\"Weekday\",y=\"Mood Score\")\n\n\n\n\nThis is interesting! I am in a relatively good mood on Monday! It’s the middle of the week when I tend to use more negative words. Then I pick up going into the weekend.\nRemember though, these are numbers of small magnitude. Are the variations statistically significant? Let’s compare Tuesday to Sunday and (which have the most extreme differences). First visually then with a t-test to see if the differences are significant. For our hypothesis we assume the the true difference in the average mood on Monday is no different than the average mood on Sunday. Based on the differences we see, can we reject this hypothesis?\n\nsunday_moods&lt;-word_scores_by_weekday %&gt;% \n  filter(doy==\"Sunday\") %&gt;% \n  group_by(doy,date) %&gt;% \n  summarise(mood=mean(value)) %&gt;% \n  select(doy,mood)\n\n`summarise()` has grouped output by 'doy'. You can override using the `.groups`\nargument.\n\ntuesday_moods&lt;-word_scores_by_weekday %&gt;% \n  filter(doy==\"Tuesday\") %&gt;% \n  group_by(doy,date) %&gt;% \n  summarise(mood=mean(value)) %&gt;% \n  select(doy,mood)\n\n`summarise()` has grouped output by 'doy'. You can override using the `.groups`\nargument.\n\nbind_rows(tuesday_moods,sunday_moods) %&gt;% ggplot(aes(mood,fill=doy))+geom_density(alpha=0.7)\n\n\n\n\n\nt.test(tuesday_moods$mood,sunday_moods$mood)\n\n\n    Welch Two Sample t-test\n\ndata:  tuesday_moods$mood and sunday_moods$mood\nt = -0.97824, df = 332.11, p-value = 0.3287\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.6292549  0.2112694\nsample estimates:\n mean of x  mean of y \n0.06088435 0.26987711 \n\n\nRats! It looks like our “interesting” observation is not interesting. The p-value of 0.32 is below 2, so we can’t reject our hypothesis. The difference in mean sentiment for Sunday and Tuesday would have to be beyond the confidence interval to give us acceptable certainty that I am most cranky on Tuesday.\nWe can’t get too excited by the density plot, either. My posts are bi-modally distributed but, given the relatively short length of my posts, chances are I use just one sentiment-loaded word and that skews the distribution. Again, small sample sizes are the problem. Pretty picture, though!\nWhat times am I most active?\n\nhours&lt;-cleaned_post_words %&gt;% mutate(hour=hour(date))\n\nhours %&gt;% ggplot(aes(hour))+geom_bar()+\n  labs(title='Facebook Activity', x='Hour',y='Posts')\n\n\n\n\n#Trends over Time\nIs there any trend to my Facebook activity over time? Let’s bucket the posts by month and look for a pattern.\n\nactivity_times &lt;- activity_times %&gt;% \n  #filter dates before I joined as bad data\n  filter(date&gt;as.Date(\"2008-01-01\")) %&gt;% \n  mutate(month=as.yearmon(date))\n\nactivity_times %&gt;% \n  ggplot(aes(as.Date(month))) + geom_bar() +labs(x=\"Month\",y=\"Posts\")\n\n\n\n\nWhat’s up with the beginning of 2013 and December in 2015? Looking at the raw activity log I see that I briefly let Spotify tell you what I was listening to via Facebook. That generated a lot of activity. I turned it off after a couple weeks. In late 2016 around the election we also see an uptick in activity. Otherwise there have been pretty mild ebbs and flows, averaging about 30 posts per month.\n\nactivity_times %&gt;% \n  group_by(month) %&gt;% \n  summarise(n=n()) %&gt;% \n  summarise(avg_per_month=mean(n)) %&gt;% \n  kable() %&gt;% \n  kable_styling(bootstrap_options = \"striped\", full_width = FALSE, position='left')\n\n\n\n\navg_per_month\n\n\n\n\n29.42105\n\n\n\n\n\n\n\n\n\nDoes my mood change over time?\nWe can repeat the sentiment analysis from above but bucket it by month.\n\nword_scores_by_month&lt;-cleaned_post_words %&gt;% \n  inner_join(get_sentiments('afinn'),by=\"word\") %&gt;% \n  select(date, word,value) %&gt;% \n  mutate(yearmonth=as.yearmon(date)) %&gt;% \n  group_by(yearmonth) %&gt;% summarise(mood=sum(value))\n\nword_scores_by_month %&gt;%  \n  ggplot(aes(x=as.Date(yearmonth),y=mood))+geom_col()+geom_smooth()+labs(x=\"Month\",y=\"Mood Score\")\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nA trend is not very evident. Month-to-month variation is very high. Is it that we don’t have a good sample size or do my moods swing wildly? The most extreme gyration is around the 2016 presidential election. Optimism followed by despair? Perhaps.\n\n\nPolitics Rears Its Ugly Head\nI try to avoid too much talk about politics on Facebook but, like most of us, it was tough in an election year. This gives us an opportunity to dig into a specific topic within the posting corpus.\nLet’s start by seeing how often I mentioned politicians names.\n\npoliticos=c(\"obama\",\"trump\",\"hillary\",\"clinton\",\"johnson\",\"kasich\",\"bush\",\"sanders\",\"romney\",\"mccain\",\"palin\")\n\ngg&lt;-cleaned_post_words %&gt;% \n  filter(word %in% politicos) %&gt;%\n  mutate(word=str_to_title(word)) %&gt;% \n  count(word, sort = TRUE) %&gt;% \n  \n  ggplot(aes(x=reorder(word,n),y=n))+geom_col()+coord_flip()+labs(y=\"Mentions\",x=\"\")\n\ngg\n\n\n\n\n“Johnson”” is Gary Johnson, in case you forgot. Unlike the the “lamestream media,” I gave much more attention to the Libertarian candidate in my posts. Oddly, that didn’t help his success in the election.\n“Hillary”” is the only first name in the list. Using a woman’s first name when equally familiar men are referred to with their last name is often sexist. In my defense, during the election it was important to distinguish between her and husband Bill so that’s why I used “Hillary.” We are not on a first name basis. On the other hand, “Bill” is a common enough name (and noun) so it’s likely that many posts using it don’t refer to the politician. Just to get an idea let’s look at the words bracketing “bill”:\n\ncleaned_post_words %&gt;% \n  mutate(phrase=paste(lag(word),word,lead(word))) %&gt;% \n  filter(word=='bill') %&gt;% \n  select(date,phrase) %&gt;% \n  kable() %&gt;% \n  kable_styling(bootstrap_options = \"striped\", full_width = FALSE, position='left')\n\n\n\n\ndate\nphrase\n\n\n\n\n2017-09-20 08:53:00\nmorning bill protection\n\n\n2017-07-23 12:27:00\nwater bill extra\n\n\n2017-03-11 14:15:00\nstitch bill doctor\n\n\n2017-03-11 13:51:00\npass bill nancy\n\n\n2017-03-05 10:45:00\nfront bill maudlin\n\n\n2016-10-18 13:13:00\nfriends bill fast\n\n\n2016-09-28 21:46:00\njohnson bill weld\n\n\n2016-09-28 21:46:00\nidealist bill policy\n\n\n2016-07-28 14:28:00\ndelegates bill clinton\n\n\n2012-08-02 12:59:00\ngovernor bill haslam\n\n\n2012-05-03 18:43:00\njobs bill money\n\n\n2012-05-03 18:43:00\ntalking bill drummond\n\n\n2012-04-21 17:45:00\nxtc bill lot\n\n\n2010-10-20 21:12:00\nbetty bill bartlett\n\n\n2009-07-02 16:57:00\ndisco bill nelson\n\n\n2009-03-09 08:36:00\npay bill 20\n\n\n2009-02-27 18:17:00\nkill bill banks\n\n\n\n\n\n\n\nIt looks like just one of the mentions of “Bill” is Bill Clinton, so we can ignore him (for this project, anyway).\nWas the uptick we saw above in posting activity around the election due to political activty? We asssume, but let’s look just the posts containing the names of the politcians I identified earlier. This does not include links I shared containing names but did not comment upon, as Facebook won’t tell me what the link was.\n\ncleaned_post_words %&gt;% \n  mutate(month=as.yearmon(date)) %&gt;% \n  filter(word %in% politicos) %&gt;% \n  mutate(word=str_to_title(word)) %&gt;% \n  group_by(month,word) %&gt;% \n  summarize(n=n()) %&gt;% \n  ggplot(aes(as.Date(x=month),y=n,fill=word)) + geom_col() +labs(x=\"Month\",y=\"Mentions\")\n\n`summarise()` has grouped output by 'month'. You can override using the\n`.groups` argument.\n\n\n\n\n\nYup, I guess so. Unlike some of my wonderful Facebook friends I have been able to let go of the emotions around the election, as my posts naming politicians have dropped back to their baseline levels. Naturally, the names have changed!\nCan you tell my political leanings from this data? Well, duh! You could just read the posts but where’s the fun in that? More practically, if we are analyzing big data sets it would be tough to glean the overall sentiment of many people from reading a feasible sample of posts. Let’s try running the sentiment analysis on all of the posts containing the name of a politician to see of anything emerges.\nNote, I often mention more than one name in a single post and this method won’t distinguish which words apply to which name.\n\npol_sentiment&lt;- NULL\npol_sent&lt;- NULL\nfor (pol in politicos) {\n  sent_words&lt;-cleaned_post_words %&gt;% \n    filter(word == pol) %&gt;%\n    select(date) %&gt;% \n    unique() %&gt;% \n    inner_join(cleaned_post_words,by='date') %&gt;%\n    select(word) %&gt;% \n    inner_join(get_sentiments('afinn'),by=\"word\")\n  #did we get anything?\n  if(nrow(sent_words)&gt;0){\n    avg_score&lt;-summarise(sent_words,opinion=mean(value),post_count=n())\n    pol_sentiment&lt;-bind_rows(bind_cols(Politician=str_to_title(pol),avg_score),pol_sentiment)\n    pol_sent&lt;-bind_rows(tibble(Politician=str_to_title(pol),\n                                   opinion=avg_score$opinion,\n                                   post_count=avg_score$post_count,\n                                   sent_words=list(sent_words$value)\n                                   ),\n    pol_sent)\n    \n  }\n\n}\npol_sent[,1:3]\n\n# A tibble: 8 × 3\n  Politician opinion post_count\n  &lt;chr&gt;        &lt;dbl&gt;      &lt;int&gt;\n1 Romney     -1.75            4\n2 Sanders     0.0333         30\n3 Bush       -0.410          39\n4 Johnson     0.226          84\n5 Clinton     0.0345         29\n6 Hillary     0.0108         93\n7 Trump       0.171          82\n8 Obama      -0.112          98\n\n\nFirst off, Palin and McCain don’t appear in this list beacuse I apparently didn’t use any words from the “afinn” lexicon in my posts mentioning them. Second, Romney is only mentioned in four posts so I don’t think we have a valid sample size. Notice that we store the list of sentiment scores for each politician in sent_words. We’ll use that in a minute.\nTaking what’s left, let’s view a chart.\n\npol_sent %&gt;% filter(Politician != \"Romney\") %&gt;% \n  ggplot(aes(x=reorder(Politician,opinion),y=opinion))+geom_col()+\n  coord_flip()+labs(y=\"Sentiment\",x=\"\")\n\n\n\n\nI’m sad to say this is far from how I would rank order my feelings about each of these names. I was a Gary Johnson supporter but, beyond that, this list might as well be random. Sample size is an issue. I am not that prolific a poster and the words I have in common with the sentiment lexicon is fewer still. Also, remember the sentiment ranges run from -5 to +5. We are talking small magnitudes here. Here is the same chart with the maximum allowable range shown.\n\npol_sent %&gt;% filter(Politician != \"Romney\") %&gt;% \n  ggplot(aes(x=reorder(Politician,opinion),y=opinion))+geom_col()+\n  coord_flip()+labs(y=\"Sentiment\",x=\"\")+ylim(c(-5,5))\n\n\n\n\nLet’s subject this to a test of significance. The “null hypothesis” we want to test is that the mean sentiment score expressed for “Hillary” is not statistically different from the “Trump” score.\n\ntrump_opinion&lt;-pol_sent %&gt;% filter(Politician==\"Trump\") %&gt;% pull(sent_words) %&gt;% unlist()\nhillary_opinion&lt;-pol_sent %&gt;% filter(Politician==\"Hillary\") %&gt;% pull(sent_words) %&gt;% unlist()\nt.test(trump_opinion,hillary_opinion)\n\n\n    Welch Two Sample t-test\n\ndata:  trump_opinion and hillary_opinion\nt = 0.46581, df = 170.76, p-value = 0.6419\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.5179631  0.8379211\nsample estimates:\n mean of x  mean of y \n0.17073171 0.01075269 \n\n\nLike before, we can’t reject our hypothesis, I liked Donald Trump as much as I liked Hillary Clinton or, rather, you can’t prove a thing!\nThis analysis shows I didn’t really express strong opinions. It was deliberate. During the election things got pretty heated, as you may recall. I have Facebook friends on all sides of the the political divides. Out of respect, I tried very hard not to trash anybody’s candidate and instead tried to accentuate the positive.\n#Bonus Friend Analysis!\nOne more thing. We’ve only looked at the ‘timeline.htm’ file but Facebook’s data dump includes a lot of other stuff including a list of your friends and the date they were added. We can look at a timeline and summary statistics for for this too.\n\nraw_friends&lt;- read_html(paste0(path,\"friends.htm\"),encoding=\"UTC-8\")\n\nHere the &lt;h2&gt; tag separates the labels for the different friend interactions. What are the categories of interactions Facebook gives us?\n\nfriend_nodes &lt;- raw_friends %&gt;% \n  html_nodes(\"h2\") %&gt;% html_text()\n\nfriend_nodes\n\n[1] \"Friends\"                  \"Sent Friend Requests\"    \n[3] \"Received Friend Requests\" \"Deleted Friend Requests\" \n[5] \"Removed Friends\"          \"Friend Peer Group\"       \n\n\nThe actual items are in lists separated by the &lt;ul&gt; tag. Let’s traverse the list, extracting the category, name and date for each. The first and the last lists do not contain relevant info so we’ll take just the middle five. We’ll also rename the “Friends” category to the more descriptive “Friends Added.”\n\nfriend_nodes[1]&lt;-\"Friends Added\"\nlist_nodes &lt;- raw_friends %&gt;% \n  html_nodes(\"ul\") %&gt;% \n  .[2:6]\n\nfriend_table&lt;- NULL\nfor (node in 1:length(list_nodes)){\n  category&lt;-friend_nodes[node]\n  items&lt;- list_nodes[node] %&gt;% \n    html_nodes(\"li\") %&gt;% \n    html_text() %&gt;% \n    tibble(item=.) %&gt;% \n    separate(item,c(\"Name\",\"Date\"),sep=\"\\\\(\",remove=TRUE) %&gt;% \n    mutate(Date=str_replace(Date,\"\\\\)\",\"\"))\n  \n    friend_table&lt;-cbind(Category=category,items,stringsAsFactors=FALSE) %&gt;% \n      bind_rows(friend_table) %&gt;% \n      as_tibble()\n}\n\nHow many Facebook friends do I have? Not many, by Facebook standards. This is, of course, the question that measures our entire self-worth. I’m very discriminating about who I friend . Well, that plus I’m not a very likeable person. Only four people have become so annoying that I unfriended them (both political extremes). There are more that I unfollowed (too many cat videos) but Facebook doesn’t include them in the log for some reason. Facebook also won’t tell me who unfriended me. But I’m sure that no one would do THAT.\n\nfriend_table %&gt;% group_by(Category) %&gt;% summarize(Number=n())\n\n# A tibble: 5 × 2\n  Category                 Number\n  &lt;chr&gt;                     &lt;int&gt;\n1 Deleted Friend Requests      67\n2 Friends Added               167\n3 Received Friend Requests      7\n4 Removed Friends               4\n5 Sent Friend Requests          3\n\n\nOnce again we have a character string for the date which we need to turn into a proper date. The wrinkle here is dates in the current year don’t specify the year in the log. We have to manually add it. The data is at a daily resolution, which is too granular for a clear picture at my level of activity. Let’s make it quarterly.\n\nfriend_table2 &lt;- friend_table %&gt;% \n  mutate(Date=if_else(str_length(Date)&lt;7,paste0(Date,\", \",year(Sys.Date())),Date)) %&gt;%\n  mutate(Date=parse_date(Date,format=\"%b %d, %Y\")) %&gt;% \n  select(Category,Date) %&gt;%\n  mutate(yearquarter=as.yearqtr(Date)) %&gt;%\n  group_by(yearquarter)\ngg&lt;-friend_table2 %&gt;% ggplot(aes(x=as.Date(yearquarter),fill=Category))+ geom_bar(width=70)\ngg&lt;-gg +labs(title=\"Facebook Friending Activity\",y=\"Count\",x=\"Quarterly\")\ngg\n\n\n\n\nNot too surprising. There was a lot of friending happening when I first joined Facebook. Perhaps a bit more curious is the recent renewed uptick in friending. There has been an influx of renewed high school aquaintances.\nSometimes it is useful too look at the balance of opposites. For example, we can see the balance of the number of friendings vs. the number of delete friend requests by assigning a negative number to deletions. There is no simple way to do this with native dplyr functions, though there should be. Base R is actually better at transforming just certain elements in a column based on some condition. Fortunately, I found a super-useful bit of code on Stack Overflow, mutate_cond(), that does exactly what we need.\n\nmutate_cond &lt;- function(.data, condition, ..., envir = parent.frame()) {\n  #change elements of a column based on a condition\n  #https://stackoverflow.com/questions/34096162/dplyr-mutate-replace-on-a-subset-of-rows/34096575#34096575\n  condition &lt;- eval(substitute(condition), .data, envir)\n    condition[is.na(condition)] = FALSE\n    .data[condition, ] &lt;- .data[condition, ] %&gt;% mutate(...)\n    .data\n}\n\n# tabulate sums of categories by quarter\nfriend_table3 &lt;- friend_table %&gt;% \n  mutate(Date=if_else(str_length(Date)&lt;7,paste0(Date,\", \",year(Sys.Date())),Date)) %&gt;%\n  mutate(Date=parse_date(Date,format=\"%b %d, %Y\")) %&gt;% \n  mutate(yearquarter=as.yearqtr(Date)) %&gt;%\n  select(Category,yearquarter) %&gt;%\n  group_by(Category,yearquarter) %&gt;% \n  summarise(count=n())\n\n`summarise()` has grouped output by 'Category'. You can override using the\n`.groups` argument.\n\n#make deleted requests negative\ngg&lt;-friend_table3 %&gt;% \n  mutate_cond(Category==\"Deleted Friend Requests\",count=-count) %&gt;% \n  filter(Category==\"Deleted Friend Requests\" | Category==\"Friends Added\") %&gt;% \n  ggplot(aes(x=as.Date(yearquarter),y=count,fill=Category))+ geom_col() \n\ngg&lt;-gg +labs(title=\"Facebook Friending Activity\",y=\"Count\",x=\"Quarterly\")\ngg\n\n\n\n\nIt seems that adding friends is associated with deleted requests. I’ll surmise that when I show up in a new friend’s network that will spark some friend requests from their network. Some, maybe most, will be from people I don’t actually know and I will reject. There are spikes in rejections because I let them stack up before I notice them.\n\n\nWrapping Up\nWell that was cool. We got to try a lot of things. HTML parsing, date functions, sentiment analysis, text mining and lots of dplyr manipulations. Like a lot of projects, once I got going I thought of many things to try beyond the initial scope. That’s where the fun is when you’re not on deadline and deliverables. Thanks for making it all the way through. Hopefully this gives you some ideas for your own explorations. Now you try!\n#Double Bonus! Making the cool speedometer at the top of this post\nBecause we love cool visualizations, let’s show my mood on a silly gauge. I won’t show the low-level code I used to generate it because it’s basically a copy of what you can find here: http://www.gastonsanchez.com/. Gaston is a visualization guru extrordinaire. This shows how far you can take base R graphics.\nWhat do you think your gauge would look like? If it’s in the red call the suicide prevention hotline.\nThe animation is created using the magick package. I am thrilled that this recent release brings the image magick program inboard to R so we no longer have to run an external program to render animated files like GIFs (which I insist on pronouncing with a hard ‘G’, BTW.)\n\n# create animated mood gif for top of notebook.\nlibrary(magick)\nmy_days_moods&lt;-word_scores_by_weekday %&gt;%\n  summarise(mood=mean(value))\n\n#interpolate to create more points for a smooth animation.\n# the trick is to create a series where the mood stays constant for a number of frames\n# then transitions smoothly to the next mood value. Examine interp_moods to see how.\ninterp_moods&lt;-tibble(doy=unlist(lapply(levels(my_days_moods$doy),rep,10)),\n                         mood_label=round(unlist(lapply(my_days_moods$mood,rep,10)),2),\n                         mood=approx(x=1:14,unlist(lapply(my_days_moods$mood,rep,2)),n=70)$y)\n\n\ninterp_moods$mood_label&lt;- paste(ifelse(interp_moods$mood_label&gt;0,\"+\",\"\"),interp_moods$mood_label)\n\n# I'll spare you the details of the low-level code.\n# see it at http://www.gastonsanchez.com/.\nsource(\"g_speedometer.r\")\n\nimg &lt;- image_graph(600, 400, res = 96)\nfor(n in 1:nrow(interp_moods)){\n  plot_speedometer(label=interp_moods$doy[n],\n                   value=round(interp_moods$mood[n],2),\n                   bottom_label=interp_moods$mood_label[n],\n                   min=-0.5,\n                   max=0.5)\n  text(-0.1,1.0,\"Faceboook Mood-o-Meter\",cex=1.3)\n}\ndev.off()\nimg &lt;- image_background(image_trim(img), 'white')\nanimation &lt;- image_animate(img, fps = 10)\nimage_write(animation, path = \"moods.gif\", format = \"gif\")"
  },
  {
    "objectID": "posts/2018-02-11-live-fast-die-young-stay-pretty/2018-02-11-live-fast-die-young-stay-pretty.html",
    "href": "posts/2018-02-11-live-fast-die-young-stay-pretty/2018-02-11-live-fast-die-young-stay-pretty.html",
    "title": "Live Fast, Die Young, Stay Pretty?",
    "section": "",
    "text": "Live fast, die young, stay pretty? That’s the stereotype for rockers, or it was. We only need to look at Keith Richards, over 70 and going strong, to find a major counterexample. Do rockers die young? What do they die of? How does that compare to the broader population (in the U.S., anyway). It turns out there are some suprising answers to those questions.\n\n\nAlong the way we’ll learn something about web scraping, html parsing and some ggplot2 tricks. We use the tidyverse dialect throughout, just so you know."
  },
  {
    "objectID": "posts/2018-02-11-live-fast-die-young-stay-pretty/2018-02-11-live-fast-die-young-stay-pretty.html#analyzing-deaths-of-rock-musicians",
    "href": "posts/2018-02-11-live-fast-die-young-stay-pretty/2018-02-11-live-fast-die-young-stay-pretty.html#analyzing-deaths-of-rock-musicians",
    "title": "Live Fast, Die Young, Stay Pretty?",
    "section": "",
    "text": "Live fast, die young, stay pretty? That’s the stereotype for rockers, or it was. We only need to look at Keith Richards, over 70 and going strong, to find a major counterexample. Do rockers die young? What do they die of? How does that compare to the broader population (in the U.S., anyway). It turns out there are some suprising answers to those questions.\n\n\nAlong the way we’ll learn something about web scraping, html parsing and some ggplot2 tricks. We use the tidyverse dialect throughout, just so you know."
  },
  {
    "objectID": "posts/2018-02-11-live-fast-die-young-stay-pretty/2018-02-11-live-fast-die-young-stay-pretty.html#a-note-on-pointers.",
    "href": "posts/2018-02-11-live-fast-die-young-stay-pretty/2018-02-11-live-fast-die-young-stay-pretty.html#a-note-on-pointers.",
    "title": "Live Fast, Die Young, Stay Pretty?",
    "section": "\nA note on “pointers.”\n",
    "text": "A note on “pointers.”\n\n\nR and the rvest package have some great functions for converting html &lt;table&gt;s into data frames. rvest is a very powerful package but one thing I learned is that it works with pointers to the data rather than the actual data. C programmers and old geezers like me will be familiar with this. I remember pop and push and stacks and all that stuff from the old days. R generally doesn’t pass values “by reference.” It passes “by value.” That’s why when you modify data in the scope of a function it doesn’t affect the value of the data outside unless you assign it with return &lt;data&gt;. Using pointers in rvest functions means modifications to html data happen without an explicit assigment.\n\n\nConsider a trival example:\n\n#usual R behavior\nmy_function&lt;- function(x){return (x+1)}\ndata=3\n`#this doesn't change data but it would if data was passed by reference\nmy_function(data)\n`# [1] 4\ndata\n`# [1] 3\n`#this does change data in the usual R way\ndata&lt;-my_function(data)\ndata\n`# [1] 4\n\nIf we were passing values “by reference” my_function(data) would change data without the need to assign it back to data. That’s how rvest works.\n\n\nWe use this behavior to combine the tables in the two Wikipedia articles into one html page by extracting the tables in the second wiki article and making them xml siblings of the tables in the first.\n\n\nAlternatively, we could load the two pages, extract the tables separately and combine them later but this is trickier!\n\n#join pre-2010 to post 2010\ndeath_page&lt;-death_page1\ndeath_page_child&lt;-death_page1 %&gt;% xml_children() %&gt;% .[2]\ndeath_page2_child&lt;-death_page2 %&gt;% xml_children() %&gt;% .[2]\n#create one big web page by adding all the first level children from the second\n#page to the first.\n#This modifies death_page by changing the list of pointers associated with it.\nxml_add_sibling(death_page_child,death_page2_child)\nwrite_html(death_page,file=\"death_page.html\")"
  },
  {
    "objectID": "posts/2018-02-26-new-winter-sports-for-new-countries/2018-02-26-new-winter-sports-for-new-countries.html",
    "href": "posts/2018-02-26-new-winter-sports-for-new-countries/2018-02-26-new-winter-sports-for-new-countries.html",
    "title": "New Winter Sports for New Countries",
    "section": "",
    "text": "Norway is a tiny country that punches way above its weight in the Winter Olympic medal count. We are not surprised as those folks are practically born on skis. At the same time, toussle-haired surfer dudes and dudettes from the US seem to be all over the hill when snowboards are involved. Notably, the sports where the US is most visible are sports which arose fairly recently. Is there a pattern here? Let’s do a quick hit to see if we can visualize the dominance of countries, not by event, but by vintage of a sport’s introduction to the games.\n\nlibrary(tidyverse)\nlibrary(rvest)\nlibrary(knitr)"
  },
  {
    "objectID": "posts/2018-02-26-new-winter-sports-for-new-countries/2018-02-26-new-winter-sports-for-new-countries.html#a-digression",
    "href": "posts/2018-02-26-new-winter-sports-for-new-countries/2018-02-26-new-winter-sports-for-new-countries.html#a-digression",
    "title": "New Winter Sports for New Countries",
    "section": "\nA Digression\n",
    "text": "A Digression\n\n\nA best practice with tidy data is to have every observation and every variable in a single data table. Where we want to use the data in a related table we use _join to add the data to the main table. This runs contrary to best practice in the early days of PC databases where “relational” was a big part of data manipulation. The data tables were kept separate and linked by keys. Keys are still how _join works, of course, but we just make one humongous table rather than look up the related fields on the fly. This is faster but uses more memory and/or storage. Back in the day when a couple megabytes of RAM was a lot, we cared about those things, even for small data projects. Now, we use local million-row tables with nary a blink of the eye. You kids don’t know how tough it was!"
  },
  {
    "objectID": "posts/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich.html",
    "href": "posts/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich.html",
    "title": "Is Free Pre-K in NYC Favoring the Rich?",
    "section": "",
    "text": "A hallmark of mayoral administration of NYC Mayor Bill DeBlasio has been free pre-K for all New York families. When the program was initially rolled out there were complaints in some quarters that upper-income neighborhoods were getting more slots.\n\n\nThis is an exploration comparing income to pre-K seats by neighborhoods. It was done mainly to help me practice with the whole workflow of data gathering, document parsing, and data tidying - plus making cool bi-variate choropleth maps! I had to invent a novel method in R to get a good looking bivariate legend onto the chart.\n\n\nThanks to Joshua Stevens for the inspiration and color theory of bi-variate maps (http://www.joshuastevens.net/cartography/make-a-bivariate-choropleth-map/). Thanks to Ari Lamstein for the awesome suite of choropleth packages (http://www.arilamstein.com/).\n\n\nIn my original version I use an outside program, PDFTOTEXT.EXE, to get parseable text out of the PDF documents at the NYC.gov web site. I share the commented out code for this here but skip the step in the notebook to save run time. Instead, I load the raw converted text files to illustrate the parsing.\n\n\nA further complication is to directly grab the income and population data from the census bureau requires an API key. You’ll have to get your own here: . I comment out the relevant lines but instead provide the raw downloaded data sets to illustrate how they get manipulated.\n\n\nNOTE: This analysis was originally done back in 201. The data is from that time. The URLs for city’s directories have changed and so too have the formats. The web scraping routines need to be modified accordingly.\n\n\nYou can find the raw data in CSV format at https://github.com/apsteinmetz/PreK."
  },
  {
    "objectID": "posts/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich.html#use-the-acs-package-to-construct-the-queries-for-census-api",
    "href": "posts/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich.html#use-the-acs-package-to-construct-the-queries-for-census-api",
    "title": "Is Free Pre-K in NYC Favoring the Rich?",
    "section": "\nUse the acs package to construct the queries for census api\n",
    "text": "Use the acs package to construct the queries for census api\n\n# -----------------------------------------------------------------------\n# get census data on children and income\n#census api key\n#see acs package documentation\n#api.key.install('your key here')\n\n# NYC county codes\nnyc_fips = c(36085,36005, 36047, 36061, 36081)\n#get the zips for all nyc counties\ndata(\"zip.regions\")\nnyc_zips&lt;-data.frame(county.fips.numeric=nyc_fips)%&gt;%inner_join(zip.regions)%&gt;%select(region)%&gt;%t\n# make an ACS geo set\nnycgeo&lt;- acs::geo.make(zip.code = nyc_zips)\n\n##Connect to census.gov Requires an API key. You can uncomment the lines below if you have a key. Otherwise skip to the next section to load the raw csv files which were prepared for this notebook.\n\n\n# Household Household income is table 190013, per capita income is 19301\n#income&lt;-acs::acs.fetch(endyear=2011,geography=nycgeo,table.number=\"B19013\")\n# #get relevant data into a data frame format\n#inc&lt;-cbind(acs::geography(income),acs::estimate(income))\n# kidsUnder3&lt;-acs::acs.fetch(endyear=2011,geography=nycgeo,table.number=\"B09001\",keyword = \"Under 3\")\n# kids&lt;-cbind(acs::geography(kidsUnder3),acs::estimate(kidsUnder3))\n# totalPop&lt;-acs.fetch(endyear=2011,geography=nycgeo,table.number=\"B01003\")\n# pop&lt;-cbind(geography(totalPop),estimate(totalPop))\n\n##Alternatively, load from csv files …the data we would have otherwise gotten from census.gov. Comment this chunk out if you fetch the census data directly.\n\n#if we can't connect to census.gov\ninc&lt;-read_csv('data/NYCincome.csv',col_types = \"ccd\")\nkids&lt;-read_csv('data/NYCkids.csv',col_types = \"ccd\")\npop&lt;-read_csv('data/NYCpopulation.csv',col_types = \"ccd\")\n\n##Massage the census data\n\nnames(inc)&lt;-c(\"NAME\",\"zip\",\"HouseholdIncome\")\n#needs some cleanup of dupes. I don't know why\ninc&lt;-distinct(select(inc,zip,HouseholdIncome))\n\n#kids under 3 in 2011 should approximate Pre-K kids in 2015\nnames(kids)&lt;-c(\"NAME\",\"zip\",\"kidsUnder3\")\nkids&lt;-distinct(select(kids,zip,kidsUnder3))\nkids&lt;-kids %&gt;% select(zip,kidsUnder3) %&gt;% distinct() %&gt;% filter(kidsUnder3!=0 | !is.na(kidsUnder3))\n\nnames(pop)&lt;-c(\"NAME\",\"zip\",\"totPop\")\npop&lt;-pop%&gt;%select(zip,totPop)%&gt;%distinct()%&gt;%filter(totPop!=0)\n\ncensus&lt;-pop%&gt;%inner_join(kids)%&gt;%inner_join(inc)%&gt;%mutate(zip=as.character(zip))"
  },
  {
    "objectID": "posts/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich.html#look-at-some-preliminary-pictures-from-the-census",
    "href": "posts/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich.html#look-at-some-preliminary-pictures-from-the-census",
    "title": "Is Free Pre-K in NYC Favoring the Rich?",
    "section": "\nLook at some preliminary pictures from the census\n",
    "text": "Look at some preliminary pictures from the census\n\n\nSo now we have some census data. We can use the ‘chorplethr’ package to easily create some meaningful maps. Let’s look at where the kids are and what incomes are in NYC Zip codes. Note that the ‘choroplethr’ package requires the inputs to be in a data frame where the geographic identifier is labeled “region” and the data to be displayed is labeled “value.”\n\n#where are zips with the most rugrats?\nkidsChor &lt;- census %&gt;% \n  transmute(region = zip, value = kidsUnder3 / totPop * 100)\nzip_choropleth(kidsChor, \n               zip_zoom = nyc_zips, \n               title = \"Percentage of Kids Under 3 in 2011\")\n\n\n\nincomeChor &lt;- census %&gt;% \n  transmute(region = zip, \n            value = HouseholdIncome)\nzip_choropleth(incomeChor, \n               zip_zoom = nyc_zips, \n               title = \"Household Income 2011\")\n## Warning in self$bind(): The following regions were missing and are being\n## set to NA: 10174, 10119, 11371, 10110, 10271, 10171, 10177, 10152, 10279,\n## 10115, 11430, 10111, 10112, 10167, 11351, 11359, 11424, 11425, 11451,\n## 10169, 10103, 10311, 10153, 10154, 10199, 10165, 10168, 10278, 10020,\n## 10173, 10170, 10172"
  },
  {
    "objectID": "posts/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich.html#download-pdfs-from-nyc.gov",
    "href": "posts/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich.html#download-pdfs-from-nyc.gov",
    "title": "Is Free Pre-K in NYC Favoring the Rich?",
    "section": "\nDownload PDFs from NYC.gov\n",
    "text": "Download PDFs from NYC.gov\n\n\nDownload the PDFs then convert to text using an outside program, PDFTOTEXT.EXE (http://www.foolabs.com/xpdf/home.html).\n\n\n# # -----------------------------------------------------------------------\n# # get NYC data on pre-K programs\n# # scan seat directory pdfs and put into a data frame by zip code\n# #DOE pre-k directories\n# urls&lt;- c(\"http://schools.nyc.gov/NR/rdonlyres/1F829192-ABE8-4BE6-93B5-1A33A6CCC32E/0/2015PreKDirectoryManhattan.pdf\",\n#          \"http://schools.nyc.gov/NR/rdonlyres/5337838E-EBE8-479A-8AB5-616C135A4B3C/0/2015PreKDirectoryBronx.pdf\",\n#          \"http://schools.nyc.gov/NR/rdonlyres/F2D95BF9-553A-4B92-BEAA-785A2D6C0798/0/2015PreKDirectoryBrooklyn.pdf\",\n#          \"http://schools.nyc.gov/NR/rdonlyres/B9B2080A-0121-4C73-AF4A-45CBC3E28CA3/0/2015PreKDirectoryQueens.pdf\",\n#          \"http://schools.nyc.gov/NR/rdonlyres/4DE31FBF-DA0D-4628-B709-F9A7421F7152/0/2015PreKDirectoryStatenIsland.pdf\")\n# \n# #assumes pdftotext.exe is in the current directory.  Edit as necessary\n# exe &lt;- \"pdftotext.exe\"\n# \n# #regex to parse address line\n# pkseattokens &lt;-\"(Address: )([.[:alnum:]- ()]+),+ ([0-9]{5})([a-zA-Z .()-:]+) ([0-9]{1,4}) (FD|HD|AM|PM|5H)\"\n# \n# # each of the PDF directories have 27 pages of intro material. Skip it. This might change for different years. Check PDFs\n# firstPage = 28\n# \n# dests &lt;- tempfile(str_match(urls,\"Directory(\\\\w.+).pdf\")[,2],fileext = \".pdf\")\n# txt&lt;- NULL\n# for (i in 1:length(urls)) {\n#   download.file(urls[i],destfile = dests[i],mode = \"wb\")\n#   # pdftotxt.exe is in current directory and convert pdf to text using \"table\" style at firstpage\n#   result&lt;-system(paste(exe, \"-table -f\", firstPage, dests[i], sep = \" \"), intern=T)\n#   # get txt-file name and open it  \n#   filetxt &lt;- sub(\".pdf\", \".txt\", dests[i])\n#   txt &lt;- append(txt,readLines(filetxt,warn=FALSE))\n# }"
  },
  {
    "objectID": "posts/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich.html#alternatively-import-and-combine-the-already-converted-text-files.",
    "href": "posts/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich.html#alternatively-import-and-combine-the-already-converted-text-files.",
    "title": "Is Free Pre-K in NYC Favoring the Rich?",
    "section": "\nAlternatively, import and combine the already converted text files.\n",
    "text": "Alternatively, import and combine the already converted text files.\n\nboroughList &lt;- c('Manhattan','Bronx','Brooklyn','Queens','Staten')\ntxt&lt;-NULL\nfor (borough in  boroughList){\n  # get txt-file name and open it  \n  filetxt &lt;- paste(\"data/\",borough, \".txt\", sep='')\n  txt &lt;- append(txt,readLines(filetxt,warn = FALSE))\n}"
  },
  {
    "objectID": "posts/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich.html#extract-relevant-info-from-text-files",
    "href": "posts/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich.html#extract-relevant-info-from-text-files",
    "title": "Is Free Pre-K in NYC Favoring the Rich?",
    "section": "\nExtract relevant info from text files\n",
    "text": "Extract relevant info from text files\n\n\nPull out the Zip, seat count and day length of each school. Note the pretty heroic (for me, anyway) regular expression, “pkseattokens.”\"\n\n# find address line which contains zip and seat count\ntxt2&lt;-txt[grep(\"Address:\",txt)]\n# strip chars that will mess up regex\npkseattokens &lt;-\"(Address: )([.[:alnum:]- ()]+),+ ([0-9]{5})([a-zA-Z .()-:]+) ([0-9]{1,4}) (FD|HD|AM|PM|5H)\"\ntxt2&lt;-sub(\"'\",\"\",txt2)\nschools&lt;-as_data_frame(str_match(txt2,pkseattokens))[,c(4,6,7)]\nnames(schools)&lt;-c(\"zip\",\"seats\",\"dayLength\")\n#have to convert from factor to character THEN to integer.  Don't know why\nschools$seats&lt;-as.integer(as.character(schools$seats))\n\n# aggregate seat count by zip code\nsumSeats &lt;- schools %&gt;% \n  group_by(zip) %&gt;% \n  summarise(count = n(), \n            numSeats = sum(seats, na.rm = TRUE))\n  names(sumSeats)&lt;-c(\"zip\",\"schools\",\"numSeats\")\n\nSo we go from this: \n\n\nthen to this:\n\ntxt[1:3]\n## [1] \"    District 1: Full-Day Pre-K Programs                                                                                      You may apply to these programs online, over the phone, or at a Family Welcome Center.\"\n## [2] \"\"                                                                                                                                                                                                                   \n## [3] \"    Bank Street Head Start (01MATK)                                                                                                            Other School Features         2015   2014 Lowest\"\n\nand then to this:\n\ntxt2[1:3]\n## [1] \"    Address: 535 East 5th Street, 10009 (East Village)                               Phone:    212-353-2532                                    Breakfast/Lunch/Snack(s)      40 FD  N/A\"\n## [2] \"    Address: 280 Rivington Street, 10002 (Lower East Side)                           Phone:    212-254-3070                                    Breakfast/Lunch/Snack(s)      40 FD  N/A\"\n## [3] \"    Address: 180 Suffolk Street, 10002 (Chinatown)                                   Phone:    212-982-6650                                    Breakfast/Lunch/Snack(s)      29 FD  N/A\"\n\n…and finally to this:\n\nschools[1:3,]\n## # A tibble: 3 x 3\n##   zip   seats dayLength\n##   &lt;chr&gt; &lt;int&gt; &lt;chr&gt;    \n## 1 10009    40 FD       \n## 2 10002    40 FD       \n## 3 10002    29 FD\n\nMan, I love when the regex works! Magic!"
  },
  {
    "objectID": "posts/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich.html#look-at-some-preliminary-pictures-from-the-pre-k-data",
    "href": "posts/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich.html#look-at-some-preliminary-pictures-from-the-pre-k-data",
    "title": "Is Free Pre-K in NYC Favoring the Rich?",
    "section": "\nLook at some preliminary pictures from the pre-K data\n",
    "text": "Look at some preliminary pictures from the pre-K data\n\n\nNot all the programs are full day. Are there a lot of schools offering shorter programs? We won’t use this data further in our analysis, but lets look at how many seats are full day vs. something else. Full day is the overwhelming majority.\n\n#how do the programs break out in terms of day length?\nsumDayLength&lt;-schools%&gt;%group_by(dayLength)%&gt;%summarise(NumSchools=n(),NumSeats=sum(seats,na.rm=TRUE))\nggplot(sumDayLength,aes(x=dayLength,y=NumSeats)) + geom_col() +\n  scale_y_continuous(labels = scales::comma)\n\n\n\n\nWhere are the most schools? Where are the most seats? We might assume this pictures look the same, and they do.\n\n# some preliminary pictures\nsumSeats %&gt;% transmute(region = zip, value = schools) %&gt;%\n  zip_choropleth(zip_zoom = nyc_zips, \n                 title = \"Number of Schools\")\n\n\n\nsumSeats %&gt;% transmute(region=zip,value=numSeats) %&gt;% \n  zip_choropleth(zip_zoom = nyc_zips,\n                 title = \"Number of Pre-K Seats\")\n## Warning in super$initialize(zip.map, user.df): Your data.frame contains the\n## following regions which are not mappable: 11249, 11376, NA\n## Warning in self$bind(): The following regions were missing and are being\n## set to NA: 10464, 11040, 10280, 10174, 10017, 10119, 11371, 10110, 10271,\n## 11003, 11370, 10171, 10069, 10162, 10177, 10152, 10279, 10115, 10005,\n## 10111, 10112, 10167, 11351, 11359, 11424, 11425, 11451, 10006, 10169,\n## 10103, 10311, 10153, 10154, 10199, 10165, 10168, 10278, 10020, 10173,\n## 10170, 10172, 11005"
  },
  {
    "objectID": "posts/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich.html#create-the-custom-legend.",
    "href": "posts/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich.html#create-the-custom-legend.",
    "title": "Is Free Pre-K in NYC Favoring the Rich?",
    "section": "\nCreate the custom legend.\n",
    "text": "Create the custom legend.\n\n\nTo create the legend we ‘simply’ create a heat map of the 3x3 bins in the map and label the axes appropriately. Then, using ‘cowplot’, shove it into a corner of the map. There are other ways we could use, but they don’t look nearly as nice.\n\n#first create a legend plot\nlegendGoal = melt(matrix(1:9, nrow = 3))\nlg &lt;- ggplot(legendGoal, aes(Var2, Var1, fill = as.factor(value))) + geom_tile()\nlg &lt;- lg + scale_fill_manual(name = \"\", values = bvColors)\nlg &lt;- lg + theme(legend.position = \"none\")\nlg &lt;- lg + theme(axis.title.x = element_text(size = rel(1), color = bvColors[3])) + \n  xlab(\" More Income --&gt;\")\nlg &lt;- lg + theme(axis.title.y = element_text(size = rel(1), color = bvColors[3])) + \n  ylab(\"   More Seats --&gt;\")\nlg &lt;- lg + theme(axis.text = element_blank())\nlg &lt;- lg + theme(line = element_blank())\nlg\n\n\n\n\nAbove we see the legend as a custom rolled heat map. There is no data in it, just a matrix corresponding to the bin indices in the zip code map. We assign colors to match."
  },
  {
    "objectID": "posts/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich.html#put-both-plots-on-a-grid",
    "href": "posts/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich/2018-11-29-is-free-pre-k-in-nyc-favoring-the-rich.html#put-both-plots-on-a-grid",
    "title": "Is Free Pre-K in NYC Favoring the Rich?",
    "section": "\nPut both plots on a grid\n",
    "text": "Put both plots on a grid\n\n\nNow we have the map in the ‘gg’ variable and the legend in the ‘lg’ variable. ‘ggdraw()’ and ‘draw_plot()’ are the ‘cowplot’ functions that let us create the canvas. We tweak the location and size parameters for rendering the legend element until it looks nice inset with the map.\n\n# put the legend together with the map\n# further annotate plot in the ggplot2 environment\n#strip out the ugly legend\ngg&lt;-bvc$render()  + theme(legend.position=\"none\")\nggdraw() + draw_plot(lg,0.2,0.5,width=0.2,height=0.35) + \n  draw_plot(gg)\n\n\n\n\nThis map shows clearly where the low income, well served areas of the city are and that the swanky manhattan zip codes have the fewest free pre-K seats per child."
  },
  {
    "objectID": "posts/2019-02-04-rick-and-morty-palettes/2019-02-04-rick-and-morty-palettes.html",
    "href": "posts/2019-02-04-rick-and-morty-palettes/2019-02-04-rick-and-morty-palettes.html",
    "title": "Rick and Morty Palettes",
    "section": "",
    "text": "This was just a fun morning exercise. Let’s mix multiple images to make a palette of their principal colors using k-means. We’ll also use the totally awesome list-columns concept to put each image’s jpeg data into a data frame of lists that we can map to a function that turns the jpeg data into a list of palette colors in a new data frame.\n\n\nThis more-or-less copies http://www.milanor.net/blog/build-color-palette-from-image-with-paletter/ with the added twist of using multiple images before creating the palette. We’ll also get into the weeds a bit more with dissecting the images. I wanted to see if some cartoon show palettes using this method matched those in the ggsci package. Did the authors use the algorithmic approach I will use here? Will my approach look any better? Don’t know. I decided to use “Rick and Morty” because my kids like it. I would certainly never watch such drivel. I’m a scientist.\n\n\nFor the record, the one pop culture derived palette I really like is the Wes Anderson palette and on CRAN. These are presumably lovingly curated and created, not like the ones created by the stupid robot I use here.\n\n\nThe drawback to using K-means to create palettes from images is that it’s likely that none of the colors created are actually in the image. They just represent the mathematical centers of the clusters of colors.\n\n\nLoad libraries.\n\nlibrary(tidyverse)\nlibrary(jpeg) #import images\nlibrary(scales) #just for for the show_col() function\nlibrary(ggsci) #to compare my palettes to its palettes\nlibrary(ggfortify) #to support kmeans plots\nlibrary(gridExtra) #multiple plots on a page\n\nLoad mulitple images. They are all Google image search thumbnails so the size is the same. This matters since we are combining images. A larger image would have a disproportional weight in our analysis.\n\n\nI first thought that, since I am combining multiple images to get one palette, I needed to tile the images then process. No. We just care about the pixel color values so it really doesn’t matter what position they are in. The most efficient approach is to just chain all the RGB values together. Duh. Still we want to do some work with the individual images so let’s label them.\n\nrm_list&lt;-list()\nfor (n in 1:6){\n  img&lt;-jpeg::readJPEG(paste0(\"img/rm\",n,\".jpg\"))\n  R&lt;-as.vector(img[,,1])\n  G&lt;-as.vector(img[,,2])\n  B&lt;-as.vector(img[,,3])\n  rm_list&lt;-bind_rows(data_frame(img=n,R,G,B),rm_list) %&gt;% \n    arrange(img)\n}\n\nrm_list &lt;- left_join(rm_list,\n                     data_frame(\n                     img = c(1, 2, 3, 4, 5, 6),\n                     name = c(\"Schwifty\",\"Portal\",\"Cable\",\n                     \"Family\", \"Outdoor\", \"Wedding\")\n                     ))\n\n\nShow Me What You Got\n\n\nI chose the images from Google image search to be representative of varying but typical scenes.\n\n\n Cable\n\n\n Family\n\n\n Wedding\n\n\n Outdoor\n\n\n Portal\n\n\n Schwifty\n\n\nFor fun let’s do some density plots of the color values.\n\n#make data tidy first\nrm_tidy &lt;- rm_list %&gt;% gather(\"color\",\"level\",-img,-name)\nggplot(rm_tidy,aes(x=level,fill=color))+\n  geom_density(alpha=0.7) + \n  scale_fill_manual(values=c(\"blue\",\"green\",\"red\")) + \n  theme_void()\n\n\n\n\nWe can see some evidence of bimodality, a preference for very bright and very dark hues. Red is more often cranked to the max, while blue is much more evenly distributed. Perhaps that is typical of the limited palette of cartoons or just a function of the small number of frames I chose.\n\nggplot(rm_tidy,aes(x=level,fill=color))+\n  geom_density(alpha=0.7) + \n  scale_fill_manual(values=c(\"blue\",\"green\",\"red\")) + \n  facet_wrap(~name)+\n  theme_void()\n\n\n\n\nIt’s interesting to compare “Cable” with “Family.” Both images share the same backdrop but “Family” is much darker.\n\n\n\n\nMake the Palettes\n\n\nWhen I was a kid with watercolors I wanted to come up with a name for the filthy color that resulted when I mixed all the colors together. I called it (trigger warning) “Hitler” (but, really, brown). What is the color that results when we average all the RGB values? What named R colors resemble it? It looks to me like it’s between “cornsilk4”\" and “darkkhaki.”\"\n\nblend_color&lt;-rm_list %&gt;% \n  summarise(R=mean(R),G=mean(G),B=mean(B)) %&gt;% \n  rgb()\n\nshow_col(c(\"cornsilk4\",blend_color,\"darkkhaki\"))\n\n\n\n\nLet’s call it “desertkhaki” which, hopefully, is not a trigger word.\n\n\nNow, for the fun part. In the Wes Anderson palette set, each movie get’s a different palette. Let’s make palettes for each of the images, which I chose for their distinctiveness.\n\n\nFor me, the good thing about open source is that I can stand on the shoulders of giants in the community. R also makes very muscular analysis trivally simple. On the other hand, it makes “script kiddies” like me potentially dangerous. I can only describe k-means in the most general terms but can run it in a snap.\n\nnum_colors = 16\npal_schwifty &lt;- rm_list %&gt;% \n  filter(name==\"Schwifty\") %&gt;% \n  select(R,G,B) %&gt;% \n  kmeans(centers = num_colors, iter.max = 30) %&gt;% \n  .$centers %&gt;% \n  rgb()\n\nshow_col(pal_schwifty)\n\n\n\n\nFor data plotting the separation between some of these colors is too small. I think 9 colors will suffice.\n\nnum_colors = 9\npal_schwifty &lt;- rm_list %&gt;% \n  filter(name==\"Schwifty\") %&gt;% \n  select(R,G,B) %&gt;% \n  kmeans(centers = num_colors, iter.max = 30) %&gt;% \n  .$centers %&gt;% \n  as_tibble() %&gt;% \n  {.}\nshow_col(rgb(pal_schwifty))\n\n\n\n\nFor plotting purposes I would like use these colors in order of intensity. Sorting colors is a topic in itself but here we’ll do it quick and simple.\n\npal_schwifty %&gt;% \n  mutate(saturation=rowSums(.[1:3])) %&gt;% \n  arrange(saturation) %&gt;% \n  rgb() %&gt;% \n  show_col()\n\n\n\n\nThat’s about right. Let’s put it all together. Go through all the images to create a series of palettes.\n\n\n#function to turn a table of RGB values to an ordered list of colors\ngen_pal &lt;- function(rgb_table) {\n  num_colors = 9\n  pal &lt;- rgb_table %&gt;%\n  select(R, G, B) %&gt;%\n  kmeans(centers = num_colors, iter.max = 30) %&gt;%\n  .$centers %&gt;%\n  as_tibble() %&gt;%\n  mutate(saturation = rowSums(.[1:3])) %&gt;%\n  arrange(saturation) %&gt;%\n  rgb()\n  return(pal)\n}\n#now make list columns, which are totally awesome, for each palette\npalette_rick&lt;-rm_list %&gt;% \n  group_by(name) %&gt;% \n  select(-img) %&gt;% \n  nest(.key=\"rgb\") %&gt;% \n  transmute(name=name,pal= map(rgb,gen_pal))\npalette_rick\n## # A tibble: 6 x 2\n##   name     pal      \n##   &lt;chr&gt;    &lt;list&gt;   \n## 1 Schwifty &lt;chr [9]&gt;\n## 2 Portal   &lt;chr [9]&gt;\n## 3 Cable    &lt;chr [9]&gt;\n## 4 Family   &lt;chr [9]&gt;\n## 5 Outdoor  &lt;chr [9]&gt;\n## 6 Wedding  &lt;chr [9]&gt;\n#a function to extract the individual palettes, given a name.\n\nextract_pal&lt;-function(palette_list,pal_name){\n  pal&lt;-palette_list %&gt;% filter(name==pal_name) %&gt;% \n    select(pal) %&gt;% \n    unlist() %&gt;% \n    as.vector()\n  return(pal)\n}\nplot_one&lt;-function(pal_name){\n  tmp &lt;- palette_rick %&gt;% unnest() %&gt;% filter(name==pal_name)\n  g&lt;- ggplot(tmp,aes(pal,fill=pal)) + geom_bar() + \n  scale_fill_manual(values=tmp$pal,guide=F) +\n  theme_void()+ggtitle(pal_name)\n  return (g)\n  \n}\n\nlapply(palette_rick$name,plot_one) %&gt;% \n  grid.arrange(grobs=.)\n\n\n\n\nFinally, let’s do what we said we’d do at the beginning, put all these images together and add it to our list column of palettes.\n\nmulti_img_pal &lt;- gen_pal(rm_list)\npalette_rick&lt;-data_frame(name=\"all\",pal=list(multi_img_pal)) %&gt;% bind_rows(palette_rick)\nshow_col(multi_img_pal)\n\n\n\n\nNot too bad. I’m glad something resembling Rick’s hair makes it into the list. Compare it to the ggsci package Rick and Morty palette. Here we see the weaknesses of an algorithmic approach. ggsci is more interesting since it has more color diversity and vividness. I assume they were hand selected. You can see Rick’s hair and Morty’s shirt color.\n\nshow_col(ggsci::pal_rickandmorty()(9))\n\n\n\n\nSince the (rather flimsy) point of this excercise is to make palettes for data graphics, let’s make some plots.\n\n#use the example in help for dplyr::gather\nstocks &lt;- data.frame(\n  time = as.Date('2009-01-01') + 0:9,\n  W = rnorm(10, 0, 1),\n  X = rnorm(10, 0, 1),\n  Y = rnorm(10, 0, 2),\n  Z = rnorm(10, 0, 4)\n)\nstocksm &lt;- stocks %&gt;% gather(stock, price, -time)\n\nggplot(stocksm,aes(time,price,color=stock))+geom_line(size=2)+\n  scale_color_manual(values = multi_img_pal) + theme_minimal()\n\n\n\nggplot(stocksm,aes(time,price,color=stock))+geom_line(size=2) +\n  theme_minimal() +\n  scale_color_manual(values = extract_pal(palette_rick,\"Wedding\"))\n\n Arguably, the perceptual differnces among the colors are less than ideal, even if the colors are pleasing. We might take the additional step of hand-selecting colors from a larger generated palette that are more suitable for plots.\n\n\n\n\nOne more thing…\n\n\nBack to the k-means analysis. When we created these palettes we were really assigning colors to the centers of the clusters of near neigbors in the a 2D space. This is a form of principal components analysis (PCA). Let’s visualize those clusters. The ggplot::autoplot() function makes this trivally easy. While we are at it, let’s crank up the number of colors to 20.\n\nnum_colors = 20\n#assign each pixel to a cluster\nkm &lt;-  rm_list[c(\"R\",\"G\",\"B\")] %&gt;% kmeans(centers = num_colors, iter.max = 30)\nrm_PCA&lt;-prcomp(rm_list[c(\"R\",\"G\",\"B\")])\n\nrm_list &lt;- rm_list %&gt;% mutate(cluster=as.factor(km$cluster))\nautoplot(rm_PCA, x=1,y=2,data = rm_list, colour = \"cluster\",\n         loadings = TRUE, loadings.colour = 'blue',\n         loadings.label = TRUE, loadings.label.size = 10) +\n  scale_color_manual(values=rgb(km$centers),guide=FALSE)+\n  theme_classic()\n\n This is every pixel colored by it’s cluster assignment and plotted. It’s clear that the x-dimension, which happens to explain 74% of the color variance, is luminosity, with darker shades on the right. The other dimension seems to be related to hue.\n\n\nWe can make it clear by plotting the second and third principal component.\n\nrm_list &lt;- rm_list %&gt;% mutate(cluster=as.factor(km$cluster))\nautoplot(rm_PCA, x=2,y=3,data = rm_list, colour = \"cluster\",\n         loadings = TRUE, loadings.colour = 'blue',\n         loadings.label = TRUE, loadings.label.size = 10) +\n  scale_color_manual(values=rgb(km$centers),guide=F)+\n  theme_classic()\n\n\n\n\nNow it’s quite clear that the second and third principal components map to the color space even though this explains only about 25% of the variation in the data.\n\n\nFeel free to get schwifty with these palettes!"
  },
  {
    "objectID": "posts/2019-02-07-where-are-the-libertarians/2019-02-07-where-are-the-libertarians.html#choose-the-questions-to-use",
    "href": "posts/2019-02-07-where-are-the-libertarians/2019-02-07-where-are-the-libertarians.html#choose-the-questions-to-use",
    "title": "Where Are The Libertarians?",
    "section": "\nChoose the Questions to Use\n",
    "text": "Choose the Questions to Use\n\n\nWhile Mr. Drutman’s analysis of the survey did not show many libertarian-leaning voters, I hoped that selecting my own set of questions narrowly focused on the relevant issues might provide more support for my point of view. So, to be honest, I went into this with a preconceived notion of the answer I wanted to get. Beware.\n\n\nOut of the dozens of questions the survey asked, I pulled out those which seemed to go to the separate dimensions of the conservative/liberal spectrum. The questions involved:\n\n\nFiscal Issues\n\n\n\nTrust of the government in Washington\n\n\nAmount of regulation of business by the government\n\n\nImportance of reducing the federal deficit\n\n\nRole of government in economy\n\n\nDesired third party position on economic issues\n\n\n\nSocial Issues\n\n\n\nDifficulty of foreigners to immigrate to US\n\n\nGender Roles “Women belong in the kitchen!”\n\n\nViews about the holy scriptures of own religion, literal truth?\n\n\nOpinion on gay marriage\n\n\nPublic restroom usage of transgender people\n\n\nView on abortion\n\n\nDesired third party position on social and cultural issues"
  },
  {
    "objectID": "posts/2019-02-07-where-are-the-libertarians/2019-02-07-where-are-the-libertarians.html#pull-out-demographic-features",
    "href": "posts/2019-02-07-where-are-the-libertarians/2019-02-07-where-are-the-libertarians.html#pull-out-demographic-features",
    "title": "Where Are The Libertarians?",
    "section": "\nPull Out Demographic Features\n",
    "text": "Pull Out Demographic Features\n\n\nNow we massage the raw data a few ways. First we gather() the data to group the interesting demographic features as separate variables and tidy up all the remaining questions and answers into two variables.\n\nvoter_18&lt;- gather(voter_18_raw,\"question\",\"answer\",\n                  -caseid,\n                  -pid3_2018,\n                  -race_2018,\n                  -gender_2018,\n                  -faminc_new_2018,\n                  -inputstate_2018\n                  ) %&gt;% \n  as_tibble() %&gt;%\n  filter(!is.na(caseid)) %&gt;% \n  filter(!is.na(answer)) %&gt;% \n  distinct()\n\n# labels of the questions we want to keep, with a (f)iscal or (s)ocial tag\nquestions_to_keep &lt;-  read_csv(\n  \"axis_flag,question\\n\n  f,trustgovt_2018\\n\n  s,immi_makedifficult_2018\\n\n  f,tax_goal_federal_2018\\n\n  f,govt_reg_2016\\n\n  s,sexism1_2018\\n\n  s,holy_2018\\n\n  s,gaymar_2016\\n\n  s,abortview3_2016\\n\n  s,third_soc_2018\\n\n  f,third_econ_2018\\n\n  f,gvmt_involment_2016\\n\",trim_ws=T)\n\nvoter_18 &lt;- voter_18 %&gt;% filter(question %in% questions_to_keep$question)\n\nvoter_18 &lt;- voter_18 %&gt;% mutate(answer=as.numeric(answer))\n# make demographic variables factors\nvoter_18 &lt;- voter_18 %&gt;%\n  mutate(caseid =as.character(caseid)) %&gt;% \n  mutate(gender_2018=as.factor(gender_2018)) %&gt;% \n  mutate(race_2018=as.factor(race_2018)) %&gt;% \n  mutate(faminc_new_2018=as.factor(faminc_new_2018)) %&gt;% \n  mutate(pid3_2018=as.factor(pid3_2018)) %&gt;% \n  rename(party_2018=pid3_2018) %&gt;% \n  rename(state_2018=inputstate_2018) %&gt;% \n  rename(income_2018=faminc_new_2018)\n  \n#map state numbers to state abbreviations\nstate_plus &lt;- c(state.abb[1:8],\"DC\",state.abb[9:50])\nvoter_18$state_2018 &lt;- factor(voter_18$state_2018)\nlevels(voter_18$state_2018) &lt;- state_plus\n\n\nlevels(voter_18$gender_2018) &lt;- c(\"Male\",\"Female\")\nlevels(voter_18$race_2018) &lt;- c(\"White\",\"Black\",\"Hispanic\",\n                                \"Asian\",\"Native Amerian\",\"Mixed\",\n                                \"Other\",\"Middle Eastern\")\n\nlevels(voter_18$party_2018) &lt;- c(\"Democrat\",\"Republican\",\"Independent\",\n                                \"Other\",\"Not Sure\")\n#Make human-readable income column\nincome_key&lt;-read_csv(\n  \"Response,Label\\n\n  1, Less than $10\\n\n  2, $10 - $19\\n\n  3,  $20 - $29\\n\n  4,  $30 - $39\\n\n  5,  $40 - $49\\n\n  6,  $50 - $59\\n\n  7,  $60 - $69\\n\n  8,  $70 - $79\\n\n  9,  $80 - $99\\n\n  10,   $100 - $119\\n\n  11,   $120 - $149\\n\n  12,   $150 - $199\\n\n  13,   $200 - $249\\n\n  14,   $250 - $349\\n\n  15,   $350 - $499\\n\n  16,   $500 or more\\n\n  97,   Prefer not to say\\n\"\n  ,col_types = \"ff\",trim_ws = TRUE)\n\nvoter_18 &lt;- voter_18 %&gt;% mutate(income_2018_000=income_2018)\nlevels(voter_18$income_2018_000)&lt;-levels(income_key$Label)\n\n# now make income_2018 continuous again, keeping income_2018_000 as a factor\n# for labeling\n# \"Prefer not to say\" (coded as 97) is set to NA.   \nvoter_18 &lt;- voter_18 %&gt;% mutate(income_2018=ifelse(income_2018==97,NA,income_2018)) %&gt;%\n                    mutate(income_2018=as.numeric(income_2018))\nvoter_18[1:10,]\n## # A tibble: 10 x 9\n##    caseid gender_2018 race_2018 income_2018 party_2018 state_2018 question\n##    &lt;chr&gt;  &lt;fct&gt;       &lt;fct&gt;           &lt;dbl&gt; &lt;fct&gt;      &lt;fct&gt;      &lt;chr&gt;   \n##  1 38248~ Female      Hispanic            7 Democrat   CA         trustgo~\n##  2 38216~ Female      White               8 Republican AZ         trustgo~\n##  3 38216~ Male        White               6 Independe~ WI         trustgo~\n##  4 38233~ Male        White               7 Republican TX         trustgo~\n##  5 38248~ Female      White               5 Democrat   CA         trustgo~\n##  6 38329~ Male        White              NA Republican WI         trustgo~\n##  7 38222~ Female      White               3 Democrat   VT         trustgo~\n##  8 38233~ Female      White              12 Independe~ FL         trustgo~\n##  9 38226~ Female      White               9 Democrat   AZ         trustgo~\n## 10 38216~ Female      White               6 Independe~ NE         trustgo~\n## # ... with 2 more variables: answer &lt;dbl&gt;, income_2018_000 &lt;fct&gt;\n# We did a lot of work.  Save it.\nsave(voter_18,file=\"data/voter_18.rdata\")\n# free up 30mb of memory\nrm(voter_18_raw)\n\nLook at some of the demographics.\n\ndemographics &lt;- voter_18 %&gt;% \n  distinct(caseid,.keep_all = TRUE) %&gt;% \n  select(-question,-answer)\n\ndemographics %&gt;% group_by(gender_2018) %&gt;%\n  summarise(count=n()) %&gt;% kable()\n\n\n\n\n\ngender_2018\n\n\ncount\n\n\n\n\n\n\nMale\n\n\n2762\n\n\n\n\nFemale\n\n\n3239\n\n\n\n\n\ndemographics %&gt;%\n  ggplot(aes(race_2018))+geom_bar()+coord_flip() + \n  labs(caption = \"Source: VoterStudyGroup.org\")"
  },
  {
    "objectID": "posts/2019-02-07-where-are-the-libertarians/2019-02-07-where-are-the-libertarians.html#rescale-answers-for-consistency",
    "href": "posts/2019-02-07-where-are-the-libertarians/2019-02-07-where-are-the-libertarians.html#rescale-answers-for-consistency",
    "title": "Where Are The Libertarians?",
    "section": "\nRescale Answers for Consistency\n",
    "text": "Rescale Answers for Consistency\n\n\nThe final step in massaging the data is to rescale all the question answers to between one and minus one, interpreted as liberal to conservative, respectively, in two dimensions. “Don’t know” (usually coded as 8) is treated as neutral (zero). If the question is “fiscal”, set “social” to NA and vice versa.\n\n#add two new columns to hold scaled answers.\nvoter_18_scaled&lt;-voter_18 %&gt;% mutate(fiscal=NA,social=NA)\n# -1 is fiscal liberal\nvoter_18_scaled &lt;- voter_18_scaled %&gt;% \n  mutate(fiscal=ifelse(question==\"trustgovt_2018\",-(answer-2),fiscal))\n\n# -1 is social liberal\nvoter_18_scaled &lt;- voter_18_scaled %&gt;%\n  mutate(social=ifelse(question==\"immi_makedifficult_2018\",(answer-3)*0.5,social))\nvoter_18_scaled &lt;- voter_18_scaled %&gt;%\n  mutate(social=ifelse(question==\"immi_makedifficult_2018\",\n                       ifelse(answer==8,0,social),social))\n\n# -1 is fiscal liberal\nvoter_18_scaled &lt;- voter_18_scaled %&gt;%\n  mutate(fiscal=ifelse(question==\"tax_goal_federal_2018\",(answer-2.5)*-(2/3),fiscal))\n\n# -1 is fiscal liberal\nvoter_18_scaled &lt;- voter_18_scaled %&gt;%\n  mutate(fiscal=ifelse(question==\"govt_reg_2016\",-(answer-2),fiscal))\nvoter_18_scaled &lt;- voter_18_scaled %&gt;%\n  mutate(fiscal=ifelse(question==\"govt_reg_2016\",\n                       ifelse(answer==8,0,fiscal),fiscal))\n\n# -1 is social liberal\nvoter_18_scaled &lt;- voter_18_scaled %&gt;%\n  mutate(social=ifelse(question==\"sexism1_2018\",(answer-2.5)*-(2/3),social))\n\n# -1 is social liberal\nvoter_18_scaled &lt;- voter_18_scaled %&gt;%\n  mutate(social=ifelse(question==\"holy_2018\",-(answer-2),social))\n\n# -1 is social liberal\nvoter_18_scaled &lt;- voter_18_scaled %&gt;%\n  mutate(social=ifelse(question==\"gaymar_2016\",(answer-1.5)*2,social))\nvoter_18_scaled &lt;- voter_18_scaled %&gt;%\n  mutate(social=ifelse(question==\"gaymar_2016\",\n                       ifelse(answer==8,0,social),social))\n\n# -1 is social liberal\nvoter_18_scaled &lt;- voter_18_scaled %&gt;%\n  mutate(social=ifelse(question==\"view_transgender_2016\",(answer-1.5)*2,social))\nvoter_18_scaled &lt;- voter_18_scaled %&gt;%\n  mutate(social=ifelse(question==\"view_transgender_2016\",\n                       ifelse(answer==8,0,social),social))\n\n# -1 is social liberal\nvoter_18_scaled &lt;- voter_18_scaled %&gt;%\n  mutate(social=ifelse(question==\"abortview3_2016\",(answer-2),social))\nvoter_18_scaled &lt;- voter_18_scaled %&gt;%\n  mutate(social=ifelse(question==\"abortview3_2016\",\n                       ifelse(answer==8,0,social),social))\n\n# -1 is social liberal\nvoter_18_scaled &lt;- voter_18_scaled %&gt;%\n  mutate(social=ifelse(question==\"third_soc_2018\",(answer-3)*0.5,social))\n\n# -1 is fiscal liberal\nvoter_18_scaled &lt;- voter_18_scaled %&gt;%\n  mutate(fiscal=ifelse(question==\"third_econ_2018\",(answer-3)*0.5,fiscal))\n\n# -1 is fiscal liberal\nvoter_18_scaled &lt;- voter_18_scaled %&gt;%\n  mutate(fiscal=ifelse(question==\"gvmt_involment_2016\",(answer-1),fiscal))\nvoter_18_scaled &lt;- voter_18_scaled %&gt;%\n  mutate(fiscal=ifelse(question==\"gvmt_involment_2016\",\n                       ifelse(answer==8,0,fiscal),fiscal))\n\n# We did a lot of work.  Save it.\nsave(voter_18_scaled,file=\"data/voter_18_scaled.rdata\")\n\nNow we have values that we can aggregate for each question. They are all normalized and given equal weight. Should each question be given equal weight? I don’t know, but now we can compute average scores for each caseid (each one is one voter) . We also add the demographic features to each observation. So now every caseid in the survey is assigned a separate fiscal and social temperament score.\n\nscores &lt;- voter_18_scaled %&gt;% \n  group_by(caseid) %&gt;% \n  summarise(social=mean(social,na.rm = T),fiscal=mean(fiscal,na.rm = T)) %&gt;% \n  left_join(demographics)   #Add demographics to scores\n\nLet’s start off at the highest level. What are the mean values for each dimension?\n\nmean_social &lt;- mean(scores$social,na.rm = T)\nmean_fiscal &lt;- mean(scores$fiscal,na.rm = T)\nprint(paste(\"Mean Fiscal=\",round(mean_fiscal,2)))\n## [1] \"Mean Fiscal= 0.06\"\nprint(paste(\"Mean Social=\",round(mean_social,2)))\n## [1] \"Mean Social= -0.16\"\n\nWell that is an encouraging start. The signs are in the libertarian quadrant, anyway, but are they statistically significant? Specifically, can we reject the hypothesis that the true mean is greater than zero for social, and less than zero for fiscal?\n\nt_s &lt;-t.test(scores$social,mu=0,conf.level = 0.95,alternative=\"greater\") %&gt;% broom::tidy()\nt_f &lt;-t.test(scores$fiscal,mu=0,conf.level = 0.95,alternative=\"less\") %&gt;% broom::tidy()\nt_both&lt;-bind_cols(Dimension=c(\"Social\",\"Fiscal\"),bind_rows(t_s,t_f)) %&gt;% \n  select(Dimension,estimate,statistic,conf99.low=conf.low,conf99.high=conf.high)\nt_both\n## # A tibble: 2 x 5\n##   Dimension estimate statistic conf99.low conf99.high\n##   &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;\n## 1 Social     -0.157      -24.6     -0.167    Inf     \n## 2 Fiscal      0.0585      11.4   -Inf          0.0669\n\nWith such a large sample size we can be pretty confident that the true mean is close to the sample mean and therefore leans libertarian. Alas, that is not enough to form an opinion. The magnitudes are still very small and a slight relative shift in the aggregate may not support my hypothesis that most people have a libertarian bias when you break down the issues. Further, we haven’t even touched on the survey methodology. It is an online survey and therefore means the respondents have computers and are facile with internet access. That population is closer and closer to “everyone” with each passing day but is still not universal.\n\n\nWith our data all cleaned up, let’s look at some pictures!\n\ngg &lt;- ggplot(scores,aes(fiscal,social)) + geom_point()\ngg &lt;- gg +  geom_jitter(width=0.05,height=0.05)\ngg &lt;- gg + geom_hline(yintercept = 0,color=\"red\")\ngg &lt;- gg + geom_vline(xintercept = 0,color=\"red\")\ngg &lt;- gg + annotate(\"text\",label=c(\"Libertarian\"),x=0.9,y=-0.9,color=\"red\")\ngg &lt;- gg + labs(title=\"Separation of Social and Fiscal Values\",\n                y = \"Social Score (Lower=More Liberal)\",\n                x = \"Fiscal Score (Lower=More Liberal)\",\n                caption = \"Source: VoterStudyGroup.org\")\ngg &lt;- gg + annotate(\"text\",x=-0.7,y=1.0,color=\"red\",\n                    label=paste(\"Mean Fiscal=\",round(mean_fiscal,2),\n                                \"Mean Social=\",round(mean_social,2)))\ngg &lt;- gg + geom_smooth(method=\"lm\")\ngg\n\n\n\n\nThe first thing to note is the values are all over the chart. We’ve added some random “jitter” noise to the position of each point with geom_jitter(). Otherwise, many of the points would overlap and obscure the density of the points. Even so, careful scrutiny of of the standard error range around the regression line shows that a huge number of points lie very close to the line.\n\n\nSadly, for a libertarian, the scores tend to line up close to the 45 degree axis, which means people who are more socially conservative are more likely to be fiscally conservative as well. The libertarian quadrant is the lower right, which is more sparsely populated.\n\nlm(scores$social~scores$fiscal) %&gt;% broom::tidy()\n## # A tibble: 2 x 5\n##   term          estimate std.error statistic   p.value\n##   &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)     -0.200   0.00518     -38.5 5.39e-290\n## 2 scores$fiscal    0.732   0.0129       56.7 0.\n\nLet’s count voter incidence in each quadrant.\n\n#call zero scores \"Neutral\"\nscores &lt;- scores %&gt;% \n  mutate(fiscal_label=cut(scores$fiscal,c(-1,-0.0001,0.0001,1),\n                      labels=c(\"Liberal\",\"Neutral\",\"Conservative\"))) %&gt;% \n  mutate(social_label=cut(scores$social,c(-1,-0.01,0.01,1),\n                      labels=c(\"Liberal\",\"Neutral\",\"Conservative\")))\n\nxtabs(~fiscal_label+social_label,scores) %&gt;% \n  as_tibble() %&gt;% \n  arrange(desc(n)) %&gt;% \n  filter(fiscal_label != \"Neutral\",social_label != \"Neutral\")\n## # A tibble: 4 x 3\n##   fiscal_label social_label     n\n##   &lt;chr&gt;        &lt;chr&gt;        &lt;int&gt;\n## 1 Liberal      Liberal       1903\n## 2 Conservative Conservative  1745\n## 3 Conservative Liberal       1046\n## 4 Liberal      Conservative   387\n\nThe largest quadrant is Liberal/Liberal followed by Conservative/Conservative. Leaving out the neutral axes, the libertarian quadrant (liberal social, conservative fiscal) is third with a respectable number of respondents. This is about 18% of the sample, far more than the 4% Mr. Drutman found. The liberal fiscal, conservative social quadrant, which is populist I suppose, includes the fewest voters.\n\n\nThis is suggestive of traditional party platforms so how does this look broken out by party?\n\ngg &lt;-ggplot(scores,aes(fiscal,social,color=party_2018))+geom_point()\ngg &lt;- gg +  geom_jitter(width=0.05,height=0.05)\ngg &lt;- gg + geom_hline(yintercept = 0)\ngg &lt;- gg + geom_vline(xintercept = 0)\ngg &lt;- gg + annotate(\"text\",label=c(\"Libertarian\"),x=0.9,y=-0.9)\ngg &lt;- gg + scale_color_manual(values=c(Republican='#e41a1c',\n                                       Democrat='#377eb8',\n                                       Independent='#4daf4a',\n                                       Other='#984ea3',\n                                       `Not Sure`='#ff7f00'))\ngg &lt;- gg + labs(title=\"Party Lines Align With Temperament\",\n                y = \"Social Score (Lower=More Liberal)\",\n                x = \"Fiscal Score (Lower=More Liberal)\",\n                caption = \"Source: VoterStudyGroup.org\")\ngg\n\n\n\n\nThere is a clear bifurcation around party, which is exactly what we’d expect.\n\n\nThe survey respondents are overwhelmingly white. What does the plot look like if we remove them from data set?\n\ngg &lt;- scores %&gt;% filter(race_2018 != \"White\") %&gt;% \n  ggplot(aes(fiscal,social,color=race_2018))+geom_point()\ngg &lt;- gg +  geom_jitter(width=0.05,height=0.05)\ngg &lt;- gg + geom_hline(yintercept = 0)\ngg &lt;- gg + geom_vline(xintercept = 0)\ngg &lt;- gg + annotate(\"text\",label=c(\"Libertarian\"),x=0.9,y=-0.9)\ngg &lt;- gg + labs(title=\"Minorities Are Not Too Different from Whites\",\n                y = \"Social Score (Lower=More Liberal)\",\n                x = \"Fiscal Score (Lower=More Liberal)\",\n                caption = \"Source: VoterStudyGroup.org\")\ngg\n\n\n\n\nThe sub sample above looks very similar to the whole data set. Black voters do skew more to the Liberal/Liberal side but Hispanic voters do not.\n\n\nLet’s meet some individuals. Who are the folks who show strong libertarian sentiments (greater than 0.5 social, less than -0.5 fiscal), all nineteen of them?\n\nscores %&gt;% filter(fiscal &lt; (0.5),social &gt; (-0.5)) %&gt;% select(gender_2018,race_2018,party_2018,income_2018_000,state_2018)\n## # A tibble: 2,984 x 5\n##    gender_2018 race_2018 party_2018  income_2018_000 state_2018\n##    &lt;fct&gt;       &lt;fct&gt;     &lt;fct&gt;       &lt;fct&gt;           &lt;fct&gt;     \n##  1 Male        White     Independent $150 - $199     MI        \n##  2 Male        White     Independent $60 - $69       SD        \n##  3 Male        White     Republican  $100 - $119     OK        \n##  4 Female      White     Republican  $10 - $19       WI        \n##  5 Male        White     Republican  $30 - $39       CA        \n##  6 Female      White     Republican  $40 - $49       WA        \n##  7 Female      White     Republican  $10 - $19       IN        \n##  8 Female      Hispanic  Democrat    $50 - $59       NY        \n##  9 Female      White     Independent $30 - $39       MD        \n## 10 Female      White     Independent $120 - $149     MA        \n## # ... with 2,974 more rows\n\nThese folks are almost all white, but our set is a tiny sub sample so I doubt any generalizations are significant. There is only one Democrat in the bunch. They are not rich and they’re spread around the country. They are men and women.\n\n\nWe have a number of additional demographic variables but let’s just look at one more of them. How do scores look conditioned on income?\n\ngg &lt;- scores %&gt;% filter(!is.na(income_2018)) %&gt;%\n  ggplot(aes(income_2018_000,fiscal,group=income_2018)) + geom_boxplot()\ngg &lt;- gg + coord_flip() + theme(axis.text.x = element_text(angle=-90))\ngg &lt;- gg + geom_hline(yintercept = 0,color=\"red\")\ngg &lt;- gg + labs(title=\"Higher Income Does  Not Make a Fiscal Conservative\",\n                x = \"Annual Income ($000)\",\n                y = \"Fiscal Score (Lower=More Liberal)\",\n                caption = \"Source: VoterStudyGroup.org\")\ngg\n\n\n\n\nSurprisingly, to me, there is no trend to prefer less government as income rises. The desire for government involvement in the economy is close to neutral across all income cohorts. Note, I did not include any tax questions for this measure. People are happy to favor higher taxes on anybody who makes more money than they do.\n\ngg &lt;- scores %&gt;% filter(!is.na(income_2018)) %&gt;%\n  ggplot(aes(income_2018_000,social,group=income_2018))+ geom_boxplot()\n\ngg &lt;- gg + coord_flip() + theme(axis.text.x = element_text(angle=-90))\ngg &lt;- gg + geom_hline(yintercept = 0,color=\"red\")\ngg &lt;- gg + labs(title=\"Higher Income Does Make One More Socially Liberal\",\n                x = \"Annual Income ($000)\",\n                y = \"Social Score (Lower=More Liberal)\",\n                caption = \"Source: VoterStudyGroup.org\")\ngg\n\n\n\n\nThere is some association with more socially liberal views as income rises. The richer you are, the more tolerant you are of other’s lifestyles, I guess. During the 2016 election there was some questioning around why poor people (mainly rural whites) voted “against their economic interest.” This suggests that voting WITH their conservative social interests was more important (I am not saying that our current president embodies conservative social values). Most pundits put a racial angle on this. In all income cohorts the median voter is at least a shade liberal on social issues."
  },
  {
    "objectID": "posts/2019-02-07-where-are-the-libertarians/2019-02-07-where-are-the-libertarians.html#how-much-does-location-matter",
    "href": "posts/2019-02-07-where-are-the-libertarians/2019-02-07-where-are-the-libertarians.html#how-much-does-location-matter",
    "title": "Where Are The Libertarians?",
    "section": "\nHow Much Does Location Matter?\n",
    "text": "How Much Does Location Matter?\n\n\nLet’s look at average scores by state. To remind us of the influence that larger states have on the overall numbers we’ll grab population data from the Census Bureau. There are a number of R packages to access the census API but those are more than we need and require an API key. Here, we’ll just grab a summary CSV file from the web site.\n\n# download population summary from census bureau\nstate_pop_raw&lt;-read_csv(\"https://www2.census.gov/programs-surveys/popest/datasets/2010-2018/national/totals/nst-est2018-alldata.csv\")\n## Parsed with column specification:\n## cols(\n##   .default = col_double(),\n##   SUMLEV = col_character(),\n##   REGION = col_character(),\n##   DIVISION = col_character(),\n##   STATE = col_character(),\n##   NAME = col_character()\n## )\n## See spec(...) for full column specifications.\nsave(state_pop_raw,file=\"data/state_pop_raw.rdata\")\nload(\"data/state_pop_raw.rdata\")\n# filter to keep only state level data and add abbreviations\n# manually insert District of Columbia as a state\nstate_pop &lt;- state_pop_raw %&gt;% \n  transmute(state=NAME,population_2018=POPESTIMATE2018) %&gt;%\n  filter(state %in% c(state.name,\"District of Columbia\")) %&gt;%\n  bind_cols(state_2018=as_factor(state_plus))\n\ngg &lt;- scores %&gt;% group_by(state_2018) %&gt;% \n  summarize(fiscal=mean(fiscal,na.rm = T),social=mean(social,na.rm = T)) %&gt;%\n  left_join(state_pop, by = \"state_2018\") %&gt;% \n  ggplot(aes(fiscal,social)) + geom_point(aes(color=population_2018,\n                                              size=population_2018))\ngg &lt;- gg + ggrepel::geom_text_repel(aes(label=state_2018))\ngg &lt;- gg + scale_size(trans=\"log10\",\n                      labels=c(\"0\",\"1 mm\",\"3 mm\",\"10 mm\",\"30 mm\",\"More\"))\ngg &lt;- gg + scale_color_gradient(trans=\"log10\",\n                                labels=c(\"0\",\"1 mm\",\"3 mm\",\"10 mm\",\"30 mm\",\"More\"))\ngg &lt;- gg + geom_hline(yintercept = 0)\ngg &lt;- gg + geom_vline(xintercept = 0)\ngg &lt;- gg + annotate(\"text\",label=c(\"Libertarian\"),x=0.15,y=-0.4)\ngg &lt;- gg + labs(title=\"Separation of Social and Fiscal Values\",\n                y = \"Social Score (Lower=More Liberal)\",\n                x = \"Fiscal Score (Lower=More Liberal)\",\n                caption = \"Source: VoterStudyGroup.org\")\n\ngg\n\n\n\n\nIf I had created this chart first I might have been excited. It shows that the average voter in most states is in the libertarian quadrant. That is NOT the same thing as saying most voters in the “libertarian” states are libertarian. We already showed that the vast majority of voters fall outside the libertarian quadrant. Still, there are some interesting things to note. The fiscal sentiments of New Hampshire voters are far different than their Vermont neighbors. I don’t see Bernie Sanders sporting this license plate:\n\n\n\n\nLive Free or Die\n\n\n\nBy the way, I wish I knew how to get color and size combined into one legend."
  },
  {
    "objectID": "posts/2019-02-07-where-are-the-libertarians/2019-02-07-where-are-the-libertarians.html#my-last-attempt-at-validation",
    "href": "posts/2019-02-07-where-are-the-libertarians/2019-02-07-where-are-the-libertarians.html#my-last-attempt-at-validation",
    "title": "Where Are The Libertarians?",
    "section": "\nMy Last Attempt at Validation\n",
    "text": "My Last Attempt at Validation\n\n\nI went through the YouGov.com survey and picked out the questions I feel are relevant, a highly subjective exercise. Even so,the results do not support my belief that maybe a plurality of people have libertarian sensibilities. But there were hints that gave me some hope.\n\n\nFirst, there is a clear yearning for a choice beyond the existing parties as this question shows:\n\n\nIn your view, do the Republican and Democratic parties do an adequate job of representing the American people, or do they do such a poor job that a third major party is needed?\n\n\n\n\n\n\nCount\n\n\nAnswer\n\n\n\n\n\n\n1,851\n\n\nDo adequate job\n\n\n\n\n4,036\n\n\nThird party is needed\n\n\n\n\n\n\nThe fact that most people want another choice tells us nothing about what that choice is. Another question does seem to suggest libertarian economic sentiment in excess of what the number of Republicans might indicate:\n\n\nIn general, do you think there is too much or too little regulation of business by the government?\n\n\n\n\n\n\nCount\n\n\nAnswer\n\n\n\n\n\n\n3,473\n\n\nToo much\n\n\n\n\n1,628\n\n\nAbout the right amount\n\n\n\n\n1,999\n\n\nToo little\n\n\n\n\n871\n\n\nDon’t know\n\n\n\n\n\n\nFinally, there are two questions in the survey that go explicitly to the separation of social and fiscal values.\n\n\n1. If you were to vote for a new third party, where would you like it to stand on social and cultural issues—like abortion and same-sex marriage?\n\n\n2. If you were to vote for a new third party, where would you like it to stand on economic issues—like how much the government spends and how many services it provides?\n\n\nThe range of answers for both is:\n\n\n\n\n\n\nScore\n\n\nAnswer\n\n\n\n\n\n\n1.0\n\n\nMore liberal than the Democratic Party\n\n\n\n\n0.5\n\n\nAbout where the Democratic Party is now\n\n\n\n\n0.0\n\n\nIn between the Democratic Party and the Republican Party\n\n\n\n\n-0.5\n\n\nAbout where the Republican Party is now\n\n\n\n\n-1.0\n\n\nMore conservative than the Republican Party\n\n\n\n\n\n\nLet’s re-do the scatter based on the answers to just those two questions. Since we are using only two questions with possible values of only 1,0 and minus 1, there are many more respondents than possible values. Again we add some random jitter to make the density clear. Every dot within each square is actually the same value. The result is a visual cross tab. I quite like the effect.\n\nscores_narrow &lt;- voter_18_scaled %&gt;% \n  filter(str_detect(question,\"third_\")) %&gt;%  \n  group_by(caseid) %&gt;% \n  summarise(social=mean(social,na.rm = T),fiscal=mean(fiscal,na.rm = T)) %&gt;% \n  left_join(demographics)\n\ngg &lt;- ggplot(scores_narrow,aes(fiscal,social,color=party_2018))+geom_point() + geom_jitter()\ngg &lt;- gg + geom_hline(yintercept = 0)\ngg &lt;- gg + geom_vline(xintercept = 0)\ngg &lt;- gg + labs(title=\"What Kind of Third Party Would Voters Prefer?\",\n                y = \"Social Score (Lower=More Liberal)\",\n                x = \"Fiscal Score (Lower=More Liberal)\",\n                caption = \"Source: VoterStudyGroup.org\")\ngg &lt;- gg + annotate(\"text\",label=c(\"Libertarian\"),x=0.9,y=-0.9,fontface=\"bold\")\ngg &lt;- gg + annotate(\"text\",label=c(\"Populist?\"),x=-0.9,y=0.9,fontface=\"bold\")\ngg &lt;- gg + annotate(\"text\",label=c(\"Left of Democrats\"),x=-0.9,y=-0.9,fontface=\"bold\")\ngg &lt;- gg + annotate(\"text\",\n                    label=c(\"Right of Republicans\"),\n                    x=0.9,y=0.9,\n                    fontface=\"bold\")\ngg &lt;- gg + scale_color_manual(values=c(Republican='#e41a1c',\n                                       Democrat='#377eb8',\n                                       Independent='#4daf4a',\n                                       Other='#984ea3',\n                                       `Not Sure`='#ff7f00'))\n\ngg\n\n\n\n\nWhat I don’t like is the result. Contrary to my pre-conceived notion, it’s clear the American electorate is not crypto-libertarian. Rather, voters want a third party that is highly centrist or highly polarized along traditional liberal/conservative lines. This makes it unlikely that any single third party could be successful at the ballot box. Rather, both an extreme left-wing and an extreme right wing party could take votes away from the traditional parties. The Republican party is more hollowed out in its relative middle than the Democrats.\n\n\nCould Howard Schulz be something of a spoiler from the center? Possibly. There are a large number of voters who would like an alternative that is less intrusive than the Democrats on economic issues and less intrusive than the Republicans on moral issues. I disagree with the Times’ assessment that, since there are so few absolute libertarians, Schulz will not find a base. As we see, there are many people who lean toward the center and away from the extremes within their parties, even if they are not libertarian, per se. But, far too many people are happy with the status quo or would like their party more on the left or right to make this likely as we see below.\n\ntmp &lt;-scores_narrow %&gt;% \n  mutate(social_direction=cut(abs(social),breaks=c(-0.1,0.25,1.1),\n                    labels=c(\"To the Center\",\n                             \"Status Quo or More Extreme\"))) %&gt;% \n  mutate(fiscal_direction=cut(abs(fiscal),breaks=c(-0.1,0.25,1.1),\n                    labels=c(\"To the Center\",\n                             \"Status Quo or More Extreme\")))\n\n\n\nxtabs(~social_direction+fiscal_direction,tmp) %&gt;% as_tibble() %&gt;% kable()\n\n\n\n\n\nsocial_direction\n\n\nfiscal_direction\n\n\nn\n\n\n\n\n\n\nTo the Center\n\n\nTo the Center\n\n\n1211\n\n\n\n\nStatus Quo or More Extreme\n\n\nTo the Center\n\n\n1116\n\n\n\n\nTo the Center\n\n\nStatus Quo or More Extreme\n\n\n496\n\n\n\n\nStatus Quo or More Extreme\n\n\nStatus Quo or More Extreme\n\n\n3037"
  },
  {
    "objectID": "posts/2019-02-07-where-are-the-libertarians/2019-02-07-where-are-the-libertarians.html#conclusion",
    "href": "posts/2019-02-07-where-are-the-libertarians/2019-02-07-where-are-the-libertarians.html#conclusion",
    "title": "Where Are The Libertarians?",
    "section": "\nConclusion\n",
    "text": "Conclusion\n\n\nI started this exercise hoping to find some support for my personal views among the broader electorate. Sadly, I didn’t find much. The strongest statement I can make is there is a slight bias among both Republicans and Democrats for more centrist policies. But the fun of data science is finding things you didn’t expect and in validating or refuting hunches and feelings with good science. I know something today I didn’t know yesterday so I’ll call it a win!\n\n\nUPDATE 2/8/2019: Based on feedback, I changed party colors and left/right positions to those most Americans are accomstomed to.\n\nsessionInfo()\n\n## R version 3.5.1 (2018-07-02)\n## Platform: x86_64-w64-mingw32/x64 (64-bit)\n## Running under: Windows 10 x64 (build 17134)\n## \n## Matrix products: default\n## \n## locale:\n## [1] LC_COLLATE=English_United States.1252 \n## [2] LC_CTYPE=English_United States.1252   \n## [3] LC_MONETARY=English_United States.1252\n## [4] LC_NUMERIC=C                          \n## [5] LC_TIME=English_United States.1252    \n## \n## attached base packages:\n## [1] stats     graphics  grDevices utils     datasets  methods   base     \n## \n## other attached packages:\n##  [1] bindrcpp_0.2.2  knitr_1.21      forcats_0.3.0   stringr_1.3.1  \n##  [5] dplyr_0.7.8     purrr_0.3.0     readr_1.3.1     tidyr_0.8.2    \n##  [9] tibble_2.0.1    ggplot2_3.1.0   tidyverse_1.2.1\n## \n## loaded via a namespace (and not attached):\n##  [1] tidyselect_0.2.5 xfun_0.4         haven_2.0.0      lattice_0.20-35 \n##  [5] colorspace_1.4-0 generics_0.0.2   htmltools_0.3.6  yaml_2.2.0      \n##  [9] utf8_1.1.4       rlang_0.3.1      pillar_1.3.1     glue_1.3.0      \n## [13] withr_2.1.2      modelr_0.1.2     readxl_1.2.0     bindr_0.1.1     \n## [17] plyr_1.8.4       munsell_0.5.0    blogdown_0.10    gtable_0.2.0    \n## [21] cellranger_1.1.0 rvest_0.3.2      evaluate_0.12    labeling_0.3    \n## [25] curl_3.3         fansi_0.4.0      highr_0.7        broom_0.5.1     \n## [29] Rcpp_1.0.0       scales_1.0.0     backports_1.1.3  jsonlite_1.6    \n## [33] hms_0.4.2        digest_0.6.18    stringi_1.2.4    ggrepel_0.8.0   \n## [37] bookdown_0.9     grid_3.5.1       cli_1.0.1        tools_3.5.1     \n## [41] magrittr_1.5     lazyeval_0.2.1   crayon_1.3.4     pkgconfig_2.0.2 \n## [45] xml2_1.2.0       lubridate_1.7.4  assertthat_0.2.0 rmarkdown_1.11  \n## [49] httr_1.4.0       rstudioapi_0.9.0 R6_2.3.0         nlme_3.1-137    \n## [53] compiler_3.5.1"
  },
  {
    "objectID": "posts/2019-04-16-solving-the-letterboxed-puzzle-in-the-new-york-times/2019-04-16-solving-the-letterboxed-puzzle-in-the-new-york-times.html",
    "href": "posts/2019-04-16-solving-the-letterboxed-puzzle-in-the-new-york-times/2019-04-16-solving-the-letterboxed-puzzle-in-the-new-york-times.html",
    "title": "Solving the Letterboxed Puzzle in the New York Times",
    "section": "",
    "text": "What is the difference between “computer programming” and “data science?” To someone not invovled in either they look much the same. Most data scientists are also coders, though they don’t need to be. Data scientists (especially amateurs like me) don’t need to be concerned with pointers, stacks, heaps, recursion, etc., but this is not a data science post.\n\n\nFor this post, I go back to my roots in the 1980s as an amateur computer scientist to solve a new New York Times puzzle called “Letterboxed.” In particular I employ recursion to build a tree of possible solutions. This exercise reminded me how languages like R allow such easy plug-ins to high-powered algorithms written by “real” computer scientists in “real” languages like C++. Data scientists stand on the shoulders of giants who wrote the low-level code.\n\n\nI’ll confess, I don’t like playing games and doing puzzles much. I also take the fun out of it for other people. When someone gave my kids “Cards Against Humanity” as a gift, I went through the deck and removed all the really filthy cards (the kids were relieved to see I left in plenty of poop references). When I see a puzzle I immediately think about an algorithm to play or solve it.\n\n\n\n\nSample board from The Times\n\n\n\nIn “Letterboxed” the object is to string words together that use all the letters in the square using as few words as possible by tracing the spelling of each word in the square. You must start each new word with the last letter of the previous word and you may not use any consecutive letters that lie on the same side of the square. In the example above, “NIL” and “LAP” would not be permitted. “TORN” followed by “NEAR” would be fine.\n\n\nToday we will forsake the usual data science workflow of: ask a question, source data, clean data, explore data and, finally, analyze data. Those proceed in a linear (in practice, circular) fashion but here we’ll go over the functions that do specific subroutines to generate and solve these puzzles.\n\n\nThe steps we’ll take are\n\n\n\nGenerate the puzzle with random letters.\n\n\nDraw the board.\n\n\nSolve the puzzle.\n\n\nPrint the answer that solves the puzzle in the fewest words.\n\n\n\nGenerating the puzzle is the easy part.\n\n\nThe first task is to generate the puzzle with random letters. It would be cruel to place no requirement to use vowels so we also specify a minimum number of vowels. We sample the required number of consonants and vowels and assign them to each segment of the polygon. The default is four sides with two consonants and one vowel per side.\n\n\nJust to be cute, let’s write the function so we can optionally expand the geometry of the puzzle to an arbitrary number of sides and number of letters per side, not just a square as we see in The Times.\n\n\nIf you are playing along at home, delete the set.seed() line in the code below after you have established you get the same results I do or you will get the same puzzle every time you call generate_puzzle.\n\n# letterboxed game\nlibrary(tidyverse)\nlibrary(wfindr)\nlibrary(gtools)\n\nsides &lt;- 4\nletters_per_side &lt;- 3\nvowels &lt;- c(\"a\",\"e\",\"i\",\"o\",\"u\")\nconsonants &lt;- letters[!(letters %in% vowels)]\n\n# scrabble dictionary from wfinder. You can subsitute any list\n# you desire, in any language.\nword_list &lt;- words.eng\n\n# ------------------------------------------------------------\ngenerate_puzzle &lt;- function(sides=4,letters_per_side=3,\n                            vowel_count=4,replacement = FALSE){\n  set.seed(1234567) # DELETE THIS LINE OR YOU WILL GET THE SAME PUZZLE EVERY TIME\n  if(sides &lt; 4){\n    print(\"Minimum Side is 4, changing to 4\")\n    sides = 4\n  }\n  if (vowel_count &lt; sides) replacement=TRUE\n  if (vowel_count &gt; length(vowels)) replacement=TRUE\n  use_vowels &lt;- sample(vowels,vowel_count,replace = replacement)\n  use_consonants &lt;- sample(consonants,letters_per_side*sides-vowel_count,replace = replacement)\n  # deal out the letters\n  letter = NULL\n  vowels_used = 1\n  consonants_used = 1\n  spot = 1\n  for (i in 1:letters_per_side){\n    for(j in 1:sides){\n      # don't put vowel at edge of side but it's just cosmetic\n      if (i == 2 & vowels_used &lt;= vowel_count){\n        letter[spot] &lt;- use_vowels[vowels_used]\n        vowels_used &lt;- vowels_used + 1\n        spot &lt;- spot + 1\n      } else{\n        letter[spot] &lt;- use_consonants[consonants_used]\n        consonants_used &lt;- consonants_used + 1      \n        spot &lt;- spot + 1\n        \n      }\n    }\n  }\n  puzzle &lt;- tibble(side=rep(1:sides,letters_per_side),\n                   spot=unlist(map(1:letters_per_side,rep,sides)), \n                   letter=letter) %&gt;% arrange(side,spot)\n  return(puzzle)\n}\n\n# let's see what this does\ngenerate_puzzle()\n## # A tibble: 12 x 3\n##     side  spot letter\n##    &lt;int&gt; &lt;int&gt; &lt;chr&gt; \n##  1     1     1 v     \n##  2     1     2 i     \n##  3     1     3 l     \n##  4     2     1 m     \n##  5     2     2 u     \n##  6     2     3 r     \n##  7     3     1 c     \n##  8     3     2 o     \n##  9     3     3 y     \n## 10     4     1 t     \n## 11     4     2 a     \n## 12     4     3 f\n\nNow we have a data frame with twelve random letters, including four vowels, assigned to one of three spots on four sides.\n\n\nIt’s not necessary to solve the puzzle, but it would be nice to draw the puzzle in the style that appears in The Times. If all we needed to do was make a square the task of drawing it would be trivial but, as noted above, I can’t leave well enough alone. If we want to make polygons of arbitrary sizes we need to do a bit of trigonometry. First we generate the vertices of our polygon, then the points on each segment where the letters will go (as an aside, I say “vertices,” the proper Latin plural. The “newspaper of record” abandoned Latin plurals a decade ago. It grinds my gears to see the Times printing “vertexes”).\n\n# -------------------------------------------------------------\nget_polygon &lt;- function(sides=4){\n  x_center &lt;- 0\n  y_center &lt;- 0\n  radius &lt;- 5\n  y &lt;- NULL\n  x &lt;- NULL\n  angle = 3.925\n  angle_increment &lt;-  2 * pi / sides\n  for (i in 1:sides){\n    x[i] = x_center + radius * cos(angle)\n    y[i] = y_center + radius * sin(angle)\n    angle = angle + angle_increment\n  }\n  #close figure\n  x[i+1] &lt;- x[1]\n  y[i+1] &lt;- y[1]\n  return(data.frame(x=x,y=y))\n}\n# -------------------------------------------------------------\nget_points_on_segment &lt;- function(end_points,num_points){\n  # poin tdistance is fraction of segment length\n  a &lt;- as.numeric(end_points[1,])\n  b &lt;- as.numeric(end_points[2,])\n  # Use atan2!\n  th = atan2( b[2]-a[2] , b[1]-a[1] )\n  # length of segment AB\n  AB = sqrt( (b[2]-a[2])^2 + (b[1]-a[1])^2 )\n  AB_fraction &lt;- AB / (num_points +1 )\n  # points equidistant on the line\n  AP = sapply(1:(num_points),function(x) x * AB_fraction)\n  # The points of interest\n  c = sapply(AP,function(d) c(x = a[1] + d*cos( th ),\n                              y = a[2] + d*sin( th ))) %&gt;% \n    t() %&gt;%\n    as.data.frame()\n  return(c)\n}\n# -----------------------------------------------------\nget_letter_coords &lt;- function(puzzle,sides=4,letters_per_side=3){\n  \n  puzzle_shape &lt;- get_polygon(sides)\n  puzzle&lt;-lapply(1:(nrow(puzzle_shape)-1),\n                     function(p) get_points_on_segment(puzzle_shape[p:(p+1),],\n                                                       letters_per_side)) %&gt;% \n    bind_rows() %&gt;% \n    bind_cols(puzzle)\n  return(puzzle)\n}\n# -------------------------------------------------------------\ndraw_puzzle &lt;-function(puzzle,sides=4,letters_per_side=3){\n  puzzle_shape &lt;- get_polygon(sides)\n  gg &lt;- puzzle_shape %&gt;% ggplot(aes(x,y)) + geom_path() + coord_fixed() +\n    geom_point(data = puzzle,aes(x,y),size=20,color=\"white\") + \n    geom_text(data = puzzle,aes(x,y,label = letter),size=10) + \n    theme_void() + \n    theme(panel.background = element_rect(fill=\"pink\")) + \n    NULL \nreturn(gg)\n}\n\n# Draw puzzle sample\ngenerate_puzzle() %&gt;%\n  get_letter_coords(sides=sides,letters_per_side = letters_per_side) %&gt;% \n  draw_puzzle()\n\n\n\n\nRemember we designed the generator to work with arbitrary dimensions. Let’s try five sides with two letters per side.\n\ngenerate_puzzle(5,2) %&gt;%\n  get_letter_coords(5,2) %&gt;% \n  draw_puzzle(5,2)\n\n\n\n\nFun!\n\n\n\nSolve the Puzzle\n\n\nMuch of the grunt work is done by the wfinder package, which generates a word list from an aribtrary set of letters, as in Scrabble. Unlike Scrabble, we CAN reuse the same letter more than once. This package also contains a list of English words we use. You can substitute any word list you like, in any language. My Mom, whose native language was German, was the champion in our family. I always struggled even though I liked to brag about my high SAT verbal score. I am grateful to Mom for knocking me down a peg. Anyhoo, I am really in awe of the power of the grep function. Regexes are a dark art to me. The idea that a short line could find every possible word in an instant boggles (don’t like that game either) the mind. Suppose you pull the Scrabble tiles “ABAHRTY”.\n\ngrep(\"^[abahrty]*$\",word_list,value = T)\n##   [1] \"aa\"       \"aah\"      \"ab\"       \"aba\"      \"abaya\"    \"abb\"     \n##   [7] \"abba\"     \"abray\"    \"aby\"      \"ah\"       \"aha\"      \"ahh\"     \n##  [13] \"ar\"       \"araba\"    \"arar\"     \"arb\"      \"arba\"     \"arhat\"   \n##  [19] \"arrah\"    \"array\"    \"art\"      \"arty\"     \"ary\"      \"at\"      \n##  [25] \"atar\"     \"att\"      \"attar\"    \"ay\"       \"ayah\"     \"ba\"      \n##  [31] \"baa\"      \"baba\"     \"baby\"     \"bah\"      \"baht\"     \"bar\"     \n##  [37] \"barb\"     \"barra\"    \"barrat\"   \"barratry\" \"baryta\"   \"bat\"     \n##  [43] \"batata\"   \"bath\"     \"batt\"     \"batta\"    \"batty\"    \"bay\"     \n##  [49] \"bayt\"     \"bra\"      \"brat\"     \"bratty\"   \"bray\"     \"brr\"     \n##  [55] \"brrr\"     \"by\"       \"ha\"       \"haar\"     \"hah\"      \"haha\"    \n##  [61] \"harry\"    \"hart\"     \"hat\"      \"hath\"     \"hay\"      \"rabat\"   \n##  [67] \"rah\"      \"rat\"      \"rata\"     \"ratatat\"  \"rath\"     \"ratty\"   \n##  [73] \"ray\"      \"raya\"     \"rayah\"    \"rhy\"      \"rhyta\"    \"rya\"     \n##  [79] \"rybat\"    \"ta\"       \"tab\"      \"tabby\"    \"taha\"     \"tahr\"    \n##  [85] \"tar\"      \"tara\"     \"tarry\"    \"tart\"     \"tartar\"   \"tarty\"   \n##  [91] \"tat\"      \"tatar\"    \"tath\"     \"tatt\"     \"tatty\"    \"tay\"     \n##  [97] \"tayra\"    \"thar\"     \"that\"     \"thy\"      \"trat\"     \"tratt\"   \n## [103] \"tray\"     \"try\"      \"ya\"       \"yabby\"    \"yah\"      \"yar\"     \n## [109] \"yarr\"     \"yarta\"    \"yatra\"    \"yay\"\n\n112 words out of a corpus of over 260 thousand. Instantly. That’s all the code it takes? That’s nuts! That’s efficient low-level coding. wfindr wraps that bit of magic with some bells and whistles to aid with word puzzles. In particular it crafts regexes that conform to the rules of scrabble. The example above creates a word list that might use more of a letter than we have in our tiles. To fix that, the simple regex I show above gets converted to a much fancier one.\n\nmodel_to_regex(allow=\"abahrty\",type=\"scrabble\")\n## [1] \"(?=^((([^a]*a[^a]*){1,2})|([^a]*))$)(?=^((([^b]*b[^b]*){1,1})|([^b]*))$)(?=^((([^h]*h[^h]*){1,1})|([^h]*))$)(?=^((([^r]*r[^r]*){1,1})|([^r]*))$)(?=^((([^t]*t[^t]*){1,1})|([^t]*))$)(?=^((([^y]*y[^y]*){1,1})|([^y]*))$)^[abhrty]*$\"\n\nWhoa! Like I said. It’s regex is a dark art.\n\n\nNow we have all the possible words to use in the puzzle. Just throwing random words around from the solution set would eventually find some answers but we can do much better than that. To find the “best” next word, we can pick the word that has the most yet-unused letters. By default, the function below returns one word but it could return more. In practice, I found iterating through more words was rarely necessary to get a solution but drastically increased computation time and memory usage of the recursive function that calls it.\n\nfind_next_best_words &lt;- function(w,needed_letters,max_return=1){\n  # the higher max_return is the more words will be traversed.  Careful,\n  # computation times will geometrically increase.\n  # puzzle_words is global\n  # find words that start with last letter of w\n  next_words&lt;-puzzle_words[str_starts(puzzle_words,str_sub(w,-1))]\n  # prioritize words by greatest overlap with unused letters\n  next_word_chars &lt;-  map(next_words,strsplit,split=\"\") %&gt;% unlist(recursive = F)\n  temp &lt;- map(next_word_chars,function(x) length(setdiff(needed_letters,x))) %&gt;% unlist()\n  if (is.vector(temp)){\n    next_words &lt;- next_words[order(temp)]\n    max_return &lt;- min(length(next_words),max_return)\n    return(next_words[1:max_return])  \n  } else{\n    return()\n  }\n}\n# -----------------------------------------------------\n# check if we have used all the letters yet\ntest_needed_letters &lt;- function(word_chain){\n  word_chain_chars &lt;-  paste0(word_chain,collapse = \"\") %&gt;% \n    strsplit(split=\"\") %&gt;%\n    unlist() %&gt;% \n    unique()\n  return(setdiff(all_puzzle_letters,\n                     word_chain_chars))\n}\n\nNow we come to the workhorse recursive function. “Recursive” just means it calls itself. I’ve learned the trick to recursive functions is getting out of them, otherwise you get deeper and deeper into the “Beyond” section of “Bed, Bath and Beyond” and run out of memory pretty quickly. At least nowadays you kids don’t have to worry about the whole machine crashing. You can just nuke the process that’s stuck.\n\n\nWe start by preparing to iterate make_chain over the full list of valid words. Naturally we expect to find a solution before traversing much of the list. We build the solution chain by choosing a word that ends with a letter that has not been an ending letter yet. Otherwise we might chase our tail forever if a solution doesn’t lie on that path. Then we pick the best next word as described above. Then we call make_chain again and again and again.\n\n\nHere we limit the solution chain to a maximum of five words. Each time make_chain is called we run some tests and climb back out of the recursive stack if one of these conditions has been met:\n\n\n\nThe chain is more than five words with no solution.\n\n\nA solution is found.\n\n\nWe run out of last letter/first letter possibilities\n\n\nThe are no next words found.\n\n\nmake_chain &lt;- function(word_chain,used_last_letters){\n  needed_letters &lt;- test_needed_letters(word_chain)\n  if (length(word_chain)&gt;6){\n    # Come on, if you can't solve in 5 words, you suck!\n    return()\n  }\n  if (length(needed_letters)==0) {\n    # Yay! We have a solution.\n    return(list(word_chain))\n  }\n  else {\n    last_word &lt;- tail(word_chain,1)\n    last_letter &lt;-str_sub(last_word,-1L)\n    if (str_detect(used_last_letters,last_letter,negate=T)){\n      used_last_letters &lt;- paste0(last_letter,used_last_letters,collapse = \"\")\n      next_word&lt;-find_next_best_words(last_word,needed_letters,max_return=1)\n       if (length(next_word)&gt;0){\n        word_chain &lt;- make_chain(c(word_chain,next_word),used_last_letters)\n        } else {\n          # no next word found\n          return()\n        }\n    } else{\n      # start of next word would be a letter that has already been used\n      return()\n    }\n  }\n} \n\nThe function solve_puzzle is a wrapper around make_chain that first gets all the possible words that our letters allow, removing words that violate the rule of no consecutive letters from the same line. Note the use of the &lt;&lt;– assignment operator that accesses global variables from within functions. This practice is frowned upon in some circles but, since we are using nested recursion, I didn’t want to make new copies of every variable each time make_chain is called.\n\n# dplyr chain-friendly permuatations\nd_permute &lt;- function(v, n, r,  set, repeats.allowed){\n  return(permutations(n, r, v, set, repeats.allowed))\n}\n\nget_line_combos &lt;- function(a_side,puzzle){\n  combos &lt;- puzzle %&gt;% filter(side==a_side) %&gt;% \n    pull(letter) %&gt;% \n    as.character() %&gt;% \n    d_permute(n=3,r=2,set=F,repeats.allowed = T) %&gt;% \n    apply(1,paste0,collapse=\"\")\n  return(combos)\n}\n\n\nsolve_puzzle &lt;- function (puzzle) {\n  # get all letter combos that are invalid because they lie on the same line segment\n  bans &lt;- map(1:sides,get_line_combos,puzzle=puzzle) %&gt;% unlist()\n  #get all possible words\n  puzzle_words &lt;&lt;- scrabble(paste0(puzzle$letter,collapse = \"\"),words=word_list)\n  length(puzzle_words)\n  #winnow out illegal ones\n  banned_words &lt;- map(bans,function(x) puzzle_words[str_which(puzzle_words,x)]) %&gt;% \n    unlist()\n  puzzle_words &lt;&lt;- puzzle_words[!(puzzle_words %in% banned_words)]\n  length(puzzle_words)\n  puzzle_words &lt;&lt;-puzzle_words[order(nchar(puzzle_words),decreasing = TRUE, puzzle_words)]\n  \n  \n  all_puzzle_letters &lt;&lt;- puzzle$letter %&gt;% as.vector()\n  \n  solutions &lt;- map(puzzle_words,make_chain,\"\") %&gt;% unlist(recursive = F)\n  return(solutions)\n}\n\nWhew! Now let’s actually solve a puzzle. The solve_puzzle function returns a list of lists with all the found solutions.\n\nvowel_count &lt;- sides\n# global variables\nall_puzzle_letters &lt;- NULL\npuzzle_words &lt;- NULL\npuzzle &lt;- generate_puzzle(sides=sides,\n                          letters_per_side = letters_per_side,\n                          vowel_count = vowel_count)\n# add letter coordinates for plot\npuzzle &lt;- get_letter_coords(puzzle,\n                            sides=sides,\n                            letters_per_side = letters_per_side)\n#draw_puzzle(puzzle)\nsolutions &lt;- solve_puzzle(puzzle)\n\nsolutions %&gt;% head()\n## [[1]]\n## [1] \"vortical\" \"loamy\"    \"yuca\"     \"aimful\"  \n## \n## [[2]]\n## [1] \"voracity\" \"ymolt\"    \"trayful\" \n## \n## [[3]]\n## [1] \"foulmart\" \"trifoly\"  \"yuca\"     \"avoutry\" \n## \n## [[4]]\n## [1] \"vacuity\" \"ymolt\"   \"trifoly\"\n## \n## [[5]]\n## [1] \"trayful\" \"lorica\"  \"avoutry\" \"ymolt\"  \n## \n## [[6]]\n## [1] \"flavory\" \"ymolt\"   \"toluic\"\n\nWe may have hundreds of solutions or none. You can look at the solutions variable to see all we found. The goal of The Times puzzle is to solve in the minimum number of words so we’ll take the solution with the least number of words (there may be many) and print that on the puzzle.\n\n# ---------------------------------------------------------\ndraw_solution &lt;- function(puzzle, solutions){\n  if (is.null(solutions)) {\n    solution &lt;- \"No Solution\"\n  } else {\n    ideal &lt;- map(solutions,length) %&gt;% unlist() %&gt;% which.min()\n    solution &lt;- c(solutions[[ideal]],paste(length(solutions)-1,\"other solutions\")) \n  }\n  gg &lt;- draw_puzzle(puzzle)\n  gg &lt;- gg + annotate(\"text\",x=0,y=0.9,label=paste(solution, collapse = \"\\n\"), size = 6)\n  print (gg)\n}\n\ndraw_solution(puzzle, solutions)\n\n\n\n\nLet’s go back to the image at the top of this post which is from The Times. We’ll use those letters to solve an actual puzzle. Do the puzzle authors generate the puzzles randomly or do they work backword from a selected word list? I have no idea.\n\nsample_letters &lt;- \"taperdnilyco\"\npuzzle &lt;- generate_puzzle() %&gt;% get_letter_coords()\n#replace random letters with the one in the known puzzle\npuzzle$letter &lt;- strsplit(sample_letters,split = NULL) %&gt;% unlist()\nsolutions &lt;- solve_puzzle(puzzle)\nsolutions %&gt;% head()\n## [[1]]\n## [1] \"lectionary\" \"yealdon\"    \"noplace\"   \n## \n## [[2]]\n## [1] \"centroidal\" \"lectionary\" \"yipe\"      \n## \n## [[3]]\n## [1] \"rantipole\" \"etypical\"  \"leporid\"  \n## \n## [[4]]\n## [1] \"planetoid\" \"dielytra\"  \"article\"  \n## \n## [[5]]\n## [1] \"placitory\" \"yealdon\"  \n## \n## [[6]]\n## [1] \"clarionet\" \"torpidly\"\n\nWe found 851 solutions to this particular puzzle, quite a few. Furthermore, If you are really good, you could solve this puzzle with two words!\n\ndraw_solution(puzzle, solutions)\n\n\n\n\nThere you have it. You might grumble that too many of the words in the scrabble dictionary are not in your vocabulary. They certainly aren’t in mine. Feel free to use a shorter word list with more common words. Here are a bunch. That will increase the liklihood that no solution is found, though.\n\n\nFurther work that might be done would be to filter for completely unique solutions, with no overlapping words. Also we might create a Shiny application that does pretty animation drawing lines across the puzzle of the solution.\n\n\nNaturally, you should only use this code to check your answer. No cheating!"
  },
  {
    "objectID": "posts/2019-06-12-why-i-migrated-from-excel-to-r/2019-06-12-why-i-migrated-from-excel-to-r.html",
    "href": "posts/2019-06-12-why-i-migrated-from-excel-to-r/2019-06-12-why-i-migrated-from-excel-to-r.html",
    "title": "Why I migrated from Excel to R",
    "section": "",
    "text": "Old Spreadsheets\n\n\nI’ve been a spreadsheet power user from the days of Visicalc for the Apple II. I migrated to Lotus 1-2-3, to Borland Quattro and finally to Excel. With Excel, I’ve bludgeoned Visual Basic to create some pretty complicated dashboards and analytics. When I started using R I used tools like RExcel that plug R into Excel as an analytic server, or I would use Excel to download data from investment databases and export it for use in R. But now I find I open up Excel only rarely and do all my quantitative investigations entirely within R. Why?\n\nSimplicity. R uses is a very different paradigm than a spreadsheet so it takes some getting used to. On the surface, R is a programming language, like C or Java. Spreadsheets were invented to free humans to get real analytic work done without becoming coders. Yet R comes at the coding angle from a very different direction than application languages. It is a data science tool first and a programming language second. Like a spreadsheet, the central unit of analytic work is data in a row and column format. Once you master the vernacular of manipulating these “data frames” the rest is easy. Like a spreadsheet, R is interactive. You try single operations and can paste the successful ones into your “program” in a sequential fashion, building up your analysis step by step. In doing so you are creating a log of your work that lets you pick up later where you left off without missing a beat or reuse bits in other projects. Most people use the R Studio development environment as their workbench. It’s simple, powerful and free!\nAuditability. Big Excel spreadsheets are a labyrinth of linked formulas. Tracing errors is extremely difficult. Noticing errors at all is often tough. They often go unnoticed for the life of the spreadsheet. When I come back to a spreadsheet I’ve created months ago, I often can’t remember how I did something. If something breaks, tracking down the broken piece takes a long time. So while spreadsheets are easy to master, mastery comes at the expense of maintainability. With R, each step of the analysis proceeds in a roughly linear fashion. Each piece building on the previous one. It is easy to see where the problems are and to insert the fixes without blowing up something else you didn’t realize was connected.\nReproducibility. How often do you share a big spreadsheet with someone else? Can they use it? Have you been bequeathed a spreadsheet that is part of the team workflow that you have no idea how to maintain? With R the logic of the analysis flows in discrete steps so every step is immediately visible. The code is its own log of all the work performed. That’s not to say you are off the hook for documentation. Well commented code is a sign of disciplined thinking and a courtesy to both others and your future self. Inserting a comment line in R code comes naturally, I find, while it requires conscious effort in Excel.\nShareability. “Notebooks” are the new thing and I love them. These are HTML documents, like this blog, that include descriptive text, R code (or whatever language you use) and the output of the code. They make sharing and showing off your work visually attractive and simple to follow. You can attach the HTML document to an email, render it as a PDF or publish it to a web site. Embedding markup language is a little extra work but the R Studio IDE creates the templates automatically, and the result is sharp.\nVisualization. It is easy to create charts in Excel. It is easy to create charts in R, though it is done in code, not interactively. What R can do that Excel can’t, is to go further to make great looking charts because of the customization that is possible. I won’t claim it is easy, though. The learning curve is steep but rewarding\n\n SOURCE\n\nScalability. As your data sets become larger, R scales with them. Excel becomes unwieldy.\nPlug-in packages. This is the single biggest reason to drop spreadsheets. There are hundreds of plug-in packages for R that extend its analytic power. They can all be installed with a couple clicks. Any new task I want to perform starts with asking if there is a package that will do it for me. The answer is almost always yes. Further, my own education in data science has advanced by leaps and bounds as I’ve learned to use these powerful new analytic tools. I would go so far as to say this is a big career hack opportunity! If you are producing highly sophisticated analyses you are going to get noticed compared to the person that is confined to the primitive capabilities of Excel.\n\nAt the end of the day the tool that gets the job done is the right tool. For me, R is a big step forward in efficiency, power and fun over Excel.\nUPDATE June 2019: I originally wrote this note for an internal corporate blog in April 2017. Then, I recommended to NOT to use R for real-time live dashboards. I would amend that statement to say “it depends.” The Shiny interactive web framework from RStudio makes interactive dashboards look very good indeed. Whether or not you can query live data APIs at the requisite frequency depends on the availability of a data feed, a package to grab it or your ability to write your own."
  },
  {
    "objectID": "posts/2019-07-16-gender-diversity-in-r-and-python-package-contributors/2019-07-16-gender-diversity-in-r-and-python-package-contributors.html",
    "href": "posts/2019-07-16-gender-diversity-in-r-and-python-package-contributors/2019-07-16-gender-diversity-in-r-and-python-package-contributors.html",
    "title": "Gender Diversity in R and Python Package Contributors",
    "section": "",
    "text": "Over the last few years I have really enjoyed becoming part of the R community. One of the best things about the community is the welcoming, inclusive and supportive nature of it. I can’t speak for other communities in the computer or data science worlds but I am well aware of the “brogrammer” culture in some circles that can be off-putting at times. The rise of codes of conduct across the open source world is changing things for the better, I think.\n\n\nA couple months ago the creator of Python was interviewed saying he thinks open source programming languages have a gender diversity problem. This got me to thinking about whether the inclusive environment I observe in the R community is reflected in female contributions to popular packages and how it compares to the Python world. Most of these packages are maintained on Github which includes all the contributors who use the Github environment to contribute. Let’s take a stab at identifying the gender of these contributors by name.\n\n\nWe will take a multi-stage approach to getting an answer to this question.\n\n\n\nGet the names of the top packages in R and Python.\n\n\nIdentify which those packages which are maintained on Github.\n\n\nGet the contributors to those packages (not as easy as it sounds).\n\n\nGet baby names by gender from the U.S. Social Security database.\n\n\nDecide whether a name is likely to be female or male.\n\n\nMap all package conrtributors to gender, where possible.\n\n\n\nAs usual I follow a couple conventions. The Tidyverse dialect is used throughout. All functions to fetch data from the Web are wrapped in a test to see if the data was already retrieved. This ensures that this notebook won’t break if things in the wild change. In that event, you must get the data files from this Github repo for this to work.\n\n\nlibrary(tidyverse)\nlibrary(purrr)\nlibrary(jsonlite)\nlibrary(rvest)\nlibrary(data.table) #for downloading CRAN/RStudio logs\nlibrary(httr)\nlibrary(gh)\nlibrary(formattable) #percent"
  },
  {
    "objectID": "posts/2019-07-16-gender-diversity-in-r-and-python-package-contributors/2019-07-16-gender-diversity-in-r-and-python-package-contributors.html#introduction",
    "href": "posts/2019-07-16-gender-diversity-in-r-and-python-package-contributors/2019-07-16-gender-diversity-in-r-and-python-package-contributors.html#introduction",
    "title": "Gender Diversity in R and Python Package Contributors",
    "section": "",
    "text": "Over the last few years I have really enjoyed becoming part of the R community. One of the best things about the community is the welcoming, inclusive and supportive nature of it. I can’t speak for other communities in the computer or data science worlds but I am well aware of the “brogrammer” culture in some circles that can be off-putting at times. The rise of codes of conduct across the open source world is changing things for the better, I think.\n\n\nA couple months ago the creator of Python was interviewed saying he thinks open source programming languages have a gender diversity problem. This got me to thinking about whether the inclusive environment I observe in the R community is reflected in female contributions to popular packages and how it compares to the Python world. Most of these packages are maintained on Github which includes all the contributors who use the Github environment to contribute. Let’s take a stab at identifying the gender of these contributors by name.\n\n\nWe will take a multi-stage approach to getting an answer to this question.\n\n\n\nGet the names of the top packages in R and Python.\n\n\nIdentify which those packages which are maintained on Github.\n\n\nGet the contributors to those packages (not as easy as it sounds).\n\n\nGet baby names by gender from the U.S. Social Security database.\n\n\nDecide whether a name is likely to be female or male.\n\n\nMap all package conrtributors to gender, where possible.\n\n\n\nAs usual I follow a couple conventions. The Tidyverse dialect is used throughout. All functions to fetch data from the Web are wrapped in a test to see if the data was already retrieved. This ensures that this notebook won’t break if things in the wild change. In that event, you must get the data files from this Github repo for this to work.\n\n\nlibrary(tidyverse)\nlibrary(purrr)\nlibrary(jsonlite)\nlibrary(rvest)\nlibrary(data.table) #for downloading CRAN/RStudio logs\nlibrary(httr)\nlibrary(gh)\nlibrary(formattable) #percent"
  },
  {
    "objectID": "posts/2019-07-16-gender-diversity-in-r-and-python-package-contributors/2019-07-16-gender-diversity-in-r-and-python-package-contributors.html#identify-the-top-packages-in-r-and-python.",
    "href": "posts/2019-07-16-gender-diversity-in-r-and-python-package-contributors/2019-07-16-gender-diversity-in-r-and-python-package-contributors.html#identify-the-top-packages-in-r-and-python.",
    "title": "Gender Diversity in R and Python Package Contributors",
    "section": "\nIdentify the top packages in R and Python.\n",
    "text": "Identify the top packages in R and Python.\n\n\nUse the cranlogs api from RStudio to get top package downloads from their CRAN mirror. This is potentially a slow function but the top package downloads are pretty stable so we choose five randomly selected dates.\n\n# ----------------------------------------------------------------\n#select 5 random days from the last six months\n# Read data from RStudio site\n# custom version of a function from the installr package. See my Github repo.\nsource(file=\"data/download_RStudio_CRAN_data.R\") \n\nif (!file.exists(\"data/r_pkg_list.rdata\")) {\n RStudio_CRAN_dir &lt;- download_RStudio_CRAN_data(START = Sys.Date()-180,END = Sys.Date(),sample=5)\n # read .gz compressed files form local directory\n RStudio_CRAN_data &lt;- read_RStudio_CRAN_data(RStudio_CRAN_dir)\n \n dim(RStudio_CRAN_data)\n \n # Find the most downloaded packages\n r_pkg_list &lt;- most_downloaded_packages(RStudio_CRAN_data,n=100) %&gt;% \n  as_tibble(.name_repair = make.names,c(\"downloads\")) %&gt;% \n  rename(package=X)\n \n save(r_pkg_list,file=\"data/r_pkg_list.rdata\")\n} else load(\"data/r_pkg_list.rdata\")\n\nWith Python the work as already been done for us here: https://hugovk.github.io/top-pypi-packages/. How helpful!\n\nif (!file.exists(\"data/python_pkg_list.rdata\")){\n \n py_pkgs_raw&lt;-read_json(\"https://hugovk.github.io/top-pypi-packages/top-pypi-packages-365-days.json\",\n             simplifyVector = TRUE)\n python_pkg_list &lt;- py_pkgs_raw$rows[1:100,] %&gt;% \n  as_tibble() %&gt;% \n  rename(package=project,downloads=download_count)\n save(python_pkg_list,file=\"data/python_pkg_list.rdata\")\n} else load(\"data/python_pkg_list.rdata\")"
  },
  {
    "objectID": "posts/2019-07-16-gender-diversity-in-r-and-python-package-contributors/2019-07-16-gender-diversity-in-r-and-python-package-contributors.html#get-the-contributor-names-for-each-package-repo.",
    "href": "posts/2019-07-16-gender-diversity-in-r-and-python-package-contributors/2019-07-16-gender-diversity-in-r-and-python-package-contributors.html#get-the-contributor-names-for-each-package-repo.",
    "title": "Gender Diversity in R and Python Package Contributors",
    "section": "\nGet the contributor names for each package repo.\n",
    "text": "Get the contributor names for each package repo.\n\n\nThis is the messy stuff. We build functions to get contributors to packages and then the real names of those contributors.\n\n\nWe start with a search for the relevant repo with just repo name and optionally the language. Suppose we want to know the names of the R dplyr contributors. The workflow looks like this:\n\n\nCall the API:\n\n\nhttps://api.github.com/search/repositories?q=dplyr+language:r\n\n\nGithub returns a list of the most relevant results based on their point system. In practice this means the package we care about will be the first item in the list. In this case:\n\n\n“full_name”: “tidyverse/dplyr”\n\n\nOne problem I encountered is that not all R packages are tagged as being in the R language. In particular, Rcpp and data.table are considered C language repos by Github. This is one reason why not all the top packages appear to have Github repos. I manually grab the contributors for the two packages mentioned above but, out of laziness, I didn’t go looking for any other missing packages. As we will see, most of the top 100 packages for both languages are found so we have a fairly representative sample…I assume.\n\n\nOnce we have the full package name we can create URLs to get the usernames of all the contributors.\n\n\nContributor url: https://api.github.com/repos/tidyverse/dplyr/contributors\n\n\nThis JSON object will not contain the “real” names but the links to user profiles. We have to make yet another call to the API to extract the real names. Note some people use pseudonyms so the real name won’t be available.\n\n\nCalling the endpoint for the username “https://api.github.com/users/romainfrancois”,\n\n\nwill return, among other things:\n\n\n“name”: “Romain François”\n\n\nFinally, we get what we are after!\n\n\nNOTE: You will need a Github API key for this work. Please refer to the documentation for the gh package.\n\n\nThe utility functions are below:\n\nmy_gh &lt;- function(end_point) {\n  return(jsonlite::fromJSON(jsonlite::toJSON(gh::gh(end_point)),simplifyVector = T))\n}\n\njson_to_df &lt;- function(json){\n  return(jsonlite::fromJSON(jsonlite::toJSON(json),simplifyVector = T))\n}\n\n# --------------------------------------------------------------------\nget_contributor_ids &lt;- function(target_repo){\n# loop through all pages of contributors \n search_url &lt;- paste0(\"/repos/\",\n            target_repo,\n            \"/contributors\")\n contributors_json &lt;- gh(search_url)\n \n # return null in case of no contributors\n if (nchar(contributors_json[1])==0) return(NULL)\n \n contrib_node &lt;- contributors_json\n repeat {\n  contrib_node &lt;- try(gh_next(contrib_node),silent=TRUE)\n  if (is(contrib_node) == \"try-error\") break\n  contributors_json &lt;- c(contributors_json,contrib_node) \n }\n\n contributor_ids &lt;- json_to_df(contributors_json) %&gt;%\n  bind_rows() %&gt;%  \n  select(login,url,avatar_url)\n return(contributor_ids)\n}\n\n# ---------------------------------------------------------------------------\n get_name &lt;- function(contrib_url){\n  user_data &lt;- my_gh(contrib_url)\n  # just return login name if real name is missing\n  if (is_empty(user_data$name)) return(user_data$login) else return(user_data$name)\n }\n\n# --------------------------------------------------------------------\nget_contrib_info &lt;- function(repo_name=\"dplyr\",language=NULL){\n  print(repo_name)\n  # we don't know the Github username associated with the package\n  #so construct a search to get the most likely candidate\n  search_url &lt;- paste0(\"/search/repositories?q=\",\n                       repo_name)\n  if (!is.null(language)){\n    search_url &lt;- paste0(search_url,\"+language:\", language)\n  }\n  # first api call.\n  repos &lt;- my_gh(search_url) %&gt;% .$items\n  # return NULL if no repos in Github are found\n  if (length(repos) == 0) return(NULL)\n  \n  # get full path for exact match on repo name\n  # there might be more than one user with repo of the same name\n  # Since they will be in order of Github \"score\", take just the first one\n  target_repo &lt;- repos %&gt;% \n    select(name,full_name) %&gt;% \n    filter(name == repo_name) %&gt;%\n    pull(full_name) %&gt;% \n    .[1] %&gt;% \n    unlist()\n  # return NULL if no repos in Github are found\n  if (is.null(target_repo)) return(NULL)\n  \n  #second api call\n  # get user urls for all contributors\n  contributor_ids &lt;- get_contributor_ids(target_repo)\n  \n  # return null in case of no contributors\n  if (is.null(contributor_ids)) return(NULL)\n  if (is.null(language)) language &lt;- \"none\"\n  contrib_names&lt;-map(contributor_ids$url,get_name) %&gt;% unlist()\n  print(paste(length(contrib_names),\" contributors\"))\n  contrib_info &lt;- tibble(language=language,\n                         package=repo_name,\n                         path=target_repo,\n                         contributor=contrib_names) %&gt;% \n    bind_cols(contributor_ids) %&gt;% \n    select(-url) %&gt;% unnest()\n  return(contrib_info)\n}\n\nNow let’s do the work of iterating through the package lists. As mentioned above, I get two packages manually before looping through the remaining packages. I chose to use a for loop, as opposed to map or apply so we can save the intermediate results. It is a fairly slow process and you may reach your API data limit before finishing. You don’t want to start from scratch halfway through! If you have to do this in multiple sessions, manually edit the package lists to include just what is left to retrieve.\n\n\nload(\"data/r_pkg_list.rdata\")\nif (!file.exists(\"data/r_pkg_contributors.rdata\")){\n  r_pkg_contributors &lt;- NULL\n  # Rcpp package is categorized as C++, not R, langauge so get it manually.\n  contrib_info_rcpp &lt;- get_contrib_info(\"Rcpp\")\n  contrib_info_rcpp &lt;- contrib_info_rcpp %&gt;% mutate(language = \"r\")\n  r_pkg_contributors &lt;- bind_rows(r_pkg_contributors,contrib_info_rcpp)\n  r_pkg_list &lt;- r_pkg_list %&gt;% filter(package != \"Rcpp\")\n  \n  # data.table package is categorized as C++, not R, langauge so get it manually.\n  contrib_info_dt &lt;- get_contrib_info(\"data.table\")\n  contrib_info_dt &lt;- contrib_info_dt %&gt;% mutate(language = \"r\")\n  r_pkg_contributors &lt;- bind_rows(r_pkg_contributors,contrib_info_dt)\n  r_pkg_list &lt;- r_pkg_list %&gt;% filter(package != \"dt\")\n  \n  # use for loop instead of map or apply so we can save intermediate steps\n  for(pkg in r_pkg_list$package) {\n    r_pkg_contributors &lt;- r_pkg_contributors %&gt;% \n      bind_rows(get_contrib_info(pkg,language=\"r\"))\n    save(r_pkg_contributors,file=\"data/r_pkg_contributors.rdata\")\n  }\n} else load(\"data/r_pkg_contributors.rdata\")\n\nload(\"data/python_pkg_list.rdata\")\nif (!file.exists(\"data/python_pkg_contributors.rdata\")){\n python_pkg_contributors &lt;- NULL\n for(pkg in python_pkg_list$package) {\n  python_pkg_contributors &lt;- python_pkg_contributors %&gt;% \n   bind_rows(get_contrib_info(pkg,language=\"python\"))\n  save(python_pkg_contributors,file=\"data/python_pkg_contributors.rdata\")\n } \n} else load(\"data/python_pkg_contributors.rdata\")\n\n#Let's merge the two datasets to simplify handling.\npkg_contributors &lt;- bind_rows(r_pkg_contributors,python_pkg_contributors)"
  },
  {
    "objectID": "posts/2019-07-16-gender-diversity-in-r-and-python-package-contributors/2019-07-16-gender-diversity-in-r-and-python-package-contributors.html#who-are-the-most-prolific-r-package-contributors",
    "href": "posts/2019-07-16-gender-diversity-in-r-and-python-package-contributors/2019-07-16-gender-diversity-in-r-and-python-package-contributors.html#who-are-the-most-prolific-r-package-contributors",
    "title": "Gender Diversity in R and Python Package Contributors",
    "section": "\nWho are the most prolific R package contributors?\n",
    "text": "Who are the most prolific R package contributors?\n\npkg_contributors %&gt;% \n filter(language==\"r\") %&gt;% \n group_by(contributor) %&gt;% \n summarise(packages=n()) %&gt;% \n arrange(desc(packages))\n## # A tibble: 1,299 x 2\n##    contributor            packages\n##    &lt;chr&gt;                     &lt;int&gt;\n##  1 Hadley Wickham               45\n##  2 Jim Hester                   36\n##  3 Kirill Müller                31\n##  4 Mara Averick                 26\n##  5 Jennifer (Jenny) Bryan       24\n##  6 Gábor Csárdi                 19\n##  7 Hiroaki Yutani               18\n##  8 Lionel Henry                 16\n##  9 Yihui Xie                    16\n## 10 Christophe Dervieux          15\n## # ... with 1,289 more rows"
  },
  {
    "objectID": "posts/2019-07-16-gender-diversity-in-r-and-python-package-contributors/2019-07-16-gender-diversity-in-r-and-python-package-contributors.html#who-are-the-most-prolific-python-package-contributors",
    "href": "posts/2019-07-16-gender-diversity-in-r-and-python-package-contributors/2019-07-16-gender-diversity-in-r-and-python-package-contributors.html#who-are-the-most-prolific-python-package-contributors",
    "title": "Gender Diversity in R and Python Package Contributors",
    "section": "\nWho are the most prolific Python package contributors?\n",
    "text": "Who are the most prolific Python package contributors?\n\npkg_contributors %&gt;% \n filter(language==\"python\") %&gt;% \n group_by(contributor) %&gt;% \n summarise(packages=n()) %&gt;% \n arrange(desc(packages))\n## # A tibble: 6,194 x 2\n##    contributor     packages\n##    &lt;chr&gt;              &lt;int&gt;\n##  1 Jon Dufresne          30\n##  2 Hugo                  27\n##  3 Marc Abramowitz       24\n##  4 Jason R. Coombs       18\n##  5 Jakub Wilk            17\n##  6 Alex Gaynor           16\n##  7 Anthony Sottile       15\n##  8 Felix Yan             15\n##  9 Ville Skyttä          15\n## 10 Donald Stufft         14\n## # ... with 6,184 more rows"
  },
  {
    "objectID": "posts/2019-07-16-gender-diversity-in-r-and-python-package-contributors/2019-07-16-gender-diversity-in-r-and-python-package-contributors.html#people-who-swing-both-ways.",
    "href": "posts/2019-07-16-gender-diversity-in-r-and-python-package-contributors/2019-07-16-gender-diversity-in-r-and-python-package-contributors.html#people-who-swing-both-ways.",
    "title": "Gender Diversity in R and Python Package Contributors",
    "section": "\nPeople who swing both ways.\n",
    "text": "People who swing both ways.\n\n\nWho are the awesome humans who have contributed to both top R and Python packages? Grouping by login name ensures that we don’t get two different people with the same name but we drop it for display. There are 44 people who have contributed to some of both the top Python and R packages.\n\ntwo_lang_contrib &lt;- pkg_contributors %&gt;% \n group_by(login,contributor,language) %&gt;%\n summarise(packages=n()) %&gt;% \n spread(language,packages) %&gt;% \n ungroup() %&gt;% \n select(-login)\n\ntwo_lang_contrib &lt;- two_lang_contrib[complete.cases(two_lang_contrib),] %&gt;% \n arrange(desc(r))\n\ntwo_lang_contrib \n## # A tibble: 46 x 3\n##    contributor              python     r\n##    &lt;chr&gt;                     &lt;int&gt; &lt;int&gt;\n##  1 Craig Citro                   3     7\n##  2 Elliott Sales de Andrade      2     5\n##  3 Philipp A.                    4     4\n##  4 Aaron Schumacher              2     3\n##  5 Ayappan                       1     2\n##  6 Chapman Siu                   1     2\n##  7 Ethan White                   1     2\n##  8 Katrin Leinweber              2     2\n##  9 Mark Sandan                   1     2\n## 10 Tim D. Smith                  3     2\n## # ... with 36 more rows"
  },
  {
    "objectID": "posts/2019-07-16-gender-diversity-in-r-and-python-package-contributors/2019-07-16-gender-diversity-in-r-and-python-package-contributors.html#try-to-determine-gender-of-contributors.",
    "href": "posts/2019-07-16-gender-diversity-in-r-and-python-package-contributors/2019-07-16-gender-diversity-in-r-and-python-package-contributors.html#try-to-determine-gender-of-contributors.",
    "title": "Gender Diversity in R and Python Package Contributors",
    "section": "\nTry to determine gender of contributors.\n",
    "text": "Try to determine gender of contributors.\n\n\nI hope you found the digressions above interesting. Now let’s do what we came to do.\n\n\nTo flag names by gender we use the Social Security baby names database for 1990. It is important to be aware of the limitations of this.\n\n\n\n\nI used 1990 because I guess that is close to the average birth year of most package contributors. Is it? My birth year is (cough) 1958. I am an outlier.\n\n\n\n\nThe dataset contains registered births for only the United States. Many contributors were born, or live today, outside the U.S. The U.S, while more of a melting pot than many countries, will have a subset of global names.\n\n\n\n\nTransliteration of names from languages that don’t use Western characters don’t follow hard and fast rules. The same name might be transliterated multiple ways. “Sergey” or “Sergei?”\n\n\n\n\nOrdering of surname and given name. Chinese names typically are reported surname first. Many Chinese people follow western conventions in global settings but maybe not. I may be tagging the surname as the given name in some (many?) cases.\n\n\n\n\nMany names are used for “both” (yes, I know) genders. I choose an aribitrary ratio of gender predominance of 75% to pronounce certainty. Noteworthy: “Hadley” is in our “Uncertain” bucket.\n\n\n\n\nGender identity becomes a choice at some age. People may choose (or not choose) a gender inconsistant with the identification in this dataset.\n\n\n\n\nSome people use pseudonyms that are not common names.\n\n\n\n\nKnowing all that, let’s plunge on.\n\n\nYou can find the link to the baby names data set here. There are CSV files for each birth year in a zip file. Download, extract and import the file called “yob1990.txt”\n\nnames_90 &lt;- read_csv(\"data/yob1990.txt\",\n           col_names=c(\"first\",\"gender\",\"count\"),\n           col_types = list(col_character(),col_character(),col_number()))\n\nnames_90 &lt;- names_90 %&gt;% \n mutate(first = tolower(first)) %&gt;% \n select(first,gender,count) %&gt;% \n spread(gender,count) %&gt;% \n mutate_if(is.numeric, ~replace(., is.na(.), 0)) %&gt;% \n mutate(prob_female=F/(F+M))\n\ncutoff = 0.75 # threshhold probability for calling gender\nnames_90 &lt;- names_90 %&gt;% mutate(gender=\"Uncertain\")\nnames_90 &lt;- names_90 %&gt;% mutate(gender=if_else(prob_female&gt;cutoff,\"Female\",gender))\nnames_90 &lt;- names_90 %&gt;% mutate(gender=if_else(prob_female&lt;(1-cutoff),\"Male\",gender))\nnames_90_subset &lt;- names_90 %&gt;% select(first,gender)\n\nNow let’s join the baby names to our contributors.\n\npkg_contributors &lt;-pkg_contributors %&gt;%\n separate(\"contributor\",into=c(\"first\"),remove=FALSE,extra=\"drop\")\ngenders &lt;- pkg_contributors %&gt;% \n select(-path,-avatar_url,-login) %&gt;% \n mutate(first = tolower(first)) %&gt;% \n left_join(names_90_subset,by=\"first\") %&gt;% \n mutate_all(~replace(., is.na(.),\"Uncertain\")) \n\nOur answer now looms into view. Base R has a nice tile plot that illustrates the proportions and sizes of the cells in a crosstab so we’ll use that.\n\n\nagg_gender &lt;- genders %&gt;%  \n select(language,gender) %&gt;% \n table() \nagg_gender %&gt;% plot(main=\"Gender Representation in Package Contributions\")\n\n\n\n\nRight away we note the large fraction of “Uncertain” genders, about a third. As we noted above, there are many more contributors to Python packages, as reflected in the width of the tiles. We also can see that the fraction of women contributing to R packages looks greater.\n\n\nFor our ultimate conclusion, let’s assume that the “Uncertain” gender breaks into male and female in the same proportions that already exist.\n\n\nagg_gender &lt;- genders %&gt;% \n filter(gender != \"Uncertain\") %&gt;% \n select(language,gender) %&gt;% \n table() %&gt;% prop.table(margin=1) \n\npercent(agg_gender,digits = 0)\n##         gender\n## language Female Male\n##   python  4%    96% \n##   r       8%    92%\n\nThere it is. This was certainly a lot of work to get to a four cell crosstab but we have our answer. Women contribute to the top R packages at twice the rate of top Python packages. Can we speculate as to a reason? R is almost exclusively a data science language and most of the top packages reflect that. Python is more of a general purpose language that is also quite popular for data science, but as we look down the list of most popular Python packages we see more utility packages. Perhaps women are less represented in general computer science than they are in data science. With both languages, more than 90% of the contributors are men. Clearly, we have a way to go with gender diversity in both communities. Narrowing down the package list to focus on just data science packages is an avenue for further exploration.\n\n\nAs a bonus, what are the most “feminine” packages?\n\ngenders %&gt;% group_by(language,package,gender) %&gt;% \n  filter(gender != \"Uncertain\") %&gt;% \n  count() %&gt;% \n  spread(gender,n) %&gt;% \n  mutate(frac_female = Female/(Female+Male)) %&gt;% \n  arrange(desc(frac_female))\n## # A tibble: 147 x 5\n## # Groups:   language, package [147]\n##    language package    Female  Male frac_female\n##    &lt;chr&gt;    &lt;chr&gt;       &lt;int&gt; &lt;int&gt;       &lt;dbl&gt;\n##  1 r        cellranger      1     1       0.5  \n##  2 r        hms             1     2       0.333\n##  3 r        rprojroot       1     2       0.333\n##  4 r        tidyselect      3     6       0.333\n##  5 r        forcats         7    18       0.28 \n##  6 r        ellipsis        1     3       0.25 \n##  7 python   decorator       2     7       0.222\n##  8 r        scales          5    18       0.217\n##  9 r        tidyr          11    41       0.212\n## 10 r        pillar          1     4       0.2  \n## # ... with 137 more rows\n\nThat’s interesting. There are 28 popular packages across both languages where more than 10% of the contributors are female. Of those 25 are R packages and only 3 are Python packages.\n\n\nThere are other dimensions of diversity we might look at that are beyond the ability to infer from names. It would be nice if we could see actual images of all contributors so we might make some observations about racial diversity or remove some of the ambiguities around gender identification. This approach would come with its own set of challenges and risks, however.\n\n\nAs mentioned at the start of this ariticle, there are many reasons to take our conclusions with a grain of salt. I certainly do not claim this analysis is definitive. A better approach might be to simply survey the contributors. Still, the results conform with what intuition might provide.\n\n\nI welcome critiques of my methods or conclusions. I have a sneaky suspicion I got the Github contributor names the hard way. Thanks for reading!"
  },
  {
    "objectID": "posts/2019-12-08-state-taxes-it-s-not-just-about-income/2019-12-08-state-taxes-it-s-not-just-about-income.html",
    "href": "posts/2019-12-08-state-taxes-it-s-not-just-about-income/2019-12-08-state-taxes-it-s-not-just-about-income.html",
    "title": "State Taxes: It’s not just about Income",
    "section": "",
    "text": "Much of the discussion around tax burdens focuses on income taxes but, at the state level, that leaves out two other big sources of tax liability, sales and property taxes. Here we’ll quickly look at the interplay of all three taxes in a graphical way. This can inform our thinking about how attractive it is to live in each state and on public policy questions involving tax fairness. The plotly package lets us easily create an interactive 3D scatter plot that is uniquely useful to visualize this.\nSales taxes vary greatly by state but, for lower income people, might be the biggest tax burden. Indeed, since low-income families spend a larger fraction of their income, these taxes are “regressive” since the relative burden grows as income falls. Income taxes are typically “progressive” since, in most states, the rate grows with income levels. Property taxes aren’t directly levied on renters but the landlords pass the tax through via higher rents, so everyone pays. Let’s take a quick look at how tax rates vary by state and category.\nThe tax data was found in three different places:\n\nIncome tax rates from https://taxfoundation.org/state-individual-income-tax-rates-brackets-2019/\nProperty tax Rates from https://wallethub.com/edu/states-with-the-highest-and-lowest-property-taxes/11585/\nSales Tax Rates https://www.salestaxinstitute.com/resources/rates\n\nI make some choices in how to present the data. First of all, I use the top marginal rates, so this represents the “worst-case” tax burden. It should be representative of the overall tax structure and useful to compare across states. Next, I add average municipal income taxes computed by the Tax Foundation for each state to the state income tax rate. If you live in New York City, this will substantially understate your tax burden and overstate it elsewhere. Some municipalities levy sales taxes as well but I do NOT include these because they vary so widely and we don’t have all day. Also, municipalities love to tax people who can’t vote, like out of towners, with hotel and rental car taxes. These would not affect your view of where to live. How about excise taxes on gasoline, cigarettes, etc? Not included.\nI already combined the data from each source with the adjustments mentioned above into a single CSV file. Load it with the required libraries.\n\nsuppressPackageStartupMessages(library(tidyverse))\nsuppressPackageStartupMessages(library(plotly))\n\nstate_rates &lt;- read_csv(\"data/state_rate.csv\",col_types = \"fnnn\") \n\nLet’s take a quick look at the summary statistics.\n\nsummary(state_rates[,2:4])\n\n   income_tax       sales_tax      property_tax  \n Min.   : 0.000   Min.   :0.000   Min.   :0.270  \n 1st Qu.: 4.925   1st Qu.:4.375   1st Qu.:0.730  \n Median : 5.950   Median :6.000   Median :0.980  \n Mean   : 5.835   Mean   :5.062   Mean   :1.119  \n 3rd Qu.: 7.190   3rd Qu.:6.250   3rd Qu.:1.550  \n Max.   :13.300   Max.   :7.250   Max.   :2.440  \n\n\nSome states have no personal income tax at all but have to raise revenue somehow. Most commonly, sales tax forms a big part of the budget. Is there a pattern where lower income tax rates correlate with higher sales or property taxes? A correlation matrix provides a quick check.\n\nknitr::kable(cor(state_rates[2:4]))\n\n\n\n\n\nincome_tax\nsales_tax\nproperty_tax\n\n\n\n\nincome_tax\n1.0000000\n0.0292638\n0.1074844\n\n\nsales_tax\n0.0292638\n1.0000000\n0.1115520\n\n\nproperty_tax\n0.1074844\n0.1115520\n1.0000000\n\n\n\n\n\nIt doesn’t look like there is any relationship.\nTax rates are not the same thing as cash out of pocket. As mentioned above, several issues affect the translation of rates to dollars. Ideally, we would like to know which states are the most expensive to live in, tax-wise. We don’t care which pocket it comes out of but we have to make assumptions.\nLet’s add adjustment factors for the impact of sales and property taxes relative to income taxes. This will let us add all three together to come up with a “tax pain” index. In theory, property taxes are levied according to a percentage of the value of the home. But there are complex formulas that go beyond just the published “rate.” In New York, it turns out that the median property tax bill is roughly equal to the median income tax liability, so I chose an adjustment factor of 1.0. How much of your taxable income is spent on consumption of things that sales tax is levied on? As mentioned above, low earners typically live hand-to-mouth. Affluent people can save more for deferred consumption, philanthropy or passing to heirs. I chose to assume 30% of household income is spent where sales taxes apply. Also note that sales tax rates are flat. Not only do poor people consume a higher fraction of their income, sales taxes aren’t scaled by income. You can play around with both of these adjustment factors based on what you want to see. There is no “correct” number. Low income families might pay no income tax and property taxes only indirectly, so sales tax is really the only tax that matters for them.\nThe tax pain index can be crudely interpreted as the fraction of a high earner’s income that will be paid in just state taxes. I call it an “index” because it can also be interpreted as a comparison of the relative tax burden across states for all wage earners.\n\n# judge how to weight realized cost of sales and property relative to income tax.\nsales_adj    = 0.3 # assume we spend 30% of our taxable income on items subject to sales tax.\nproperty_adj = 1.0 # assume median income tax liability is about equal to the property tax on the median home. \n\n# use these adjustments to create ranking that we will use to color the markers in the plot.\n# the sum of the adjusted values is a *rough* guide to the total tax burden.\n\nstate_rates_adj &lt;- state_rates %&gt;% \n   mutate(tax_pain = income_tax + (sales_tax * sales_adj) + (property_tax * property_adj)) %&gt;% \n   arrange(desc(tax_pain))\n\nknitr::kable(state_rates_adj[1:10,])\n\n\n\n\nstate\nincome_tax\nsales_tax\nproperty_tax\ntax_pain\n\n\n\n\nCalifornia\n13.30\n7.25\n0.77\n16.245\n\n\nNew Jersey\n11.25\n6.63\n2.44\n15.679\n\n\nNew York\n10.69\n4.00\n1.68\n13.570\n\n\nMinnesota\n9.85\n6.88\n1.15\n13.064\n\n\nHawaii\n11.00\n4.00\n0.27\n12.470\n\n\nVermont\n8.75\n6.00\n1.83\n12.380\n\n\nIowa\n8.75\n6.00\n1.53\n12.080\n\n\nMaryland\n8.60\n6.00\n1.10\n11.500\n\n\nOregon\n10.28\n0.00\n1.04\n11.320\n\n\nDistrict of Columbia\n8.95\n6.00\n0.55\n11.300\n\n\n\n\n\n\nstate_rates_adj %&gt;% \n   # reorder the state factor levels so they display in order of tax pain, not alphabetically\n   mutate(state = fct_reorder(state,tax_pain)) %&gt;% \n   ggplot(aes(state,tax_pain)) + geom_col() + \n   labs(title = \"Cumulative Impact of State Taxes\",\n        subtitle = \"Income, Sales and Property\",\n        x = \"State\",\n        y = '\"Tax Pain\" Index') + \n   theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))\n\n\n\n\nNo big surprises here. Florida, good. California, bad. Seeing Vermont at the high tax end while New Hampshire is at the low end is interesting. The two states are about the same size and have the same climate. The low tax state has over twice the population and a 33% higher median income. Just sayin’….\nWe would like to visualize the interplay of the three tax vectors and a 3D scatterplot is ideal for this. Further, the plotly package lets us interactively rotate the plot, which is critical for perceiving the 3D volume on a 2D surface. There are a lot of gratuitous uses of 3D visualization out there. This is one instance where 3D really adds to our understanding.\n\n# Create 3d animated plot of 3 state tax rate dimensions,\n# income, property and sales\nplot_ly(state_rates_adj,x = ~income_tax,\n        y= ~sales_tax,\n        z= ~property_tax,\n        type=\"scatter3d\", \n        mode=\"markers\",\n        color = ~tax_pain,\n        hoverinfo = \"text\",\n        text= ~state) %&gt;% \n   layout(title = \"Major Tax Rates by State\",\n          scene = list(xaxis = list(title = 'Income Tax'),\n                       yaxis = list(title = 'Sales Tax'),\n                       zaxis = list(title = 'Property Tax')))\n\n\n\n\n\nPlay around with dragging the image and you start to appreciate the volume. Each piece of the tax picture gets an axis. The tax pain index is represented by color of the markers. You can quickly see that income tax is still the big driver of tax pain across the nation. New Jersey applies high taxes in all dimensions. California is heavily skewed to income tax but is comparatively low in the property tax dimension.\nNevada is a great state to live in if you have income and property. The state gets about a third of its revenue from out-of-state tourists who are spending liberally. Gambling is big, obviously, but a high sales tax is a way to get revenue from visitors while making the tax burden lighter on residents. As we know, sales taxes are regressive so, at first glance, the poor residents of Nevada might be the unintended losers from this scheme. Fortunately, Nevada lightens the relative burden on the poor by exempting drugs and groceries from sales tax.\nAnother great place to live if you hate taxes is in Washington State, on the Oregon border. Washington levies no income tax and Oregon levies no sales tax. I was surprised to see, in a quick Google maps search, no evidence that big box retailers shun the Washington side of the border. In theory, if an Oregon business knows you live in Washington they are supposed to charge taxes (Ha!). Across the border, Oregon residents could avoid paying sales tax in Washington by flashing an Oregon ID but that ended in the summer of 2019.\nFinally, Alaska is the most tax-friendly state overall with low taxes in all dimensions. The state goes even further, though. Oil revenues go into a fund which pays a cash dividend to every resident, every year. Most recently it was $1,600 so some residents, in effect, receive taxes from the state. So, move there."
  },
  {
    "objectID": "posts/2019-12-08-state-taxes-it-s-not-just-about-income/2019-12-08-state-taxes-it-s-not-just-about-income.html#which-states-impose-the-most-tax-pain",
    "href": "posts/2019-12-08-state-taxes-it-s-not-just-about-income/2019-12-08-state-taxes-it-s-not-just-about-income.html#which-states-impose-the-most-tax-pain",
    "title": "State Taxes: It’s not just about Income",
    "section": "",
    "text": "Much of the discussion around tax burdens focuses on income taxes but, at the state level, that leaves out two other big sources of tax liability, sales and property taxes. Here we’ll quickly look at the interplay of all three taxes in a graphical way. This can inform our thinking about how attractive it is to live in each state and on public policy questions involving tax fairness. The plotly package lets us easily create an interactive 3D scatter plot that is uniquely useful to visualize this.\nSales taxes vary greatly by state but, for lower income people, might be the biggest tax burden. Indeed, since low-income families spend a larger fraction of their income, these taxes are “regressive” since the relative burden grows as income falls. Income taxes are typically “progressive” since, in most states, the rate grows with income levels. Property taxes aren’t directly levied on renters but the landlords pass the tax through via higher rents, so everyone pays. Let’s take a quick look at how tax rates vary by state and category.\nThe tax data was found in three different places:\n\nIncome tax rates from https://taxfoundation.org/state-individual-income-tax-rates-brackets-2019/\nProperty tax Rates from https://wallethub.com/edu/states-with-the-highest-and-lowest-property-taxes/11585/\nSales Tax Rates https://www.salestaxinstitute.com/resources/rates\n\nI make some choices in how to present the data. First of all, I use the top marginal rates, so this represents the “worst-case” tax burden. It should be representative of the overall tax structure and useful to compare across states. Next, I add average municipal income taxes computed by the Tax Foundation for each state to the state income tax rate. If you live in New York City, this will substantially understate your tax burden and overstate it elsewhere. Some municipalities levy sales taxes as well but I do NOT include these because they vary so widely and we don’t have all day. Also, municipalities love to tax people who can’t vote, like out of towners, with hotel and rental car taxes. These would not affect your view of where to live. How about excise taxes on gasoline, cigarettes, etc? Not included.\nI already combined the data from each source with the adjustments mentioned above into a single CSV file. Load it with the required libraries.\n\nsuppressPackageStartupMessages(library(tidyverse))\nsuppressPackageStartupMessages(library(plotly))\n\nstate_rates &lt;- read_csv(\"data/state_rate.csv\",col_types = \"fnnn\") \n\nLet’s take a quick look at the summary statistics.\n\nsummary(state_rates[,2:4])\n\n   income_tax       sales_tax      property_tax  \n Min.   : 0.000   Min.   :0.000   Min.   :0.270  \n 1st Qu.: 4.925   1st Qu.:4.375   1st Qu.:0.730  \n Median : 5.950   Median :6.000   Median :0.980  \n Mean   : 5.835   Mean   :5.062   Mean   :1.119  \n 3rd Qu.: 7.190   3rd Qu.:6.250   3rd Qu.:1.550  \n Max.   :13.300   Max.   :7.250   Max.   :2.440  \n\n\nSome states have no personal income tax at all but have to raise revenue somehow. Most commonly, sales tax forms a big part of the budget. Is there a pattern where lower income tax rates correlate with higher sales or property taxes? A correlation matrix provides a quick check.\n\nknitr::kable(cor(state_rates[2:4]))\n\n\n\n\n\nincome_tax\nsales_tax\nproperty_tax\n\n\n\n\nincome_tax\n1.0000000\n0.0292638\n0.1074844\n\n\nsales_tax\n0.0292638\n1.0000000\n0.1115520\n\n\nproperty_tax\n0.1074844\n0.1115520\n1.0000000\n\n\n\n\n\nIt doesn’t look like there is any relationship.\nTax rates are not the same thing as cash out of pocket. As mentioned above, several issues affect the translation of rates to dollars. Ideally, we would like to know which states are the most expensive to live in, tax-wise. We don’t care which pocket it comes out of but we have to make assumptions.\nLet’s add adjustment factors for the impact of sales and property taxes relative to income taxes. This will let us add all three together to come up with a “tax pain” index. In theory, property taxes are levied according to a percentage of the value of the home. But there are complex formulas that go beyond just the published “rate.” In New York, it turns out that the median property tax bill is roughly equal to the median income tax liability, so I chose an adjustment factor of 1.0. How much of your taxable income is spent on consumption of things that sales tax is levied on? As mentioned above, low earners typically live hand-to-mouth. Affluent people can save more for deferred consumption, philanthropy or passing to heirs. I chose to assume 30% of household income is spent where sales taxes apply. Also note that sales tax rates are flat. Not only do poor people consume a higher fraction of their income, sales taxes aren’t scaled by income. You can play around with both of these adjustment factors based on what you want to see. There is no “correct” number. Low income families might pay no income tax and property taxes only indirectly, so sales tax is really the only tax that matters for them.\nThe tax pain index can be crudely interpreted as the fraction of a high earner’s income that will be paid in just state taxes. I call it an “index” because it can also be interpreted as a comparison of the relative tax burden across states for all wage earners.\n\n# judge how to weight realized cost of sales and property relative to income tax.\nsales_adj    = 0.3 # assume we spend 30% of our taxable income on items subject to sales tax.\nproperty_adj = 1.0 # assume median income tax liability is about equal to the property tax on the median home. \n\n# use these adjustments to create ranking that we will use to color the markers in the plot.\n# the sum of the adjusted values is a *rough* guide to the total tax burden.\n\nstate_rates_adj &lt;- state_rates %&gt;% \n   mutate(tax_pain = income_tax + (sales_tax * sales_adj) + (property_tax * property_adj)) %&gt;% \n   arrange(desc(tax_pain))\n\nknitr::kable(state_rates_adj[1:10,])\n\n\n\n\nstate\nincome_tax\nsales_tax\nproperty_tax\ntax_pain\n\n\n\n\nCalifornia\n13.30\n7.25\n0.77\n16.245\n\n\nNew Jersey\n11.25\n6.63\n2.44\n15.679\n\n\nNew York\n10.69\n4.00\n1.68\n13.570\n\n\nMinnesota\n9.85\n6.88\n1.15\n13.064\n\n\nHawaii\n11.00\n4.00\n0.27\n12.470\n\n\nVermont\n8.75\n6.00\n1.83\n12.380\n\n\nIowa\n8.75\n6.00\n1.53\n12.080\n\n\nMaryland\n8.60\n6.00\n1.10\n11.500\n\n\nOregon\n10.28\n0.00\n1.04\n11.320\n\n\nDistrict of Columbia\n8.95\n6.00\n0.55\n11.300\n\n\n\n\n\n\nstate_rates_adj %&gt;% \n   # reorder the state factor levels so they display in order of tax pain, not alphabetically\n   mutate(state = fct_reorder(state,tax_pain)) %&gt;% \n   ggplot(aes(state,tax_pain)) + geom_col() + \n   labs(title = \"Cumulative Impact of State Taxes\",\n        subtitle = \"Income, Sales and Property\",\n        x = \"State\",\n        y = '\"Tax Pain\" Index') + \n   theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))\n\n\n\n\nNo big surprises here. Florida, good. California, bad. Seeing Vermont at the high tax end while New Hampshire is at the low end is interesting. The two states are about the same size and have the same climate. The low tax state has over twice the population and a 33% higher median income. Just sayin’….\nWe would like to visualize the interplay of the three tax vectors and a 3D scatterplot is ideal for this. Further, the plotly package lets us interactively rotate the plot, which is critical for perceiving the 3D volume on a 2D surface. There are a lot of gratuitous uses of 3D visualization out there. This is one instance where 3D really adds to our understanding.\n\n# Create 3d animated plot of 3 state tax rate dimensions,\n# income, property and sales\nplot_ly(state_rates_adj,x = ~income_tax,\n        y= ~sales_tax,\n        z= ~property_tax,\n        type=\"scatter3d\", \n        mode=\"markers\",\n        color = ~tax_pain,\n        hoverinfo = \"text\",\n        text= ~state) %&gt;% \n   layout(title = \"Major Tax Rates by State\",\n          scene = list(xaxis = list(title = 'Income Tax'),\n                       yaxis = list(title = 'Sales Tax'),\n                       zaxis = list(title = 'Property Tax')))\n\n\n\n\n\nPlay around with dragging the image and you start to appreciate the volume. Each piece of the tax picture gets an axis. The tax pain index is represented by color of the markers. You can quickly see that income tax is still the big driver of tax pain across the nation. New Jersey applies high taxes in all dimensions. California is heavily skewed to income tax but is comparatively low in the property tax dimension.\nNevada is a great state to live in if you have income and property. The state gets about a third of its revenue from out-of-state tourists who are spending liberally. Gambling is big, obviously, but a high sales tax is a way to get revenue from visitors while making the tax burden lighter on residents. As we know, sales taxes are regressive so, at first glance, the poor residents of Nevada might be the unintended losers from this scheme. Fortunately, Nevada lightens the relative burden on the poor by exempting drugs and groceries from sales tax.\nAnother great place to live if you hate taxes is in Washington State, on the Oregon border. Washington levies no income tax and Oregon levies no sales tax. I was surprised to see, in a quick Google maps search, no evidence that big box retailers shun the Washington side of the border. In theory, if an Oregon business knows you live in Washington they are supposed to charge taxes (Ha!). Across the border, Oregon residents could avoid paying sales tax in Washington by flashing an Oregon ID but that ended in the summer of 2019.\nFinally, Alaska is the most tax-friendly state overall with low taxes in all dimensions. The state goes even further, though. Oil revenues go into a fund which pays a cash dividend to every resident, every year. Most recently it was $1,600 so some residents, in effect, receive taxes from the state. So, move there."
  },
  {
    "objectID": "posts/2020-01-15-what-do-the-ramones-want/2020-01-15-what-do-the-ramones-want.html",
    "href": "posts/2020-01-15-what-do-the-ramones-want/2020-01-15-what-do-the-ramones-want.html",
    "title": "What Do The Ramones Want?",
    "section": "",
    "text": "Recently I saw a tweet that shared this hilarious poster of Ramones “wants”. Very cool, but how accurate is it? I asked the graphic designer and he says he took some artistic license, as he should! You may accuse me of being that pedantic “Comic Book Guy” from “The Simpsons” but, when I saw it, I immediately wondered how I could tally these Ramones lyrics myself or, rather, get R to do it for me. The tidytext mining package makes short work of the project, as we’ll see."
  },
  {
    "objectID": "posts/2020-01-15-what-do-the-ramones-want/2020-01-15-what-do-the-ramones-want.html#why-the-ramones",
    "href": "posts/2020-01-15-what-do-the-ramones-want/2020-01-15-what-do-the-ramones-want.html#why-the-ramones",
    "title": "What Do The Ramones Want?",
    "section": "\nWhy the RAMONES?\n",
    "text": "Why the RAMONES?\n\n\nThe Ramones hold a special place in my heart. As a college student in central Ohio, in the late 70s, my frat brothers and I were huge fans. We were completely ridiculous of course. Preppy nerds bobbing to “Beat on the Brat” The Sigma Chis thought we were idiots (Lynrd Skynrd? Come on! History has judged). I never saw the Ramones at CBGBs but when we heard they were coming to a cowboy bar on notorious High St. across from Ohio State, we were thrilled. I blew off studying for a Poly-Sci mid-term the next day. I got my worst college grade ever but it was totally worth it. I said my future self would thank me and I was right!\n\n\nWithout any further adieu, hey, ho, let’s go!"
  },
  {
    "objectID": "posts/2020-01-15-what-do-the-ramones-want/2020-01-15-what-do-the-ramones-want.html#load-packages",
    "href": "posts/2020-01-15-what-do-the-ramones-want/2020-01-15-what-do-the-ramones-want.html#load-packages",
    "title": "What Do The Ramones Want?",
    "section": "\nLoad Packages\n",
    "text": "Load Packages\n\n\nFirst, load packages.\n\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(rvest)\nlibrary(reshape2)\nlibrary(wordcloud)\nlibrary(scales)\nlibrary(genius)\nlibrary(ggthemr)\nlibrary(ggrepel)\n\nggthemr(\"earth\",type=\"outer\")"
  },
  {
    "objectID": "posts/2020-01-15-what-do-the-ramones-want/2020-01-15-what-do-the-ramones-want.html#get-lyrics-from-genius-api",
    "href": "posts/2020-01-15-what-do-the-ramones-want/2020-01-15-what-do-the-ramones-want.html#get-lyrics-from-genius-api",
    "title": "What Do The Ramones Want?",
    "section": "\nGet Lyrics from Genius API\n",
    "text": "Get Lyrics from Genius API\n\n\nHave you ever spent an enormous amount of time on something, only to discover there was a much simpler way? Yes, yes you have. For this project we need a source of Ramones lyrics. Originally, I built a very finicky web scraping routine to get lyrics from a site I commonly use in my browser. I coaxed it to get all of the lyrics but I didn’t want to share it in this post because you would likely not be able to get it to work smoothly. Months passed then it occurred to me to Google “lyrics api” and, “viola!”, I found http://genius.com and the genius R package by Josiah Parry, available on CRAN. Access to the lyric API does require a free application access token. You can generate one here. I will leave installing the token called GENIUS_API_TOKEN into your R environment as an exercise for the reader. There are numerous tutorials on this subject around.\n\n\nAs always, we will be working in the tidyverse veracular. First we build a data frame of album names and the year of release. This gets fed into a single function genius::add_genius which returns all the lyrics. I’m embarressed to think about the tangled mess of web scraping code I was previously using.\n\n\nAs usual, we check to see if the file with all the downloaded data is already available so, as we iterate versions of our project, we don’t hit the API over and over.\n\n#make sure you have a Genius API token\n# my token is in the .Reviron file\n\n# All the studio albums\nramones_albums &lt;- tribble(\n  ~album, ~year,\n  \"Ramones\", 1976,\n  \"Leave Home\", 1977,\n  \"Rocket To Russia\", 1977,\n  \"Road To Ruin\", 1978,\n  \"End Of The Century\", 1980,\n  \"Pleasant Dreams\", 1981,\n  \"Subterranean Jungle\", 1983,\n  \"Too Tough To Die\", 1984,\n  \"Animal Boy\", 1986,\n  \"Halfway To Sanity\",1987,\n  \"Brain Drain\",1989,\n  \"Mondo Bizarro\",1992,\n  \"Acid Eaters\",1993,\n  \"¡Adios Amigos!\",1995\n)\nartist_albums &lt;- ramones_albums %&gt;% \n  mutate(artist=\"Ramones\") %&gt;% \n  select(artist,album) %&gt;%\n  {.}\n\nif (file.exists(\"data/ramones_lyrics_genius.rdata\")){\n  load(\"data/ramones_lyrics_genius.rdata\")\n} else {\n  ramones_lyrics_genius &lt;- genius::add_genius(artist_albums,artist,album)\n  save(ramones_lyrics_genius,file=\"data/ramones_lyrics_genius.rdata\")\n}"
  },
  {
    "objectID": "posts/2020-01-15-what-do-the-ramones-want/2020-01-15-what-do-the-ramones-want.html#put-lyics-in-tidytext-form",
    "href": "posts/2020-01-15-what-do-the-ramones-want/2020-01-15-what-do-the-ramones-want.html#put-lyics-in-tidytext-form",
    "title": "What Do The Ramones Want?",
    "section": "\nPut Lyics in Tidytext Form\n",
    "text": "Put Lyics in Tidytext Form\n\n\nMost projects require a huge amount of data wrangling before we can get any real analysis done. This project is pretty clean. We are already nearly good to go. Further, tidytext makes the remaining manipulation of the data soooo easy! To wit, let’s tokenize the data into individual words.\n\nramones_lyrics &lt;- ramones_lyrics_genius\n#make factor to keep albums in order of issue date\nramones_lyrics$album &lt;- as_factor(ramones_lyrics$album)\nramones_albums$album &lt;- as_factor(ramones_albums$album)\nramones_lyrics &lt;- right_join(ramones_lyrics,ramones_albums,by=\"album\")\nlyric_words &lt;- ramones_lyrics  %&gt;% \n  unnest_tokens(word,lyric) %&gt;%\n  rename(song_name=track_title)\n\nSee, I said it was easy."
  },
  {
    "objectID": "posts/2020-01-15-what-do-the-ramones-want/2020-01-15-what-do-the-ramones-want.html#how-needy-are-the-ramones",
    "href": "posts/2020-01-15-what-do-the-ramones-want/2020-01-15-what-do-the-ramones-want.html#how-needy-are-the-ramones",
    "title": "What Do The Ramones Want?",
    "section": "\nHow Needy Are The Ramones?\n",
    "text": "How Needy Are The Ramones?\n\n\nOut of 193 songs on all their studio albums, 16 mention wanting or not wanting in the title. “I Wanna” songs are a thing with the Ramones.\n\nwant_phrases &lt;- \"Wanna|Want\"\nramones_lyrics %&gt;% \n  select(album, track_title) %&gt;% \n  distinct() %&gt;%\n  filter(str_detect(track_title,want_phrases)) %&gt;% \n  {.}\n## # A tibble: 16 x 2\n##    album             track_title                                    \n##    &lt;fct&gt;             &lt;chr&gt;                                          \n##  1 Ramones           I Wanna Be Your Boyfriend                      \n##  2 Ramones           Now I Wanna Sniff Some Glue                    \n##  3 Ramones           I Don't Wanna Go Down to the Basement          \n##  4 Ramones           I Don't Wanna Walk Around with You             \n##  5 Leave Home        Now I Wanna Be a Good Boy                      \n##  6 Rocket To Russia  Do You Wanna Dance?                            \n##  7 Rocket To Russia  I Wanna Be Well                                \n##  8 Road To Ruin      I Just Want To Have Something To Do            \n##  9 Road To Ruin      I Wanted Everything                            \n## 10 Road To Ruin      I Don't Want You                               \n## 11 Road To Ruin      I Wanna Be Sedated                             \n## 12 Road To Ruin      I Want You Around (Ed Stasium Version)         \n## 13 Pleasant Dreams   We Want the Airwaves                           \n## 14 Halfway To Sanity I Wanna Live                                   \n## 15 Brain Drain       Merry Christmas (I Don't Want To Fight Tonight)\n## 16 ¡Adios Amigos!    I Don't Want to Grow Up"
  },
  {
    "objectID": "posts/2020-01-15-what-do-the-ramones-want/2020-01-15-what-do-the-ramones-want.html#do-some-sentiment-analysis",
    "href": "posts/2020-01-15-what-do-the-ramones-want/2020-01-15-what-do-the-ramones-want.html#do-some-sentiment-analysis",
    "title": "What Do The Ramones Want?",
    "section": "\nDo Some Sentiment Analysis\n",
    "text": "Do Some Sentiment Analysis\n\n\nBefore we look at the what the Ramones want we might as well run the, now routine, sentiment analysis you may have learned about from Julia Silge and David Robinson here. The Ramones are no Jane Austen but, hey, they have feelings, ya know? SAD NOTE: They “had” feelings. All the original four are dead.\n\n\nTo start our sentiment analysis let’s pull out stop words that don’t provide much context and label all the words in the “bing” sentiment database as either positive or negative.\n\nlyric_words_cleaned &lt;- lyric_words %&gt;% anti_join(get_stopwords(),by=\"word\")\n\n#quick sentiment analysis\npositive &lt;- get_sentiments(\"bing\") %&gt;%\n  filter(sentiment == \"positive\")\n\nnegative &lt;- get_sentiments(\"bing\") %&gt;%\n  filter(sentiment == \"negative\")\n\nlyric_words_cleaned %&gt;%\n  semi_join(positive,by=\"word\") %&gt;%\n  group_by(song_name) %&gt;% \n  count(word) %&gt;% \n  group_by(song_name) %&gt;% \n  tally(sort = TRUE,name=\"Happy Words\")\n## # A tibble: 162 x 2\n##    song_name                   `Happy Words`\n##    &lt;chr&gt;                               &lt;int&gt;\n##  1 The Crusher                            10\n##  2 It's Gonna Be Alright                   9\n##  3 Palisades Park                          9\n##  4 Too Tough to Die                        9\n##  5 Censorshit                              8\n##  6 I Don't Want to Grow Up                 8\n##  7 In the Park                             8\n##  8 My Back Pages                           8\n##  9 Gimme Gimme Shock Treatment             7\n## 10 Glad to See You Go                      7\n## # ... with 152 more rows\nlyric_words_cleaned %&gt;%\n  semi_join(negative,by=\"word\") %&gt;%\n  group_by(song_name) %&gt;% \n  count(word) %&gt;% \n  group_by(song_name) %&gt;% \n  tally(sort = TRUE,name=\"Sad Words\")\n## # A tibble: 156 x 2\n##    song_name                       `Sad Words`\n##    &lt;chr&gt;                                 &lt;int&gt;\n##  1 I'm Not Afraid of Life                   21\n##  2 Endless Vacation                         17\n##  3 Don't Bust My Chops                      16\n##  4 Love Kills                               15\n##  5 Wart Hog                                 13\n##  6 My Back Pages                            12\n##  7 Cretin Family                            10\n##  8 Something to Believe In                  10\n##  9 Anxiety                                   9\n## 10 Howling at the Moon (Sha-La-La)           9\n## # ... with 146 more rows\n\nNow we change the sign of the count of negative words so we can get the net balance of happy vs. sad words.\n\nlyric_words_cleaned %&gt;%\n  inner_join(get_sentiments(\"bing\"),by=\"word\") %&gt;%\n  group_by(song_name) %&gt;% \n  count(sentiment,sort=TRUE) %&gt;% \n  mutate(n = ifelse(sentiment == \"negative\", -n, n)) %&gt;%\n  group_by(song_name) %&gt;% \n  summarise(net_sentiment=sum(n)) %&gt;% \n  filter(abs(net_sentiment) &gt; 10) %&gt;%\n  mutate(song_name = reorder(song_name, net_sentiment)) %&gt;%\n  mutate(sentiment=ifelse(net_sentiment&lt;0,\"Negative\",\"Positive\")) %&gt;% \n  ggplot(aes(song_name, net_sentiment, fill = sentiment)) +\n  geom_col() +\n  coord_flip() +\n  labs(title=\"How Happy are RAMONES Songs?\",\n       y = \"Very Sad &lt;---   ---&gt; Very Happy\",\n       x= \"\") +\n  scale_fill_manual(values = c(\"red\",\"darkgrey\"))+\n  theme(axis.text.y =  element_text(size=7,hjust=1))"
  },
  {
    "objectID": "posts/2020-01-15-what-do-the-ramones-want/2020-01-15-what-do-the-ramones-want.html#sentiment-over-time",
    "href": "posts/2020-01-15-what-do-the-ramones-want/2020-01-15-what-do-the-ramones-want.html#sentiment-over-time",
    "title": "What Do The Ramones Want?",
    "section": "\nSentiment Over Time\n",
    "text": "Sentiment Over Time\n\n\nThe average sentiment over the whole lyric corpus is about evenly split between positive and negative words but if we look at sentiment by album we see a gyrating trend with an intersting dip in their middle years.\n\nlyric_words_cleaned %&gt;%\n  inner_join(get_sentiments(\"bing\"),by=\"word\") %&gt;%\n  group_by(album, year) %&gt;% \n  count(sentiment,sort=TRUE) %&gt;% \n  arrange(album) %&gt;% \n  pivot_wider(values_from = n,names_from = sentiment) %&gt;% \n  mutate(fraction_happy = positive/(negative+positive)) %&gt;%\n  ggplot(aes(year,fraction_happy)) + geom_line(color=\"red\") + geom_point(color=\"red\") +\n  labs(title = \"RAMONES Mood Over Time\",\n       y= \"Fraction of Happy Words\",\n       x= \"Album Release Year\") + \n    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n    geom_text_repel(aes(label=album),\n                    color=\"white\",\n                    segment.color = \"white\")\n\n\n\n\nWe can generate word clouds for any album. Their “happiest” album is “Road to Ruin.”\n\n{par(bg=\"black\")\n  lyric_words_cleaned %&gt;%\n    filter(album == \"Road To Ruin\") %&gt;% \n    inner_join(get_sentiments(\"bing\"),by=\"word\") %&gt;%\n    count(word, sentiment, sort = TRUE) %&gt;%\n    acast(word ~ sentiment, value.var = \"n\", fill = 0) %&gt;%\n    #  comparison.cloud(colors = c(\"#F8766D\", \"#00BFC4\"),\n    #                   max.words = 100)\n    comparison.cloud(colors = c(\"red\", \"grey60\"),\n                     max.words = 100,\n                     title.bg.colors=\"grey60\")\n    text(x=1.1,y=0.5,\"RAMONES\",col=\"red\",cex=4,srt=270)\n    text(x=-0.1,y=0.5,\"Road To Ruin\",col=\"grey60\",cex=4,srt=90)\n}\n\n\n\n\n… and their angriest, “Animal Boy.” You start to think there is something to this sentiment analysis stuff when you read the opening of this album’s review at http://allmusic.com:\n\n\n\nAnimal Boy wasn’t a very happy record for the Ramones. Since the release of Too Tough to Die (a slight return to form) nearly two years earlier, the band’s fortunes had gone from bad to worse; interest in the band kept dwindling with every release and the “bruthas” were constantly at each other’s throat.\n\n\n{par(bg=\"black\")\n  lyric_words_cleaned %&gt;%\n    filter(album == \"Animal Boy\") %&gt;% \n    inner_join(get_sentiments(\"bing\"),by=\"word\") %&gt;%\n    count(word, sentiment, sort = TRUE) %&gt;%\n    acast(word ~ sentiment, value.var = \"n\", fill = 0) %&gt;%\n    #  comparison.cloud(colors = c(\"#F8766D\", \"#00BFC4\"),\n    #                   max.words = 100)\n    comparison.cloud(colors = c(\"red\", \"grey60\"),\n                     max.words = 100,\n                     title.bg.colors=\"grey60\")\n    text(x=1.1,y=0.5,\"RAMONES\",col=\"red\",cex=4,srt=270)\n    text(x=-0.1,y=0.5,\"Animal Boy\",col=\"grey60\",cex=4,srt=90)\n}"
  },
  {
    "objectID": "posts/2020-01-15-what-do-the-ramones-want/2020-01-15-what-do-the-ramones-want.html#what-do-the-ramones-want-and-not-want",
    "href": "posts/2020-01-15-what-do-the-ramones-want/2020-01-15-what-do-the-ramones-want.html#what-do-the-ramones-want-and-not-want",
    "title": "What Do The Ramones Want?",
    "section": "\nWhat do the RAMONES want… and not want?\n",
    "text": "What do the RAMONES want… and not want?\n\n\nNow lets find what the Ramones Want. An n-gram is simply a cluster of words of length n. Let’s look at the most common n-grams, which would include the phrases like “I want” and “I wanna.”\n\n\nStart with shortest n-gram that is a complete thought and work up to longer phrases. We take the the shortest phrase that makes sense unless appending more words doesn’t change the frequency. Then we take the longer phrase. For instance if “I wanna steal some money” and “I wanna steal from the rich” both exist we take “I wanna steal” since it would have a higher frequency than either longer phrase. In this case, the only phrase starting with “I wanna steal” is “I wanna steal from the rich” so we use that.\n\nwant_phrases &lt;- \"^(i wanna |i want |we want |we wanna |i wanted |i just want |i just wanna )\"\n\nget_ngrams &lt;- function(lyrics,n,prefixes=\"\"){\n  min_instance = 0\n  lyric_ngram &lt;- lyrics %&gt;% \n    unnest_tokens(ngram,lyric,token = \"ngrams\",n=n) %&gt;% \n    group_by(ngram) %&gt;% \n    filter(str_detect(ngram,prefixes)) %&gt;% \n    count() %&gt;% \n    arrange(desc(n)) %&gt;% \n    filter(n&gt;min_instance) %&gt;% \n    mutate(want=str_remove(ngram,prefixes)) %&gt;% \n   rowid_to_column()\n  return(lyric_ngram)\n  \n}\n\nwant &lt;- ramones_lyrics %&gt;% get_ngrams(5,want_phrases)\nwant\n## # A tibble: 43 x 4\n## # Groups:   ngram [43]\n##    rowid ngram                         n want             \n##    &lt;int&gt; &lt;chr&gt;                     &lt;int&gt; &lt;chr&gt;            \n##  1     1 i want i want i              14 i want i         \n##  2     2 i want to be your            13 to be your       \n##  3     3 i just want to walk           7 to walk          \n##  4     4 i just want to have           6 to have          \n##  5     5 i just want to be             4 to be            \n##  6     6 i wanna be your boyfriend     4 be your boyfriend\n##  7     7 i want to live my             4 to live my       \n##  8     8 i want to run away            4 to run away      \n##  9     9 i want to be a                3 to be a          \n## 10    10 i want you by my              3 you by my        \n## # ... with 33 more rows\n\nWhat a human needs to do is decide which phrases are complete thoughts. We manually select the row numbers to build our ultimate table.\n\n\nRemember what I said before about data wrangling? Well, sure, getting the words was easy. Determining meaningful phrases not (for a computer). If this was Spotify, our AI could figure these out, but this is not Spotify. This is an iterative process of manually inspecting tables of ever-longer n-grams and noting which rows have complete thoughts until we don’t see any sensible new phrases. We run through twice, first for “want” then “don’t want.” We flip the sign on the count of “don’t wants” to negative. I won’t bore you with every iteration so let’s skip ahead. Think of this as the cinematic training montage.\n\n# WANT\n# make \"wanna\" in to \"want to\" which also frequently appears so we get a good count.\nramones_lyrics &lt;- ramones_lyrics %&gt;% mutate(lyric=str_replace_all(lyric,\"wanna\",\"want to\"))\ndo_want &lt;- tibble()\nall_wants &lt;- tibble() # for debugging\n# why make the code below a function, if we only call it once?\n# Since we cumulatively modify all_wants each step is dependent on the prior one executing first\n# this organizes the code into a block that tells future self to execute as a block\nbuild_wants &lt;- function(all_wants) {\n  want_phrases &lt;- \"^(i wanna |i want |we want |we wanna |i wanted |i just want |i just wanna )\"\n  #select the 3-gram phrases that are complete thoughts using manual inspection\n  want &lt;- ramones_lyrics %&gt;% get_ngrams(3,want_phrases)\n  # visually inspect the want variable and select which lines to add to all_wants\n  # pause after each instance of get_ngrams to do this.\n  all_wants &lt;- bind_rows(all_wants,want[c(2,8,11,13),])\n  # move to the 4-gram phrases, etc\n  want &lt;- ramones_lyrics %&gt;% get_ngrams(4,want_phrases)\n  all_wants &lt;- bind_rows(all_wants,want[c(5,6,9,13,14,17,24,28,30,31,37),])\n  want &lt;- ramones_lyrics %&gt;% get_ngrams(5,want_phrases)\n  all_wants &lt;- bind_rows(all_wants,want[c(3,4,6,9,21,22),])\n  want &lt;- ramones_lyrics %&gt;% get_ngrams(6,want_phrases)\n  all_wants &lt;- bind_rows(all_wants,want[c(1,11,12,22,25,28),])\n  want &lt;- ramones_lyrics %&gt;% get_ngrams(7,want_phrases)\n  all_wants &lt;- bind_rows(all_wants,want[c(5,6,7,9,10,12,21),])\n  want &lt;- ramones_lyrics %&gt;% get_ngrams(8,want_phrases)\n  all_wants &lt;- bind_rows(all_wants,want[c(7,3),])\n  return (all_wants)\n}\n\ndo_want &lt;- build_wants(do_want)\ndo_want &lt;- do_want %&gt;% \n  mutate(want=str_to_title(want)) %&gt;% \n  group_by(want) %&gt;% \n  summarise(n=sum(n)) %&gt;% \n  arrange(desc(n))\n\n# DONT'T WANT\ndont_want &lt;- tibble()\nall_wants &lt;- tibble() # for debugging only\nramones_lyrics &lt;- ramones_lyrics %&gt;% mutate(lyric=str_replace_all(lyric,\"wanna\",\"want to\"))\nwant_phrases &lt;- \"^(i don't want |we don't want |i didn't want )\"\nbuild_dont_wants &lt;- function(all_wants) {\n  want &lt;- ramones_lyrics %&gt;% get_ngrams(4,want_phrases)\n  all_wants &lt;- bind_rows(all_wants,want[c(2),])\n  want &lt;- ramones_lyrics %&gt;% get_ngrams(5,want_phrases)\n  all_wants &lt;- bind_rows(all_wants,want[c(3,5,6,7,9,11,15),])\n  want &lt;- ramones_lyrics %&gt;% get_ngrams(6,want_phrases)\n  all_wants &lt;- bind_rows(all_wants,want[c(1,7),])\n  want &lt;- ramones_lyrics %&gt;% get_ngrams(7,want_phrases)\n  all_wants &lt;- bind_rows(all_wants,want[c(2,17),])\n  want &lt;- ramones_lyrics %&gt;% get_ngrams(8,want_phrases)\n  all_wants &lt;- bind_rows(all_wants,want[c(7,8,9,16),])\n  want &lt;- ramones_lyrics %&gt;% get_ngrams(9,want_phrases)\n  all_wants &lt;- bind_rows(all_wants,want[c(3,10,12),])\n  want &lt;- ramones_lyrics %&gt;% get_ngrams(10,want_phrases)\n  #there it is - Pet Sematary!\n  all_wants &lt;- bind_rows(all_wants,want[c(1),])\n}\ndont_want &lt;- build_dont_wants(dont_want)\ndont_want &lt;- dont_want %&gt;%\n  mutate(n = -n) %&gt;% \n  mutate(want=str_to_title(want)) %&gt;% \n  group_by(want) %&gt;%\n  summarise(n=sum(n)) %&gt;% \n  arrange(n)\n\nFinally we put it all together to get what we’re after.\n\nultimate_want &lt;- bind_rows(do_want,dont_want) %&gt;% \n  group_by(want) %&gt;%\n  summarise(n=sum(n)) %&gt;%   \n  mutate(Sentiment = ifelse(n &gt; 0,\"Want\",\"Don't Want\")) %&gt;% \n  arrange(n) %&gt;% \n  {.}\n\np &lt;- ultimate_want %&gt;% mutate(want=reorder(want,n)) %&gt;% \n  filter(abs(n) &gt; 1) %&gt;% \n  ggplot(aes(want,n,fill=Sentiment)) + geom_col()+coord_flip()+\n  labs(title=\"What Do The RAMONES Want?\",\n       y=\"How Much Do The RAMONES Want It?\",\n       x=\"\")\np + \n  scale_fill_manual(values = c(\"red\",\"darkgrey\"))+\n  theme(axis.text.y =  element_text(size=7,hjust=1))"
  },
  {
    "objectID": "posts/2020-01-15-what-do-the-ramones-want/2020-01-15-what-do-the-ramones-want.html#bringing-it-full-circle",
    "href": "posts/2020-01-15-what-do-the-ramones-want/2020-01-15-what-do-the-ramones-want.html#bringing-it-full-circle",
    "title": "What Do The Ramones Want?",
    "section": "\nBringing It Full Circle\n",
    "text": "Bringing It Full Circle\n\n\nSometimes, late at night, after everyone else is asleep, I hide under the covers, open my laptop and look at… pie charts. Ed Tufte says I will go blind if I keep doing it. Still, for the sake of bringing this full circle (ahem) back to the chart that inspired it, let’s make a version of Grayhood’s poster with our data. So it’s not a complete mess, we lump any phrases that occur less than 4 times in “Other.” That takes some of the fun out of things since we lose memorable phrases like “I wanna sniff some glue” which the poster above includes. This is data science, not art. It’s not supposed to be fun! While I use ggplot2 pretty much exclusively, the base R pie plot produces pretty clean results that approximate the style of the poster with no embellishment.\n\ncollapsed_want &lt;- ultimate_want %&gt;%\n  filter(Sentiment==\"Want\") %&gt;%\n  mutate(want = ifelse(n&lt;4,\"Other\",want)) %&gt;%\n  group_by(want) %&gt;% \n  summarise(n=sum(n)) %&gt;% \n  arrange(desc(n)) %&gt;% \n  {.}\n\n with(collapsed_want,\n      pie(n, \n          labels=paste0(as.character(want), \" \", n, \"%\"),\n          col=c(\"brown\",\"red\",\"black\",\"darkblue\",\"pink\",\"purple\"),\n          radius=1,\n          density=30,\n          bg=\"sienna\",\n          main=\"The Ramones Want...\"))\n\n\n\ncollapsed_want &lt;- ultimate_want %&gt;%\n  filter(Sentiment==\"Don't Want\") %&gt;%\n  mutate(n = -n) %&gt;% \n  mutate(want = ifelse(n&lt;2,\"Other\",want)) %&gt;%\n  group_by(want) %&gt;% \n  summarise(n=sum(n)) %&gt;% \n  arrange(desc(n)) %&gt;% \n  {.}\n\n with(collapsed_want,\n      pie(n, \n          labels=paste0(as.character(want), \" \", n, \"%\"),\n          col=c(\"brown\",\"red\",\"black\",\"darkblue\",\"pink\",\"purple\"),\n          radius=1,\n          density=30,\n          bg=\"sienna\",\n          main=\"The RAMONES Don't Want...\"))\n\n\n\n\nIt must be comforting to know the Ramones want you more than anything but they aren’t going down to the basement with you. Okay, so maybe this was a little fun. Thanks for reading!"
  },
  {
    "objectID": "posts/2020-12-06-covid-cases-vs-deaths/2020-12-06-covid-cases-vs-deaths.html",
    "href": "posts/2020-12-06-covid-cases-vs-deaths/2020-12-06-covid-cases-vs-deaths.html",
    "title": "Covid Cases vs. Deaths",
    "section": "",
    "text": "Introduction\n\n\nI have a macabre fascination with tracking the course of the COVID-19 pandemic. I suspect there are two reasons for this. One, by delving into the numbers I imagine I have some control over this thing. Second, it feels like lighting a candle to show that science can reveal truth at a time when the darkness of anti-science is creeping across the land.\n\n\nThe purpose of this project is, as usual, twofold. First, to explore an interesting data science question and, second, to explore some techniques and packages in the R universe. We will be looking at the relationship of COVID-19 cases to mortality. What is the lag between a positive case and a death? How does that vary among states? How has it varied as the pandemic has progressed? This is an interesting project because is combines elements of time series forecasting and dependent variable prediction.\n\n\nI have been thinking about how to measure mortality lags for a while now. What prompted to do a write-up was discovering a new function in Matt Dancho’s timetk package, tk_augment_lags, which makes short work of building multiple lags. Not too long ago, managing models for multiple lags and multiple states would have been a bit messy. The emerging “tidy models” framework from RStudio using “list columns” is immensely powerful for this sort of thing. It’s great to reduce so much analysis into so few lines of code.\n\n\nThis was an exciting project because I got some validation of my approach. I am NOT an epidemiologist or a professional data scientist. None of the results I show here should be considered authoritative. Still, while I was working on this project I saw this article in the “Wall Street Journal” which referenced the work by Dr. Trevor Bedford, an epidemiologist at the University of Washington. He took the same approach I did and got about the same result.\n\n\n\n\nAquire and Clean Data\n\n\nThere is no shortage of data to work with. Here we will use the NY Times COVID tracking data set which is updated daily. The package covid19nytimes lets us refresh the data on demand.\n\n# correlate deaths and cases by state\nlibrary(tidyverse)\nlibrary(covid19nytimes)\nlibrary(timetk)\nlibrary(lubridate)\nlibrary(broom)\nlibrary(knitr)\n\n# source https://github.com/nytimes/covid-19-data.git\nus_states_long &lt;- covid19nytimes::refresh_covid19nytimes_states()\n\n# if link is broken\n#load(\"../data/us_states_long.rdata\")\n\n# use data from November 15 to stay consistent with text narrative\ncutoff_start &lt;- as.Date(\"2020-03-15\") # not widespread enough until then\ncutoff_end &lt;- max(us_states_long$date) -7 # discard last week since there are reporting lags\n\nus_states_long &lt;- us_states_long %&gt;% filter(date &gt;= cutoff_start)\nus_states_long &lt;- us_states_long %&gt;% filter(date &lt;= cutoff_end)\n# Remove tiny territories\nterritories &lt;- c(\"Guam\",\"Northern Mariana Islands\")\nus_states_long &lt;- us_states_long %&gt;% filter(!(location %in% territories))\nsave(us_states_long,file=\"us_states_long.rdata\")\nus_states_long %&gt;% head() %&gt;% kable()\n\n\n\n\n\ndate\n\n\nlocation\n\n\nlocation_type\n\n\nlocation_code\n\n\nlocation_code_type\n\n\ndata_type\n\n\nvalue\n\n\n\n\n\n\n2020-11-28\n\n\nAlabama\n\n\nstate\n\n\n01\n\n\nfips_code\n\n\ncases_total\n\n\n244993\n\n\n\n\n2020-11-28\n\n\nAlabama\n\n\nstate\n\n\n01\n\n\nfips_code\n\n\ndeaths_total\n\n\n3572\n\n\n\n\n2020-11-28\n\n\nAlaska\n\n\nstate\n\n\n02\n\n\nfips_code\n\n\ncases_total\n\n\n31279\n\n\n\n\n2020-11-28\n\n\nAlaska\n\n\nstate\n\n\n02\n\n\nfips_code\n\n\ndeaths_total\n\n\n115\n\n\n\n\n2020-11-28\n\n\nArizona\n\n\nstate\n\n\n04\n\n\nfips_code\n\n\ncases_total\n\n\n322774\n\n\n\n\n2020-11-28\n\n\nArizona\n\n\nstate\n\n\n04\n\n\nfips_code\n\n\ndeaths_total\n\n\n6624\n\n\n\n\n\n\nThe NY Times data is presented in a “long” format. When we start modeling, long will suit us well but first we have to add features to help us and that will require pivoting to wide, adding features and then back to long. The daily data is so irregular the first features we will add are 7-day moving averages to smooth the series. We’ll also do a nation-level analysis first so we aggregate the state data as well.\n\n# Create rolling average changes\n# pivot wider\n# this will also be needed when we create lags\nus_states &lt;- us_states_long %&gt;%\n  # discard dates before cases were tracked.\n  filter(date &gt; as.Date(\"2020-03-01\")) %&gt;% \n  pivot_wider(names_from=\"data_type\",values_from=\"value\") %&gt;% \n  rename(state=location) %&gt;%\n  select(date,state,cases_total,deaths_total) %&gt;%\n  mutate(state = as_factor(state)) %&gt;% \n  arrange(state,date) %&gt;% \n  group_by(state) %&gt;%\n  #smooth the data with 7 day moving average\n  mutate(cases_7day = (cases_total - lag(cases_total,7))/7) %&gt;%\n  mutate(deaths_7day = (deaths_total - lag(deaths_total,7))/7) %&gt;%\n  {.}\n\n# national analysis\n# ----------------------------------------------\n# aggregate state to national\nus &lt;- us_states %&gt;%\n  group_by(date) %&gt;% \n  summarize(across(.cols=where(is.double),\n                   .fns = function(x)sum(x,na.rm = T),\n                   .names=\"{.col}\"))\n\nus[10:20,] %&gt;% kable()\n\n\n\n\n\ndate\n\n\ncases_total\n\n\ndeaths_total\n\n\ncases_7day\n\n\ndeaths_7day\n\n\n\n\n\n\n2020-03-24\n\n\n53906\n\n\n784\n\n\n6857.571\n\n\n95.28571\n\n\n\n\n2020-03-25\n\n\n68540\n\n\n1053\n\n\n8599.714\n\n\n127.28571\n\n\n\n\n2020-03-26\n\n\n85521\n\n\n1352\n\n\n10448.571\n\n\n162.85714\n\n\n\n\n2020-03-27\n\n\n102847\n\n\n1769\n\n\n12121.286\n\n\n213.14286\n\n\n\n\n2020-03-28\n\n\n123907\n\n\n2299\n\n\n14199.143\n\n\n277.00000\n\n\n\n\n2020-03-29\n\n\n142426\n\n\n2717\n\n\n15625.714\n\n\n322.85714\n\n\n\n\n2020-03-30\n\n\n163893\n\n\n3367\n\n\n17202.429\n\n\n398.42857\n\n\n\n\n2020-03-31\n\n\n188320\n\n\n4302\n\n\n19202.000\n\n\n502.57143\n\n\n\n\n2020-04-01\n\n\n215238\n\n\n5321\n\n\n20956.857\n\n\n609.71429\n\n\n\n\n2020-04-02\n\n\n244948\n\n\n6537\n\n\n22775.286\n\n\n740.71429\n\n\n\n\n2020-04-03\n\n\n277264\n\n\n7927\n\n\n24916.714\n\n\n879.71429\n\n\n\n\n\n\n\n\nExploratory Data Analysis\n\n\nWe might be tempted to simply regress deaths vs. cases but a scatter plot shows us that would not be satisfactory. As it turns out, the relationship of cases and deaths is strongly conditioned on date. This reflects the declining mortality rate as we have come to better understand the disease.\n\n# does a simple scatterplot tell us anything \n# about the relationship of deaths to cases? No.\nus %&gt;% \n  ggplot(aes(deaths_7day,cases_7day)) + geom_point() +\n  labs(title = \"Not Useful\",\n       caption = \"Source: NY Times, Arthur Steinmetz\")\n\n\n\n\nWe can get much more insight plotting smoothed deaths and cases over time. It is generally bad form to use two different y axes on a single plot but but this example adds insight. A couple of observations are obvious. First when cases start to rise, deaths follow with a lag. Second, we have had three spikes in cases so far and in each successive instance the mortality has risen by a smaller amount. This suggests that, thankfully, we are getting better at treating this disease. It is NOT a function of increased testing because positivity rates have not been falling.\n\n#visualize the relationship between rolling average of weekly cases and deaths\ncoeff &lt;- 30\nus %&gt;% \n  ggplot(aes(date,cases_7day)) + geom_line(color=\"orange\") +\n  theme(legend.position = \"none\") +\n  geom_line(aes(x=date,y=deaths_7day*coeff),color=\"red\") +\n  scale_y_continuous(labels = scales::comma,\n                     name = \"Cases\",\n                     sec.axis = sec_axis(deaths_7day~./coeff,\n                                         name=\"Deaths\",\n                                         labels = scales::comma)) +\n  theme(\n    axis.title.y = element_text(color = \"orange\", size=13),\n    axis.title.y.right = element_text(color = \"red\", size=13)\n  ) +\n  labs(title =  \"U.S. Cases vs. Deaths\",\n       subtitle = \"7-Day Average\",\n       caption = \"Source: NY Times, Arthur Steinmetz\",\n       x = \"Date\")\n\n This illustrates a problem for any modeling we might do.It looks like the more cases surge, the less the impact on deaths. This is NOT a valid conclusion. A simple regression of deaths vs. cases and time shows the passage of time has more explanatory power than cases in predicting deaths so we have to take that into account.\n\n# passage of time affects deaths more than cases\nlm(deaths_7day~cases_7day+date,data=us) %&gt;% tidy()\n## # A tibble: 3 x 5\n##   term           estimate   std.error statistic  p.value\n##   &lt;chr&gt;             &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n## 1 (Intercept) 76542.      10054.           7.61 5.15e-13\n## 2 cases_7day      0.00828     0.00112      7.41 1.86e-12\n## 3 date           -4.11        0.547       -7.52 9.11e-13\n\n\n\nBuild Some Models\n\n\nWe’ll approach this by running regression models of deaths and varying lags (actually leads) of cases. We chose to lead deaths as opposed to lagging cases because it will allow us to make predictions about the future of deaths given cases today. We include the date as a variable as well. Once we’ve run regressions against each lead period, we’ll chose the lead period that has the best fit (R-Squared) to the data.\n\n\nThe requires a lot of leads and a lot of models. Fortunately, R provides the tools to make this work very simple and well organized. First we add new columns for each lead period using timetk::tk_augment_lags. This one function call does all the work but it only does lags so we have to futz with it a bit to get leads.\n\n\nI chose to add forty days of leads. I don’t really think that long a lead is realistic and, given the pandemic has been around only nine months, there aren’t as many data points forty days ahead. Still, I want to see the behavior of the models. Once we have created the leads we remove any dates for which we don’t have led deaths.\n\n#create columns for deaths led 0 to 40 days ahead\nmax_lead &lt;- 40\nus_lags &lt;- us %&gt;%\n  # create lags by day\n  tk_augment_lags(deaths_7day,.lags = 0:-max_lead,.names=\"auto\")\n  # fix names to remove minus sign\n  names(us_lags) &lt;- names(us_lags) %&gt;% str_replace_all(\"lag-|lag\",\"lead\")\n\n# use only case dates where we have complete future knowledge of deaths for all lead times.\nus_lags &lt;- us_lags %&gt;% filter(date &lt; cutoff_end-max_lead)\n\nus_lags[1:10,1:7] %&gt;% kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndate\n\n\ncases_total\n\n\ndeaths_total\n\n\ncases_7day\n\n\ndeaths_7day\n\n\ndeaths_7day_lead0\n\n\ndeaths_7day_lead1\n\n\n\n\n\n\n2020-03-15\n\n\n3597\n\n\n68\n\n\n0.000\n\n\n0.00000\n\n\n0.00000\n\n\n0.00000\n\n\n\n\n2020-03-16\n\n\n4504\n\n\n91\n\n\n0.000\n\n\n0.00000\n\n\n0.00000\n\n\n0.00000\n\n\n\n\n2020-03-17\n\n\n5903\n\n\n117\n\n\n0.000\n\n\n0.00000\n\n\n0.00000\n\n\n0.00000\n\n\n\n\n2020-03-18\n\n\n8342\n\n\n162\n\n\n0.000\n\n\n0.00000\n\n\n0.00000\n\n\n0.00000\n\n\n\n\n2020-03-19\n\n\n12381\n\n\n212\n\n\n0.000\n\n\n0.00000\n\n\n0.00000\n\n\n0.00000\n\n\n\n\n2020-03-20\n\n\n17998\n\n\n277\n\n\n0.000\n\n\n0.00000\n\n\n0.00000\n\n\n0.00000\n\n\n\n\n2020-03-21\n\n\n24513\n\n\n360\n\n\n0.000\n\n\n0.00000\n\n\n0.00000\n\n\n55.57143\n\n\n\n\n2020-03-22\n\n\n33046\n\n\n457\n\n\n4204.714\n\n\n55.57143\n\n\n55.57143\n\n\n69.57143\n\n\n\n\n2020-03-23\n\n\n43476\n\n\n578\n\n\n5565.143\n\n\n69.57143\n\n\n69.57143\n\n\n95.28571\n\n\n\n\n2020-03-24\n\n\n53906\n\n\n784\n\n\n6857.571\n\n\n95.28571\n\n\n95.28571\n\n\n127.28571\n\n\n\n\n\n\n…etc up to 40\n\n\nNow we start the job of actually building the linear models and seeing the real power of the tidy modeling framework. Since we have our lead days in columns we revert back to long-form data. For each date we have a case count and 40 lead days with the corresponding death count. As will be seen below, the decline in the fatality rate has been non-linear, so we use a second-order polynomial to regress the date variable.\n\n\nOur workflow looks like this:\n\n\n\nCreate the lags using tk_augment_lag (above).\n\n\npivot to long form.\n\n\nnest the data by lead day and state.\n\n\nmap the data set for each lead day to a regression model.\n\n\nPull out the adjusted R-Squared using glance for each model to determine the best fit lead time.\n\n\n\nThe result is a data frame with our lead times, the nested raw data, model and R-squared for each lead time.\n\n# make long form to nest\n# initialize models data frame\nmodels &lt;- us_lags %&gt;% ungroup %&gt;% \n  pivot_longer(cols = contains(\"lead\"),\n               names_to = \"lead\",\n               values_to = \"led_deaths\") %&gt;% \n  select(date,cases_7day,lead,led_deaths) %&gt;% \n  mutate(lead = as.numeric(str_remove(lead,\"deaths_7day_lead\"))) %&gt;% \n\n  nest(data=c(date,cases_7day,led_deaths)) %&gt;% \n  # Run a regression on lagged cases and date vs deaths\n  mutate(model = map(data,\n                     function(df) \n                       lm(led_deaths~cases_7day+poly(date,2),data = df)))\n\n# Add regression coefficient\n# get adjusted r squared\nmodels &lt;- models %&gt;% \n  mutate(adj_r = map(model,function(x) glance(x) %&gt;% \n                       pull(adj.r.squared))\n         %&gt;% unlist)\nmodels\n## # A tibble: 41 x 4\n##     lead data               model  adj_r\n##    &lt;dbl&gt; &lt;list&gt;             &lt;list&gt; &lt;dbl&gt;\n##  1     0 &lt;tibble [218 x 3]&gt; &lt;lm&gt;   0.164\n##  2     1 &lt;tibble [218 x 3]&gt; &lt;lm&gt;   0.187\n##  3     2 &lt;tibble [218 x 3]&gt; &lt;lm&gt;   0.212\n##  4     3 &lt;tibble [218 x 3]&gt; &lt;lm&gt;   0.241\n##  5     4 &lt;tibble [218 x 3]&gt; &lt;lm&gt;   0.272\n##  6     5 &lt;tibble [218 x 3]&gt; &lt;lm&gt;   0.307\n##  7     6 &lt;tibble [218 x 3]&gt; &lt;lm&gt;   0.343\n##  8     7 &lt;tibble [218 x 3]&gt; &lt;lm&gt;   0.383\n##  9     8 &lt;tibble [218 x 3]&gt; &lt;lm&gt;   0.424\n## 10     9 &lt;tibble [218 x 3]&gt; &lt;lm&gt;   0.467\n## # ... with 31 more rows\n\nTo decide the best-fit lead time we choose the model with the highest R-squared.\n\n# Show model fit by lead time\n# make predictions using best model\nbest_fit &lt;- models %&gt;% \n  summarize(adj_r = max(adj_r)) %&gt;% \n  left_join(models,by= \"adj_r\")\n\nmodels %&gt;%\n  ggplot(aes(lead,adj_r)) + geom_line() +\n  labs(subtitle = paste(\"Best fit lead =\",best_fit$lead,\"days\"),\n       title = \"Model Fit By Lag Days\",\n       x = \"Lead Time in Days for Deaths\",\n       caption = \"Source: NY Times, Arthur Steinmetz\",\n       y= \"Adjusted R-squared\")\n\n We can have some confidence that we are not overfitting the date variable because the significance of the case count remains. With a high enough degree polynomial on the date variable, cases would vanish in importance.\n\nbest_fit$model[[1]] %&gt;% tidy()\n## # A tibble: 4 x 5\n##   term             estimate  std.error statistic  p.value\n##   &lt;chr&gt;               &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n## 1 (Intercept)      436.      38.0           11.5 4.21e-24\n## 2 cases_7day         0.0167   0.000993      16.8 5.45e-41\n## 3 poly(date, 2)1 -7306.     227.           -32.2 5.87e-84\n## 4 poly(date, 2)2  4511.     167.            26.9 1.02e-70\n\n\n\nMake Predictions\n\n\nThe best-fit lead time is 23 days but let’s use predict to see how well our model fits to the actual deaths.\n\n# ------------------------------------------\n# see how well our model predicts\n# Function to create prediction plot\nshow_predictions &lt;- function(single_model,n.ahead){\n  predicted_deaths = predict(single_model$model[[1]],newdata = us)\n  date = seq.Date(from=min(us$date) + n.ahead,to=max(us$date) + n.ahead,by=1)\n  display = full_join(us,tibble(date,predicted_deaths))\n\n  gg &lt;- display %&gt;% \n    pivot_longer(cols = where(is.numeric)) %&gt;% \n    filter(name %in% c(\"deaths_7day\",\"predicted_deaths\")) %&gt;% \n    ggplot(aes(date,value,color=name)) + geom_line() +\n    labs(title=\"Actual vs. Predicted Deaths\",\n         x = \"Date\", \n         y = \"Count\",\n         caption = \"Source: NY Times, Arthur Steinmetz\")\n  gg\n}\n\nshow_predictions(best_fit,best_fit$lead)\n\n\n\n\nThis is a satisfying result, but sadly shows deaths about to spike. This is despite accounting for the improvements in treatment outcomes we’ve accomplished over the past several months. The 23-day lead time model shows a 1.7% mortality rate over the whole length of observations but conditioned on deaths falling steadily over time.\n\n\n\n\nDeclining Mortality Rate\n\n\nOnce we’ve settled on the appropriate lag time, we can look at the fatality rate per identified case. This is but one possible measure of fatality rate, certainly not THE fatality rate. Testing rate, positivity rate and others variables will affect this measure. We also assume our best-fit lag is stable over time so take the result with a grain of salt. The takeaway should be how it is declining, not exactly what it is.\n\n\nEarly on, only people who were very sick or met strict criteria were tested so, of course, fatality rates (on this metric) were much, much higher. To minimize this we start our measure at the middle of April.\n\n\nSadly, we see that fatality rates are creeping up again.\n\nfatality &lt;- best_fit$data[[1]] %&gt;% \n  filter(cases_7day &gt; 0) %&gt;%\n  filter(date &gt; as.Date(\"2020-04-15\")) %&gt;%\n  mutate(rate = led_deaths/cases_7day)\n\nfatality %&gt;% ggplot(aes(date,rate)) + geom_line() + \n  geom_smooth() +\n  labs(x=\"Date\",y=\"Fatality Rate\",\n       title = \"Fatality Rates are Creeping Up\",\n       subtitle = \"Fatality Rate as a Percentage of Lagged Cases\",\n       caption = \"Source: NY Times, Arthur Steinmetz\") +\n  scale_y_continuous(labels = scales::percent)\n\n\n\n\n\n\nState-Level Analysis\n\n\nOne problem with the national model is each state saw the arrival of the virus at different times, which suggests there might also be different relationships between cases and deaths. Looking at a few selected states illustrates this.\n\n# ------------------------------------------\n# state by state analysis\n\nstate_subset &lt;- c(\"New York\",\"Texas\",\"California\",\"Ohio\")\n\n# illustrate selected states\nus_states %&gt;% \n  filter(state %in% state_subset) %&gt;% \n  ggplot(aes(date,cases_7day)) + geom_line(color=\"orange\") +\n  facet_wrap(~state,scales = \"free\") +\n  theme(legend.position = \"none\") +\n  geom_line(aes(y=deaths_7day*coeff),color=\"red\") +\n  scale_y_continuous(labels = scales::comma,\n                     name = \"Cases\",\n                     sec.axis = sec_axis(deaths_7day~./coeff,\n                                         name=\"Deaths\",\n                                         labels = scales::comma)) +\n  theme(\n    axis.title.y = element_text(color = \"orange\", size=13),\n    axis.title.y.right = element_text(color = \"red\", size=13)\n  ) +\n  labs(title =  \"U.S. Cases vs. Deaths\",\n       subtitle = \"7-Day Average\",\n       caption = \"Source: NY Times, Arthur Steinmetz\",\n       x = \"Date\")\n\n\n\n\nIn particular we note New York, where the virus arrived early and circulated undetected for weeks. Testing was rare and we did not know much about the course of the disease so the death toll was much worse. Tests were often not conducted until the disease was in advanced stages so we would expect the lag to be shorter.\n\n\nIn Texas, the virus arrived later. There it looks like the consequences of the first wave were less dire and the lag was longer.\n\n\n\n\nRun State Models\n\n\nNow we can run the same workflow we used above over the state-by-state data. Our data set is much larger because we have a full set of lags for each state but building our data frame of list columns is just as easy.\n\n\nLooking at the lags by state shows similar results to the national model, on average, as we assume, but the dispersion is large. Early in the pandemic, in New York, cases were diagnosed only for people who were already sick so the lead time before death was much shorter.\n\n# create lags\nus_states_lags &lt;- us_states %&gt;%\n  # create lags by day\n  tk_augment_lags(deaths_7day,.lags = -max_lead:0,.names=\"auto\") %&gt;% \n  {.}\n# fix names to remove minus sign\nnames(us_states_lags) &lt;- names(us_states_lags) %&gt;% str_replace_all(\"lag-\",\"lead\")\n\n# make long form to nest\n# initialize models data frame\nmodels_st &lt;- us_states_lags %&gt;% ungroup %&gt;% \n  pivot_longer(cols = contains(\"lead\"),\n               names_to = \"lead\",\n               values_to = \"led_deaths\") %&gt;% \n  select(state,date,cases_7day,lead,led_deaths) %&gt;% \n  mutate(lead = as.numeric(str_remove(lead,\"deaths_7day_lead\"))) %&gt;% \n  {.}\n\n# make separate tibbles for each regression\nmodels_st &lt;- models_st %&gt;% \n  nest(data=c(date,cases_7day,led_deaths)) %&gt;% \n  arrange(lead)\n\n#Run a linear regression on lagged cases and date vs deaths\nmodels_st &lt;- models_st %&gt;% \n  mutate(model = map(data,\n                     function(df) \n                       lm(led_deaths~cases_7day+poly(date,2),data = df)))\n\n\n# Add regression coefficient\n# get adjusted r squared\nmodels_st &lt;- models_st %&gt;% \n  mutate(adj_r = map(model,function(x) glance(x) %&gt;% \n                       pull(adj.r.squared))\n         %&gt;% unlist)\n\nmodels_st %&gt;%\n  filter(state %in% state_subset) %&gt;% \n  ggplot(aes(lead,adj_r)) + geom_line() +\n  facet_wrap(~state) +\n  labs(title = \"Best Fit Lead Time\",\n       caption = \"Source: NY Times, Arthur Steinmetz\")\n\n\n\n\nTo see how the fit looks for the data set as a whole we look at a histogram of all the state R-squareds. We see many of the state models have a worse accuracy than the national model.\n\n# best fit lag by state\nbest_fit_st &lt;- models_st %&gt;% \n  group_by(state) %&gt;% \n  summarize(adj_r = max(adj_r)) %&gt;% \n  left_join(models_st)\n\nbest_fit_st %&gt;% ggplot(aes(adj_r)) + \n  geom_histogram(bins = 10,color=\"white\") +\n  geom_vline(xintercept = best_fit$adj_r[[1]],color=\"red\") +\n  annotate(geom=\"text\",x=0.75,y=18,label=\"Adj-R in National Model\") +\n  labs(y = \"State Count\",\n       x=\"Adjusted R-Squared\",\n       title = \"Goodness of Fit of State Models\",\n       caption = \"Source:NY Times,Arthur Steinmetz\")\n\n\n\n\nThere are vast differences in the best-fit lead times across the states but the distribution is in agreement with our national model.\n\nbest_fit_st %&gt;% ggplot(aes(lead)) + \n  geom_histogram(binwidth = 5,color=\"white\") +\n  scale_y_continuous(labels = scales::label_number(accuracy = 1)) +\n  geom_vline(xintercept = best_fit$lead[[1]],color=\"red\") +\n  annotate(geom=\"text\",x=best_fit$lead[[1]]+7,y=10,label=\"Lead in National Model\") +\n  labs(y = \"State Count\",\n    x=\"Best Fit Model Days from Case to Death\",\n    title = \"COVID-19 Lag Time From Cases to Death\",\n    caption = \"Source:NY Times,Arthur Steinmetz\")\n\n\n\n\n\n\nValidate with Individual Case Data from Ohio\n\n\nThis whole exercise has involved proxying deaths by time and quantity of positive tests. Ideally, we should look at longitudinal data which follows each individual. The state of Ohio provides that so we’ll look at just this one state to provide a reality check on the foregoing analysis. In our proxy model, Ohio shows a best-fit lead time of 31 days, which is much longer than our national-level model.\n\n# ----------------------------------------------------\nbest_fit_st %&gt;% select(-data,-model) %&gt;% filter(state == \"Ohio\") %&gt;% kable()\n\n\n\n\n\nstate\n\n\nadj_r\n\n\nlead\n\n\n\n\n\n\nOhio\n\n\n0.7548416\n\n\n31\n\n\n\n\n\n\nThe caveat here is the NY Times data uses the “case” date which is presumably the date a positive test is recorded. The Ohio data uses “onset” date, which is the date the “illness began.” That is not necessarily the same as the test date.\n\n# source: https://coronavirus.ohio.gov/static/dashboards/COVIDSummaryData.csv\nohio_raw &lt;- read_csv(\"https://coronavirus.ohio.gov/static/dashboards/COVIDSummaryData.csv\", \n                     col_types = cols(`Admission Date` = col_date(format = \"%m/%d/%Y\"), \n                                      `Date Of Death` = col_date(format = \"%m/%d/%Y\"), \n                                      `Onset Date` = col_date(format = \"%m/%d/%Y\")))\n\n# helper function to fix column names to best practice\nfix_df_colnames &lt;- function(df){\n  names(df)&lt;-names(df) %&gt;% \n    str_replace_all(c(\" \" = \"_\" , \",\" = \"\" )) %&gt;% \n    tolower()\n  return(df)\n}\n\n# clean up the data\nohio &lt;- ohio_raw %&gt;% \n  rename(death_count = `Death Due to Illness Count`) %&gt;% \n  filter(County != \"Grand Total\") %&gt;%\n  fix_df_colnames() %&gt;% \n  # data not clean before middle of march\n  filter(onset_date &gt;= cutoff_start)\n\nHow comparable are these data sets? Let’s compare the NY Times case count and dates to the Ohio “Illness Onset” dates.\n\n# create rolling average function\nmean_roll_7 &lt;- slidify(mean, .period = 7, .align = \"right\")\n\ncomps &lt;- ohio %&gt;% \n  group_by(onset_date) %&gt;% \n  summarise(OH = sum(case_count),.groups = \"drop\") %&gt;%\n  mutate(OH = mean_roll_7(OH)) %&gt;% \n  ungroup() %&gt;% \n  mutate(state = \"Ohio\") %&gt;% \n  rename(date=onset_date) %&gt;% \n  left_join(us_states,by=c(\"date\",\"state\")) %&gt;% \n  transmute(date,OH,NYTimes = cases_7day)\n\ncomps %&gt;% \n  pivot_longer(c(\"OH\",\"NYTimes\"),names_to = \"source\",values_to = \"count\") %&gt;%  \n  ggplot(aes(date,count,color=source)) + geom_line() +\n  labs(title =  \"Case Counts from Different Sources\",\n       caption = \"Source: State of Ohio, NY Times\",\n       subtitle = \"NY Times and State of Ohio\",\n       x = \"Date\",\n       y = \"Daily Case Count (7-day Rolling Average)\")\n\n We clearly see the numbers line up almost exactly but the Ohio data runs about 4 days ahead of the NY Times data.\n\n\nFor each individual death, we subtract the onset date from the death date. Then we aggregate the county-level data to statewide and daily data to weekly. Then take the weekly mean of deaths.\n\n# aggregate the data to weekly\nohio &lt;- ohio %&gt;% \n  mutate(onset_to_death = as.numeric(date_of_death - onset_date),\n         onset_year = year(onset_date),\n         onset_week = epiweek(onset_date))\n\n\nonset_to_death &lt;- ohio %&gt;%\n  filter(death_count &gt; 0) %&gt;% \n  group_by(onset_year,onset_week) %&gt;%\n  summarise(death_count_sum = sum(death_count),\n            mean_onset_to_death = weighted.mean(onset_to_death,\n                                                death_count,\n                                                na.rm = TRUE)) %&gt;%\n  mutate(date=as.Date(paste(onset_year,onset_week,1),\"%Y %U %u\")) %&gt;%\n  {.}\n\nonset_to_death %&gt;% ggplot(aes(date,death_count_sum)) + geom_col() +\n    labs(title =  \"Ohio Weekly Deaths\",\n       caption = \"Source: State of Ohio, Arthur Steinmetz\",\n       subtitle = \"Based on Illness Onset Date\",\n       x = \"Date of Illness Onset\",\n       y = \"Deaths\")\n\n When we measure the average lag, we find that it has been fairly stable over time in Ohio. Unfortunately, it differs substantially from our proxy model using untracked cases.\n\n# helper function to annotate plots \npos_index &lt;- function(index_vec,fraction){\n  return(index_vec[round(length(index_vec)*fraction)])\n}\n\navg_lag &lt;- round(mean(onset_to_death$mean_onset_to_death))\n\nonset_to_death %&gt;% ggplot(aes(date,mean_onset_to_death)) + \n  geom_col() +\n  geom_hline(yintercept = avg_lag) +\n  annotate(geom=\"text\",\n           label=paste(\"Average Lag =\",round(avg_lag)),\n           y=20,x=pos_index(onset_to_death$date,.8)) +\n  labs(x = \"Onset Date\",\n       y = \"Mean Onset to Death\",\n       title = \"Ohio Days from Illness Onset Until Death Over Time\",\n       caption = \"Source: State of Ohio, Arthur Steinmetz\",\n       subtitle = paste(\"Average =\",\n                     avg_lag,\"Days\"))\n\n Note the drop off at the end of the date range. This is because we don’t yet know the outcome of the most recently recorded cases. Generally, while we have been successful in lowering the fatality rate of this disease, the duration from onset to death for those cases which are fatal has not changed much, at least in Ohio.\n\n\nSince we have the actual number of deaths associated with every onset date we can calculate the “true” fatality rate. As mentioned, the fatality rate of the more recent cases is not yet known. Also the data is too sparse at the front of the series so we cut off the head and the tail of the data.\n\nohio_fatality_rate &lt;- ohio %&gt;% \n  group_by(onset_date) %&gt;% \n  summarize(case_count = sum(case_count),\n            death_count = sum(death_count),.groups=\"drop\") %&gt;% \n  mutate(fatality_rate = death_count/case_count) %&gt;% \n  mutate(fatality_rate_7day = mean_roll_7(fatality_rate)) %&gt;% \n# filter out most recent cases we we don't know outcome yet\n  filter(onset_date &lt; max(onset_date)-30) \n\nohio_fatality_rate %&gt;% \n  filter(onset_date &gt; as.Date(\"2020-04-15\")) %&gt;% \n  ggplot(aes(onset_date,fatality_rate_7day)) + geom_line() +\n  geom_smooth() +\n  labs(x=\"Illness Onset Date\",y=\"Ohio Fatality Rate\",\n       caption = \"Source: State of Ohio, Arthur Steinmetz\",\n       title = \"Ohio Fatality Rate as a Percentage of Tracked Cases\") +\n  scale_y_continuous(labels = scales::percent,breaks = seq(0,0.12,by=.01))\n\n\n\n\nThe fatality rate in Ohio seems to have been worse than our national model but it is coming down. Again, this result comes from a different methodology than our proxy model.\n\n\n\n\nConclusion\n\n\nAmong the vexing aspects of this terrible pandemic is that we don’t know what the gold standard is for treatment and prevention. We are learning as we go. The good news is we ARE learning. For a data analyst the challenge is the evolving relationship of of all of the disparate data. Here we have gotten some insight into the duration between a positive test and mortality. We can’t have high confidence that our proxy model using aggregate cases is strictly accurate because the longitudinal data from Ohio shows a different lag. We have clearly seen that mortality has been declining but our model suggests that death will nonetheless surge along with the autumn surge in cases.\n\n\nWhat are the further avenues for modeling? There is a wealth of data around behavior and demographics with this disease that we don’t fully understand yet. On the analytics side, we might get more sophisticated with our modeling. We have only scratched the surface of the tidymodels framework and we might apply fancier predictive models than linear regression. Is the drop in the fatality rate we saw early in the pandemic real? Only people who were actually sick got tested in the early days. Now, many positive tests are from asymptomatic people. Finally, the disagreement between the case proxy model and the longitudinal data in Ohio shows there is more work to be done."
  },
  {
    "objectID": "posts/2023-01-24_Switching-to-Quarto/2023-1-24_switching-to-quarto.html",
    "href": "posts/2023-01-24_Switching-to-Quarto/2023-1-24_switching-to-quarto.html",
    "title": "Switching to Quarto from Blogdown",
    "section": "",
    "text": "It all started when I decided to change up my Hugo theme. Up until that point I was happily using the Blogdown add-in for RStudio to initiate new blog posts. At some point the default directory structure for Hugo blogs changed and when I tried to update my theme the whole web site got impossibly messy and finally broke. I managed to cobble it back together but I really had no idea what I was doing and became afraid of messing with it any more. This led to bloggers block.\nWhen Quarto came on the scene I was attracted to the idea of starting fresh but also intimidated by the thought of porting all my old content from R Markdown to Quarto. Plenty of people told me that R Markdown was not going away and there was no big need to switch. It is the big improvement over Blogdown/Hugo for blogging that has made me a convert. Quarto is simpler.\nIdeally you should just be able to render the R Markdown code in Quarto and it will just work. For me, the problem was that many of my posts would no longer render in R because of package updates or out-of-date web links. Quarto solves that problem with the freeze option but it only works for content that has been rendered at least once with Quarto.\nI needn’t have fretted. It turns out porting old blog or web content is ridiculously easy, though the existing Quarto guides don’t discuss this one simple trick. Here’s the TL;DR version: all you have to do is change the file extension of your fully rendered HTML files from .html to .qmd and Quarto will happily render them, wrapping its own HTML code around the old HTML but not otherwise messing with it. Any theming or subsequent changes in theming will be properly rendered. You can get rid of your old R Markdown files (though they will live in GitHub forever, right?).\nI never did figure out the directory structure of a Hugo web site. Fortunately, Quarto is much simpler. The main directory and subdirectories of your blog contain all the Quarto files. You can name them as you like. One special directory called _site contains the rendered HTML files. It will be created the first time you render your site. There are a few more things to do to bring it all together. Let’s go through them step-by-step.\nFrom this point you can go down the rabbit hole and play with all the theming and formatting options that Quarto allows. You’ll make most of these changes in your index.qmd and _quarto.yml files. Any changes you make will be reflected in your old posts. Easy peasy!"
  },
  {
    "objectID": "posts/2023-01-24_Switching-to-Quarto/2023-1-24_switching-to-quarto.html#publishing-your-blog-on-netlify",
    "href": "posts/2023-01-24_Switching-to-Quarto/2023-1-24_switching-to-quarto.html#publishing-your-blog-on-netlify",
    "title": "Switching to Quarto from Blogdown",
    "section": "Publishing Your Blog on Netlify",
    "text": "Publishing Your Blog on Netlify\nYou will have to make some tiny changes before Netfliy will re-publish your sparkling remodeled blog.\n\nOnce you are happy with the finished port, delete everything in the old blog’s directory EXCEPT the .git file. Copy everything from the new blog EXCEPT the .git file over to the old directory. It’s scary but you are just burning the old site down.\nGit should show all the deletions and additions to be made to the repo. Commit them all and push to GitHub.\nNow visit your deploy settings at Netlify.com. You’ll find them at https://app.netlify.com/sites/&lt;yoursitename&gt;/settings/deploys.\nClick on Edit settings\nYou’ll change the build command from “Hugo” to nothing and change the publish directory to “_site.” Then save.\n\n\nThat’s it! Netlfiy should regenerate your site in a few seconds and you are back in business. Have fun!\n\n\n\n\n\n\nThank you!\n\n\n\nThanks to Emily Robinson for jump starting me on this project. She pointed me to great resources on writing a Quarto blog and convinced me that “you can do it!”"
  },
  {
    "objectID": "posts/2023-04-15-analyze-tweets-with-google-translate-pt-1/index.html",
    "href": "posts/2023-04-15-analyze-tweets-with-google-translate-pt-1/index.html",
    "title": "Sentiment Analysis Using Google Translate (Pt. 1)",
    "section": "",
    "text": "Can an English speaker use Google Cloud Translate to conduct experiments with natural language processing? Let’s fund out. Like all my data science posts, I find interesting questions (to me, anyway) and answer them in ways that sharpen my R coding skills. I am reminded of the movie “Fast Times At Ridgemont High” where Sean Penn’s character thinks he can simultaneously “learn about Cuba and have some food.” The teacher, played by Ray Walston, takes a dim view of the pizza delivery. Hopefully, you, dear reader, will be more accepting."
  },
  {
    "objectID": "posts/2023-04-15-analyze-tweets-with-google-translate-pt-1/index.html#inspired-by-tidytuesday",
    "href": "posts/2023-04-15-analyze-tweets-with-google-translate-pt-1/index.html#inspired-by-tidytuesday",
    "title": "Sentiment Analysis Using Google Translate (Pt. 1)",
    "section": "Inspired by TidyTuesday",
    "text": "Inspired by TidyTuesday\nSome of the the R data science community participate in a weekly challenge called “Tidy Tuesday,” where an interesting data set is presented for analysis but mostly visualization. There are some tremendous examples of beautiful work posted on Twitter with the hashtag #tidytuesday."
  },
  {
    "objectID": "posts/2023-04-15-analyze-tweets-with-google-translate-pt-1/index.html#african-tweets-and-sentiment",
    "href": "posts/2023-04-15-analyze-tweets-with-google-translate-pt-1/index.html#african-tweets-and-sentiment",
    "title": "Sentiment Analysis Using Google Translate (Pt. 1)",
    "section": "African Tweets and Sentiment",
    "text": "African Tweets and Sentiment\nRecently, the weekly dataset was a collection of over 100,000 tweets, apparently from 2022, in 14 African languages, with sentiment labels. The paper describing the set and methods is here (Muhammad et al. 2023). The TidyTuesday project and raw data are here. This is quite a diverse data set including many tweets in English, tweets in languages which, like English, use the Latin character set and tweets in other character sets, including Arabic.\nI saw this as an avenue to ask a couple interesting questions.\n\nCan we apply sentiment analysis techniques to a translated version of this dataset? How good is Google Translate, anyway?\nOver the past year there has been much talk about the differences in attitudes of the “global north” vs. the “global south.” Does this data set reveal anything about that?\n\nI also saw an opportunity to sharpen my skills in a couple areas, using the Google API for batch translation and using RStudio’s Tidytext and Tidymodels toolsets.\nI split these explorations into four snack-able posts.\n\nIn this post we show how to use Google Cloud Translate to batch translate the entire data set.\nHere we use the tidytext framework to do sentiment analysis with word valences.\nNext, we’ll compare machine learning approaches in the tidymodels framework to do sentiment analysis on both the native and translated tweets.\nFinally, let’s use the already assigned sentiment tags to explore African attitudes to the “global north.”\n\n\n\n\n\n\n\nDisclaimer\n\n\n\nThe usual caveats apply. I am not a social scientist. I am a hobbyist. This is an exercise in R coding so I make no claim that my conclusions about any of this data are valid."
  },
  {
    "objectID": "posts/2023-04-15-analyze-tweets-with-google-translate-pt-1/index.html#get-the-data",
    "href": "posts/2023-04-15-analyze-tweets-with-google-translate-pt-1/index.html#get-the-data",
    "title": "Sentiment Analysis Using Google Translate (Pt. 1)",
    "section": "Get the Data",
    "text": "Get the Data\nHere are the packages we’ll need for this project.\n\nsuppressPackageStartupMessages({\n  library(tidyverse)\n  library(googleLanguageR)\n  library(future)\n  library(furrr)\n  library(rvest)\n})\n\nThe TidyTuesday github repo has the Afrisenti dataset with all the languages combined. Let’s load it.\n\nafrisenti &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-02-28/afrisenti.csv',\n                             show_col_types = FALSE)\n\nafrisenti\n\n# A tibble: 111,720 × 4\n   language_iso_code tweet                                         label inten…¹\n   &lt;chr&gt;             &lt;chr&gt;                                         &lt;chr&gt; &lt;chr&gt;  \n 1 amh               አማራ ክልል ፈልቶበታል ልኩን ማስገባት ነው!!! ሙስሊሞችን ጠልቶ 85… nega… dev    \n 2 amh               ሰውን አንገት በሚያስደፋ መልኩ ዝም ብሎ ሙድ መያዝ....ስልህ ያ ሰው… nega… dev    \n 3 amh               የቤት ውስጥ ጥቃት – ያለሰሚ – ያለተመልካች                  nega… dev    \n 4 amh               Ethiopia ወያኔን ለመጣል ምን ድርሻ ነበራችሁ ? ከወደቀ በኋላ ጉ… nega… dev    \n 5 amh               ኦሮሞ ምንም ቢማር ከብት ነዉ አያስተዉልም ጥንብ ዘረኛ ናቸዉ        nega… dev    \n 6 amh               ቲሽ ጨለምተኛ ዱቄት 97 ላይ ቆመሃል እንዴ ጊዜው ነጉዷል 2012 ነው… nega… dev    \n 7 amh               በምዕራብ ኦሮሚያ በሚገኙ በሁለቱ የወለጋ ዞኖች (ምስራቅ ወለጋና ሆሮ … nega… dev    \n 8 amh               ያየሰው ሺመልስ ላይ የደረሰው ነገር ያሳዝናል። በቃል ኣላምረውም ላይም… nega… dev    \n 9 amh               ያልተረጋጋች ሀገር ምርጫ አያስፈልጋትም                      nega… dev    \n10 amh               ደደቡ እና አረፋው የኢትዮጵያው ጠ/ሚ አብዪ ከኤርትራው ኣያቶላህ ኢሰያ… nega… dev    \n# … with 111,710 more rows, and abbreviated variable name ¹​intended_use"
  },
  {
    "objectID": "posts/2023-04-15-analyze-tweets-with-google-translate-pt-1/index.html#translate-the-tweets",
    "href": "posts/2023-04-15-analyze-tweets-with-google-translate-pt-1/index.html#translate-the-tweets",
    "title": "Sentiment Analysis Using Google Translate (Pt. 1)",
    "section": "Translate the Tweets",
    "text": "Translate the Tweets\nTo use Google translate in batch mode we’ll need an API key. I don’t understand Google. For some of their services, like Maps, a single API key is needed. Instead, for Translate, we need a JSON file with the key. Once you get the key, store the file name in your .Renviron file with the key name “GL_AUTH” then the googlelanguageR package will automatically authenticate when it loads.\n\n\n\n\n\n\nNote\n\n\n\nGetting the Google Language API key is a complicated procedure and I won’t detail it here but you can find complete instructions in the googlelanguageR package introduction.\n\n\nOnce once your key is created you can start translating with R. This isn’t free. Translating over 100,000 tweets cost me about US$15. A couple bucks was wasted because I submitted all the tweets including those in English. You might choose to filter English tweets out first. If you just want to work with the same data set you can download my translations (see below) for FREE.\nI first tried shooting the whole data set into the translation routine but Google protested that I was sending too much. I divided the set into batches of 100 tweets at a time which fixed the problem.\nWe can dramatically speed things up using the furrr and future packages to allow parallel processing using just three lines of code. furrr adapts the purrr::map() family of functions to allow parallel execution. Very simple. Amazing!\n\nfuture::plan(multicore) # will use all available cores\nbatch_size = min(100, nrow(afrisenti))\nafrisenti_translated &lt;- \n  seq(0, nrow(afrisenti) - batch_size,\n      by = batch_size) |&gt;\n  furrr::future_map_dfr(\\(x) {\n    gl_translate(afrisenti$tweet[(x + 1):(x + batch_size)])\n    }, .progress = TRUE)\nfuture::plan(sequential) # back to normal\n\nIt’s worth looking over the code above because it packs a lot of power in few lines and, to me, shows how cool R is. Basically, we identify the batches of rows from the data set we want to ship out to Google and translate them in as many parallel streams as our hardware allows.\nLet’s clean up the data a little.\n\n# merge with source data and clean up a little\nafrisenti_translated &lt;- afrisenti_translated |&gt; \n  na.omit() |&gt; \n  select(-text) |&gt; \n  bind_cols(afrisenti) |&gt; \n  rowid_to_column(var = \"tweet_num\") |&gt; \n  mutate(tweet_num = as.numeric(tweet_num))\n  mutate(intended_use = as_factor(intended_use)) |&gt;\n  mutate(detectedSourceLanguage = as_factor(detectedSourceLanguage)) |&gt;\n  mutate(language_iso_code = as_factor(language_iso_code)) |&gt;\n  mutate(label = as.factor(label))\n\nWhile it’s not strictly necessary, I wanted to see the long names for the languages, rather than just 2-character ISO codes. Using Wikipedia I created a file that we can use for reference. The rvest package makes turning an HTML table into a data frame easy. At the same time, let’s make sure the language labels from the data set are consistent with the Google language labels.\n\n# get languages from wikipedia\n# take the second table on the page\niso_lang &lt;- html_table(read_html(\"https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes#External_links\"))[[2]]\n\n# since Wikipedia is subject to frequent change, if the entry doesn't work\n# you can get the file here\n# iso_lang &lt;- readr::read_csv('https://raw.githubusercontent.com/apsteinmetz/tidytuesday/master/2023-02-28_african_language/data/iso_lang.csv',\n#                              show_col_types = FALSE)\n\niso_lang &lt;- iso_lang %&gt;% \n  rename(assigned_language = `639-2/T`,\n         detected_language = `639-1`,\n         language = `ISO language name`) %&gt;% \n  select(1:3)\n\n# clean up langauge names\nafrisenti_translated &lt;- afrisenti_translated %&gt;% \n  mutate(language_iso_code = str_replace_all(language_iso_code,\"pt-MZ\",\"por\")) %&gt;% \n  mutate(language_iso_code = str_replace_all(language_iso_code,\"ary\",\"ara\")) %&gt;% \n  mutate(language_iso_code = str_replace_all(language_iso_code,\"arq\",\"ara\")) %&gt;% \n  mutate(language_iso_code = str_replace_all(language_iso_code,\"pcm\",\"eng\")) %&gt;% \n  rename(assigned_language = language_iso_code,\n         detected_language = detectedSourceLanguage) %&gt;% \n  left_join(select(iso_lang,-assigned_language)) %&gt;% \n  rename(detected_long = language) %&gt;% \n  left_join(select(iso_lang,-detected_language)) %&gt;% \n  rename(assigned_long = language)\n  \n# save it for later use\nsave(afrisenti_translated,file=\"data/afrisenti_translated.rdata\")"
  },
  {
    "objectID": "posts/2023-04-15-analyze-tweets-with-google-translate-pt-1/index.html#save-some-money",
    "href": "posts/2023-04-15-analyze-tweets-with-google-translate-pt-1/index.html#save-some-money",
    "title": "Sentiment Analysis Using Google Translate (Pt. 1)",
    "section": "Save Some Money",
    "text": "Save Some Money\nThat done, YOU don’t want to pay $15 and you don’t have to. Let’s download the translated Afrisenti data set from my repo instead.\n\nafrisenti_translated &lt;- readr::read_csv('https://raw.githubusercontent.com/apsteinmetz/tidytuesday/master/2023-02-28_african_language/data/afrisenti_translated.csv',\n                                        show_col_types = FALSE)\nafrisenti_translated\n\n# A tibble: 111,720 × 9\n   tweet_num translatedText  detec…¹ assig…² tweet label inten…³ detec…⁴ assig…⁵\n       &lt;dbl&gt; &lt;chr&gt;           &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;  \n 1         1 Amhara region … am      amh     አማራ … nega… dev     Amharic Amharic\n 2         2 Having a mood … am      amh     ሰውን … nega… dev     Amharic Amharic\n 3         3 Domestic viole… am      amh     የቤት … nega… dev     Amharic Amharic\n 4         4 Ethiopia, what… am      amh     Ethi… nega… dev     Amharic Amharic\n 5         5 No matter how … am      amh     ኦሮሞ … nega… dev     Amharic Amharic\n 6         6 Tish, dark pow… am      amh     ቲሽ ጨ… nega… dev     Amharic Amharic\n 7         7 Local resident… am      amh     በምዕራ… nega… dev     Amharic Amharic\n 8         8 What happened … am      amh     ያየሰው… nega… dev     Amharic Amharic\n 9         9 An unstable co… am      amh     ያልተረ… nega… dev     Amharic Amharic\n10        10 The idiot and … am      amh     ደደቡ … nega… dev     Amharic Amharic\n# … with 111,710 more rows, and abbreviated variable names ¹​detected_language,\n#   ²​assigned_language, ³​intended_use, ⁴​detected_long, ⁵​assigned_long"
  },
  {
    "objectID": "posts/2023-04-15-analyze-tweets-with-google-translate-pt-1/index.html#do-a-reality-check",
    "href": "posts/2023-04-15-analyze-tweets-with-google-translate-pt-1/index.html#do-a-reality-check",
    "title": "Sentiment Analysis Using Google Translate (Pt. 1)",
    "section": "Do a Reality Check",
    "text": "Do a Reality Check\nNow that the hard work is done let’s do some preliminary checks, Let’s see if the language that Google detects agrees with the assigned language in the data set. First we convert the language long names to factors and see how many levels there are. These are the 13 assigned languages.\n\nafrisenti_translated &lt;- afrisenti_translated |&gt; \n  mutate(across(contains(\"long\"),\\(x) as.factor(x)))\n\nlevels(afrisenti_translated$assigned_long)\n\n [1] \"Amharic\"     \"Arabic\"      \"English\"     \"Hausa\"       \"Igbo\"       \n [6] \"Kinyarwanda\" \"Oromo\"       \"Portuguese\"  \"Swahili\"     \"Tigrinya\"   \n[11] \"Tsonga\"      \"Twi\"         \"Yoruba\"     \n\n\nHere are the languages that Google detects.\n\nlevels(afrisenti_translated$detected_long)\n\n [1] \"Afrikaans\"                     \"Akan\"                         \n [3] \"Amharic\"                       \"Arabic\"                       \n [5] \"Aymara\"                        \"Bambara\"                      \n [7] \"Basque\"                        \"Bengali\"                      \n [9] \"Bosnian\"                       \"Bulgarian\"                    \n[11] \"Catalan, Valencian\"            \"Chichewa, Chewa, Nyanja\"      \n[13] \"Chinese\"                       \"Corsican\"                     \n[15] \"Croatian\"                      \"Czech\"                        \n[17] \"Danish\"                        \"Dutch, Flemish\"               \n[19] \"English\"                       \"Esperanto\"                    \n[21] \"Estonian\"                      \"Ewe\"                          \n[23] \"Finnish\"                       \"French\"                       \n[25] \"Gaelic, Scottish Gaelic\"       \"Galician\"                     \n[27] \"Ganda\"                         \"German\"                       \n[29] \"Greek, Modern (1453–)\"         \"Guarani\"                      \n[31] \"Gujarati\"                      \"Haitian, Haitian Creole\"      \n[33] \"Hausa\"                         \"Hindi\"                        \n[35] \"Hungarian\"                     \"Igbo\"                         \n[37] \"Indonesian\"                    \"Irish\"                        \n[39] \"Italian\"                       \"Japanese\"                     \n[41] \"Javanese\"                      \"Kannada\"                      \n[43] \"Kinyarwanda\"                   \"Korean\"                       \n[45] \"Kurdish\"                       \"Latin\"                        \n[47] \"Latvian\"                       \"Lingala\"                      \n[49] \"Luxembourgish, Letzeburgesch\"  \"Malagasy\"                     \n[51] \"Malay\"                         \"Malayalam\"                    \n[53] \"Maltese\"                       \"Maori\"                        \n[55] \"Marathi\"                       \"Norwegian\"                    \n[57] \"Oromo\"                         \"Pashto, Pushto\"               \n[59] \"Persian\"                       \"Polish\"                       \n[61] \"Portuguese\"                    \"Quechua\"                      \n[63] \"Romanian, Moldavian, Moldovan\" \"Russian\"                      \n[65] \"Samoan\"                        \"Shona\"                        \n[67] \"Sindhi\"                        \"Slovak\"                       \n[69] \"Slovenian\"                     \"Somali\"                       \n[71] \"Southern Sotho\"                \"Spanish, Castilian\"           \n[73] \"Sundanese\"                     \"Swahili\"                      \n[75] \"Swedish\"                       \"Tamil\"                        \n[77] \"Telugu\"                        \"Tigrinya\"                     \n[79] \"Tsonga\"                        \"Turkish\"                      \n[81] \"Turkmen\"                       \"Ukrainian\"                    \n[83] \"Urdu\"                          \"Uzbek\"                        \n[85] \"Vietnamese\"                    \"Welsh\"                        \n[87] \"Western Frisian\"               \"Xhosa\"                        \n[89] \"Yoruba\"                        \"Zulu\"                         \n\n\nUh, oh. 90 detected languages vs. 13 assigned languages. Is this a problem? What fraction of tweets are in languages not mentioned in the original set?\n\nalt_count &lt;- afrisenti_translated |&gt; \n  filter(!(detected_long %in% levels(assigned_long))) |&gt; \n  nrow()/nrow(afrisenti_translated)*100 \n\npaste0(round(alt_count,1),\"%\")\n\n[1] \"6.9%\"\n\n\nNot a big number. Let’s collapse all but the top 15 languages into an “other” category.\nHow frequently does Google disagree with the assigned language?\n\nafrisenti_translated &lt;- afrisenti_translated |&gt; \n  mutate(detected_long = replace_na(as.character(detected_long,\"Unknown\"))) |&gt; \n  mutate(detected_long = fct_lump_n(detected_long,15))\n\nxt &lt;- xtabs(~afrisenti_translated$assigned_long +\n        afrisenti_translated$detected_long) |&gt; \n  broom::tidy() |&gt; \n  rename(assigned = 1,google = 2) |&gt; \n  group_by(assigned) |&gt; \n  mutate(Proportion = n/sum(n))\n   \nxt |&gt;\n  ggplot(aes(assigned, google,fill=Proportion)) + geom_tile() +\n  scale_fill_gradient(low = \"#FFBF00\", high = \"#007000\") +\n  theme(\n    plot.background = element_rect(fill = \"#FDECCD\", color = NA),\n    legend.background = element_blank(),\n    axis.ticks = element_blank(),\n    axis.text.x = element_text(angle = 45,vjust = .7,hjust = .6), \n    panel.background = element_blank(),\n    panel.grid = element_blank()\n  ) +\n  labs(\n    title = \"African Languages Tweets\\nQ: Does Google Detect The Same Language?\",\n    subtitle = \"A: Almost Entirely\",\n    x = \"Afrisenti Assigned Language\",\n    y = \"Google Translate Detected Language\",\n    caption = \"source: Afrisenti Data Set\"\n  )\n\n\n\n\nThe fact that disagreement about the tweet language is so rare gives us some confidence that we are on the right track.\nThen look at the first row.\n\nprint(cat(\nafrisenti_translated$tweet[1],\"\\n\",\nafrisenti_translated$translatedText[1],\"\\n\",\nafrisenti_translated$label[1]))\n\nአማራ ክልል ፈልቶበታል ልኩን ማስገባት ነው!!! ሙስሊሞችን ጠልቶ 85% ሙስሊሞች በሚኖርባት ኦሮምያ ጋር ግንኙነትን አትሰበው !!! \n Amhara region needs moderation!!! He hates Muslims and does not think of relations with Oromia, where 85% of Muslims live!!! \n negativeNULL\n\n\nA quick glance at the translation shows obviously negative sentiment. We are off to a promising start. In the next post we’ll use the tidytext framework to measure the net balance of sentiment for each tweet."
  },
  {
    "objectID": "posts/2023-04-16-analyze-tweets-with-google-translate-pt-2/index.html",
    "href": "posts/2023-04-16-analyze-tweets-with-google-translate-pt-2/index.html",
    "title": "Sentiment Analysis Using Google Translate (Pt. 2)",
    "section": "",
    "text": "Sentiment analysis is a common task in natural language processing. Many of the tools available for this are calibrated to the English language. Can Google Translate let us apply this toolset to a broader set of languages? We will use the tidytext framework to measure sentiment of tweets originally in African languages but translated to English. We’ll measure sentiment at both the word level and sentence level and see how it agrees with the sentiment, positive, negative or neutral, already assigned in the Afrisenti data set."
  },
  {
    "objectID": "posts/2023-04-16-analyze-tweets-with-google-translate-pt-2/index.html#introduction",
    "href": "posts/2023-04-16-analyze-tweets-with-google-translate-pt-2/index.html#introduction",
    "title": "Sentiment Analysis Using Google Translate (Pt. 2)",
    "section": "",
    "text": "Sentiment analysis is a common task in natural language processing. Many of the tools available for this are calibrated to the English language. Can Google Translate let us apply this toolset to a broader set of languages? We will use the tidytext framework to measure sentiment of tweets originally in African languages but translated to English. We’ll measure sentiment at both the word level and sentence level and see how it agrees with the sentiment, positive, negative or neutral, already assigned in the Afrisenti data set."
  },
  {
    "objectID": "posts/2023-04-16-analyze-tweets-with-google-translate-pt-2/index.html#in-our-last-episode",
    "href": "posts/2023-04-16-analyze-tweets-with-google-translate-pt-2/index.html#in-our-last-episode",
    "title": "Sentiment Analysis Using Google Translate (Pt. 2)",
    "section": "In Our Last Episode…",
    "text": "In Our Last Episode…\nIn part one of this series we translated the afrisenti data set using the Google Cloud Translate API. This data set contains a little over 110,000 tweets in 13 different African languages(Muhammad et al. 2023). We saw that , at first glance, Google Translate does a good job on these languages. So now let’s ask:\n\nAre the translations good enough to run sentiment analysis on?\nWill the sentiments we measure agree with the original data? This is a higher standard, of course."
  },
  {
    "objectID": "posts/2023-04-16-analyze-tweets-with-google-translate-pt-2/index.html#measuring-sentiment",
    "href": "posts/2023-04-16-analyze-tweets-with-google-translate-pt-2/index.html#measuring-sentiment",
    "title": "Sentiment Analysis Using Google Translate (Pt. 2)",
    "section": "Measuring Sentiment",
    "text": "Measuring Sentiment\nNow that we have all of the tweets translated into English we can use sentiment tools calibrated to the English language. We will try two different approaches. For both we’ll compare the measured sentiment of the English tweet to the assigned sentiment in the data set.\n\nMeasure the sentiment “valence” of each word as either positive or negative and take the balance of positive vs. negative as the sentiment of the tweet.\nNegation turns a positive sentiment into a negative one. “I do not like” has one positive word, “like,” but is clearly a negative sentiment. Rather than measure each word, we can run a sentence-level model to see if we get a better result."
  },
  {
    "objectID": "posts/2023-04-16-analyze-tweets-with-google-translate-pt-2/index.html#sentiment-measurement-at-the-word-level",
    "href": "posts/2023-04-16-analyze-tweets-with-google-translate-pt-2/index.html#sentiment-measurement-at-the-word-level",
    "title": "Sentiment Analysis Using Google Translate (Pt. 2)",
    "section": "Sentiment Measurement at the Word Level",
    "text": "Sentiment Measurement at the Word Level\nWe will be using the tidytext package in this project and following the approach shown in the “Introduction to tidytext” by Julia Silge and Dave Robinson and Tidy Text Mining(Silge and Robinson 2016).\nFirst load the needed packages and the translated tweets from part one of this project.\n\n#load libraries\nsuppressPackageStartupMessages({\n  library(tidyverse)\n  library(tidytext)\n  library(hunspell)\n  library(sentimentr)\n  library(magrittr)\n})\n\n# set up some chart defaults\ntan1 &lt;- \"#FDECCD\"\nyellow1 &lt;- \"#FFBF00\"\ngreen1 &lt;- \"#007000\"\n\ntheme_afri &lt;- function(...){\n  # making a function allows passing further theme elements\n  ggplot2::theme(\n    plot.background = element_rect(fill = tan1, color = NA),\n    panel.background = element_blank(),\n    legend.background = element_blank(),\n    legend.box.background = element_blank(),\n    legend.key = element_blank(),\n    ...\n    ) \n}\nafrisenti_translated &lt;- readr::read_csv('https://raw.githubusercontent.com/apsteinmetz/tidytuesday/master/2023-02-28_african_language/data/afrisenti_translated.csv',\n                                        show_col_types = FALSE) |&gt;\n  select(tweet_num,assigned_long,translatedText,label) |&gt; \n  mutate(label = as.factor(label))\nafrisenti_translated\n\n# A tibble: 111,720 × 4\n   tweet_num assigned_long translatedText                                  label\n       &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt;                                           &lt;fct&gt;\n 1         1 Amharic       Amhara region needs moderation!!! He hates Mus… nega…\n 2         2 Amharic       Having a mood in a way that annoys someone....  nega…\n 3         3 Amharic       Domestic violence - without hearing - without … nega…\n 4         4 Amharic       Ethiopia, what was your role in overthrowing T… nega…\n 5         5 Amharic       No matter how Oromo learns, they don't realize… nega…\n 6         6 Amharic       Tish, dark powder, are you standing at 97? Tim… nega…\n 7         7 Amharic       Local residents said that the internet and tel… nega…\n 8         8 Amharic       What happened to Schimmels is sad. By the word… nega…\n 9         9 Amharic       An unstable country does not need elections     nega…\n10        10 Amharic       The idiot and the bubble, the Ethiopian PM Abi… nega…\n# ℹ 111,710 more rows\n\n\nThe tidytext package makes it easy to divide each tweet into separate words and measure the sentiment valence of each one. The tidytext package has a few sentiment lexicons. Here, we decide the sentiment of each word using the “afinn” lexicon. We chose this because it has 11 shades of sentiment and we hope the finer granularity will be helpful. If a word is not in the lexicon we code it as”0” or neutral. Further, we need to use the stems of words. “Idiots” is not in our sentiment sources, “Idiot” is. Word stemming will fix that. The hunspell package using hunspell_stem() will do the trick. It returns a list of possible stems (usually just one, but not always) so we have to unnest() the list column. The trade-off is that if the word is not in hunspell’s dictionary, it drops the word. Fortunately, there are over 49,000 words in the dictionary.\n\ntweet_word_sentiment &lt;- afrisenti_translated %&gt;%\n  select(tweet_num, translatedText, label) %&gt;%\n  unnest_tokens(word, translatedText) |&gt;\n  anti_join(stop_words, by = join_by(word)) %&gt;%\n  mutate(word = hunspell_stem(word)) |&gt; \n  unnest(word) |&gt; \n  left_join(get_sentiments(\"afinn\"),\n            multiple = \"all\",\n            by = join_by(word)) %&gt;%\n  mutate(value = replace_na(value, 0)) %&gt;%\n  rename(sentiment_afinn = value)\n\nI don’t know who decided that “superb” gets a “+5” and “euphoric” gets a “+4,” but there you are. We can see how the valences are distributed in our tweets. The vast majority of words are not in the lexicon and therefore are coded as zero, neutral.\n\ntweet_word_sentiment |&gt; \n  ggplot(aes(sentiment_afinn)) + \n  geom_histogram(bins = 10,fill = yellow1,color = tan1,binwidth = 1) + \n  scale_x_continuous(n.breaks = 10,limits = c(-5,+5)) + \n  theme_afri() + \n  labs(title = '\"Afinn\" Sentiment Database Allows More Nuance')\n\n\n\n\nNow let’s look at how stemming works. The tweet below is coded as negative in the Afrisenti data set.\n\nafrisenti_translated$translatedText[45]\n\n[1] \"These idiots are cannibals and unforgivable renegades of the Eritrean people\"\n\n\n\ntweet_word_sentiment |&gt; \n  filter(tweet_num == \"45\") |&gt; \n  select(word,sentiment_afinn)\n\n# A tibble: 5 × 2\n  word       sentiment_afinn\n  &lt;chr&gt;                &lt;dbl&gt;\n1 idiot                   -3\n2 cannibal                 0\n3 forgivable               0\n4 renegade                 0\n5 people                   0\n\n\nThe benefits and drawbacks of stemming are apparent. We can find the stem of “idiots” and “cannibals” but “unforgivable” gets changed to “forgivable” which flips its sentiment. No matter because neither word is in our sentiment lexicon. There are no positive words in this tweet so the sum of all the valences is negative, which matches with the assigned sentiment.\nNext we add up the valences for each tweet to arrive at the net sentiment.\n\ntweet_sentiment &lt;- tweet_word_sentiment %&gt;% \n  group_by(tweet_num) %&gt;% \n  summarise(sentiment_afinn = as.integer(sum(sentiment_afinn))) %&gt;% \n  ungroup() |&gt; \n  full_join(afrisenti_translated,by = join_by(tweet_num)) |&gt; \n  mutate(language = as_factor(assigned_long)) |&gt; \n  rename(label_original = label) |&gt; \n  # set NA sentiment to neutral\n  mutate(across(contains(\"sentiment\"),~replace_na(.x,0)))\n\nWe have numeric valences. If we want to compare our measurment to the original we have to make a choice what to label each number. Obviously, zero is neutral but should we expand the neutral range to include, say -1 and +1? Sadly, no. The summary below shows that we already assigned far more neutral labels than the original data set has. This is not a good omen. We wish we had more granularity.\n\ntweet_sentiment &lt;- tweet_sentiment |&gt; \n  mutate(label_afinn = as.factor(cut(sentiment_afinn,\n                                     breaks = c( -Inf,-1,0,Inf),                         labels=c(\"negative\",\"neutral\",\"positive\"))))  |&gt; \n  select(language,tweet_num,translatedText,label_original,label_afinn)\n\nsummary(select(tweet_sentiment,label_original,label_afinn))\n\n  label_original    label_afinn   \n negative:36564   negative:27564  \n neutral :39151   neutral :51688  \n positive:36005   positive:32468"
  },
  {
    "objectID": "posts/2023-04-16-analyze-tweets-with-google-translate-pt-2/index.html#results",
    "href": "posts/2023-04-16-analyze-tweets-with-google-translate-pt-2/index.html#results",
    "title": "Sentiment Analysis Using Google Translate (Pt. 2)",
    "section": "Results",
    "text": "Results\nHow did we do? Here are samples of positive-scored tweets along with the original sentiment. If it’s not “positive” we disagree.\n\nset.seed(1)\ntweet_sentiment |&gt; \n  filter(label_afinn == \"positive\") |&gt; \n  slice_sample(n=10) |&gt; \n  select(label_original,translatedText)\n\n# A tibble: 10 × 2\n   label_original translatedText                                                \n   &lt;fct&gt;          &lt;chr&gt;                                                         \n 1 positive       \"@user Amen Thanks so much fam... God bless you all\"          \n 2 neutral        \"No need to extend us. https://t.co/cKIft7Aqk5\"               \n 3 positive       \"Our king is one Mohammed VI... Long live the king and long l…\n 4 positive       \"'@user @user @user @user and every time the son of their mou…\n 5 positive       \"@user This problem of insecurity that we are dealing with ma…\n 6 positive       \"Just as our bodies have a great need for food, so even our h…\n 7 neutral        \"RT @user: You definitely can improve your #yoruba by watchin…\n 8 positive       \"@user Tanemi forgiveness and forgiveness, God is the best, G…\n 9 positive       \"@user Don't agree! We want him to continue as president beca…\n10 negative       \"This is the appropriate response that deserves the likes of …\n\n\nOur scoring looks pretty good. Where we disagree with Afrisenti, I side with tidytext for the most part. Having said that, the provided sentiments are human-curated with at least 2 out of 3 native language speakers agreeing on how to code each tweet. So who am I to argue?\nHere are samples of negative-scored tweets with along the original sentiment.\n\nset.seed(5)\ntweet_sentiment |&gt; \n  filter(label_afinn == \"negative\") |&gt; \n  slice_sample(n=10) |&gt; \n  select(label_original,translatedText)\n\n# A tibble: 10 × 2\n   label_original translatedText                                                \n   &lt;fct&gt;          &lt;chr&gt;                                                         \n 1 positive       \"@user Good man, this time is up. The children and the childr…\n 2 neutral        \"who died in power\"                                           \n 3 positive       \"SAY NO TO SUICIDE!!! Reposted from @user - We have some tips…\n 4 negative       \"Hunger has no respect \\U0001fae1\"                            \n 5 negative       \"@user And you started ruining it with lies \\U0001f914\\U0001f…\n 6 positive       \"@user @user @user @user @user @user God forbid evil!\"        \n 7 neutral        \"'It's father Kashi, you can leave this beautifully and go ho…\n 8 neutral        \"Tete provincial finance director arrested https://t.co/guJWN…\n 9 negative       \"@user Just as Barcelona were tired, PSG will also be tired, …\n10 negative       \"u see diz life e no go make u hype somethings see diz otondo…\n\n\nIt’s the same story with the negative tweets. We do a reasonable job and the correctness of the original sentiment is arguable. There is a lot of disagreement.\nWe can generate a confusion matrix with some additional statistics to look at the agreement of our measurements vs. the human classifiers. Ideally all the observations would lie on the diagonal from top left to bottom right.\n\nxt &lt;- table(tweet_sentiment$label_original,tweet_sentiment$label_afinn)\nxt\n\n          \n           negative neutral positive\n  negative    15083   14619     6862\n  neutral      7761   23921     7469\n  positive     4720   13148    18137\n\n\n\ncaret::confusionMatrix(xt) |&gt; \n  broom::tidy() |&gt; \n  slice_head(n=2)\n\n# A tibble: 2 × 6\n  term     class estimate conf.low conf.high    p.value\n  &lt;chr&gt;    &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 accuracy &lt;NA&gt;     0.511    0.509     0.514  2.58e-234\n2 kappa    &lt;NA&gt;     0.264   NA        NA     NA"
  },
  {
    "objectID": "posts/2023-04-16-analyze-tweets-with-google-translate-pt-2/index.html#its-not-me-its-you.",
    "href": "posts/2023-04-16-analyze-tweets-with-google-translate-pt-2/index.html#its-not-me-its-you.",
    "title": "Sentiment Analysis Using Google Translate (Pt. 2)",
    "section": "It’s not me, it’s you.",
    "text": "It’s not me, it’s you.\nAs we look at the cross-tab there are many, many incorrect classifications. The accuracy for both of these measures is scarcely more that 50%. The “Kappa” statistic shows that we are only about 26% better than random chance. It’s not zero but it’s not good. Why the disappointing result? First of all, our valence measure isn’t opinionated enough. There are far too many neutral tweets.\n\navgs &lt;- tweet_sentiment |&gt; \n  group_by(language) |&gt; \n  summarise(across(contains(\"label\"),\n                   ~mean(as.numeric(.x)-2),\n                   .names = \"mean_{str_remove(.col,'label_')}\")) |&gt; \n  pivot_longer(cols = contains(\"mean\"), names_to = \"source\",values_to = \"average\") |&gt; \n  mutate(source = str_remove(source,\"mean_\"))\n\n# plot the sentiment distribution by language\navgs |&gt; ggplot(aes(y = average,x = language)) +\n  geom_point(aes(color = source,shape = source),\n             size = 4) +\n  geom_line() + \n  scale_y_continuous(n.breaks = 5) +\n  scale_shape_manual(values = c(afinn = 15,\n                                sentence=19,original =3)) +\n  scale_color_manual(values = c(afinn = \"red\",\n                                sentence=green1,original =\"black\")) +\n  scale_fill_manual(values = c(afinn = \"red\",\n                               sentence=green1,original =\"black\")) +\n  \n  theme_afri() +\n  labs(y=\"Sentiment\",x=\"Language\",\n       title = \"There is Disagreement About The Average Sentiment\",\n       subtitle = \"Why is English the most divergent?\") +\n  ggplot2::coord_flip()\n\n\n\n\nYou may wonder if one of the other sentiment lexicons would produce a better result. I have tried the others but I don’t include them here because the results are substantially identical.\nIn our defense, I’m not sure the Afrisenti sentiment assignments are better, as we saw above. But maybe that just means Google Translate has stripped some of the emotion out of them that is present in the original language. I don’t know, but this would mean Google Cloud Translate doesn’t work for this purpose.\nHere’s the puzzle, though. The biggest disagreement about sentiment is in English-language tweets, where no translation is needed so we can’t blame Google for this. A look at some English tweets reveals that they are mostly in Pidgin so the vocabulary is not what we would expect in the sentiment sources we’re using. Here are some random English tweets:\n\nafrisenti_translated |&gt; \n  filter(assigned_long == \"English\") |&gt; \n  slice_sample(n=5) |&gt; \n  select(translatedText)\n\n# A tibble: 5 × 1\n  translatedText                                                                \n  &lt;chr&gt;                                                                         \n1 jesus taught i had sef                                                        \n2 better all those figures they are generating if u never dey vex reach na the …\n3 people don dey find themselves from twitter oo and that ilorin where everyone…\n4 it's sweet soul abeg                                                          \n5 as a guyman used to paying the lump sum and calling it a day with the mindset…\n\n\nSo, paradoxically, the translated tweets of truly foreign languages are rendered into more standard English.\nMaybe we’ll get more agreement if we address negation, as mentioned above. The tweet below illustrates the problem.\n\nafrisenti_translated$translatedText[28670]\n\n[1] \"@user Why don't America want the world to be peaceful 🤔🤔🤔\"\n\n\n\ntweet_word_sentiment |&gt; \n  filter(tweet_num == \"28670\")\n\n# A tibble: 3 × 4\n  tweet_num label    word     sentiment_afinn\n      &lt;dbl&gt; &lt;fct&gt;    &lt;chr&gt;              &lt;dbl&gt;\n1     28670 negative user                   0\n2     28670 negative world                  0\n3     28670 negative peaceful               2\n\n\nUnless the negation is itself a negative valence word, many negative tweets will test as positive. In this example “peaceful” is positive but “don’t want” clearly flips it.\nA More Sophisticated Approach\nThere are a number of approaches to solve the negation problem. A simple one is to combine negation words like “no” and “not” with the subsequent one. We will use this approach in our next post attempting machine learning. Here we’ll try sentence-level measurement using the sentimentr package. This package understands negation better. Our test tweet from above is a challenge. This is since the negation of “peaceful” comes eight words before “not.” The default of n.before = 5 doesn’t give us the right answer.\n\nmytweets &lt;- \"Why don't America want the world to be peaceful\"\n\nsentimentr::sentiment(mytweets,n.before = 5)\n\n   element_id sentence_id word_count sentiment\n1:          1           1          9      0.25\n\n\nSetting n.before = Inf captures this but comes at the expense of slower processing speed and potentially more false negatives.\n\nsentimentr::sentiment(mytweets,n.before = Inf)\n\n   element_id sentence_id word_count sentiment\n1:          1           1          9     -0.25\n\n\nWith that noted, lets forge ahead.\nAs before, we start by breaking the tweets up into constituent parts, sentences in this case. Most tweets will be just one sentence of course. Again we compute a sentiment score, this time with the sentiment_by() function. It yields a finer grained sentiment score than our earlier measures.\n\ntweet_sentence &lt;- afrisenti_translated |&gt;\n  mutate(language = as.factor(assigned_long)) |&gt;\n  mutate(sentence = get_sentences(translatedText)) %$%\n  sentiment_by(sentence,list(language,tweet_num),n.before = Inf)\n\n\n\n\n\n\n\nNote\n\n\n\nI am using a less common pipe operator below, %$% from the magrittr package, which expands the list-column created by get_sentences. Normally I would use tidyr::unnest() to do this but it loses the special object class that sentiment_by() needs. The sentimentr package uses the fast data.table vernacular, not the dplyr one, which I mostly use here.\n\n\nBefore we look at the tweet-level sentiment let’s group by language. This lets us see if the sentiment measure is consistent across languages.\n\navgs &lt;- tweet_sentence |&gt; \n  group_by(language) |&gt; \n  summarise(ave_sentiment = mean(ave_sentiment))\n\navgs_orig &lt;- afrisenti_translated |&gt; \n  group_by(assigned_long) |&gt; \n  summarise(ave_sentiment = mean(as.numeric(label)-2))\n\n# plot the sentiment distribution by language\ntweet_sentence |&gt;\n  as_tibble() |&gt;\n  # make the size manageable\n  slice_sample(n=1000) |&gt; \n  group_by(language) |&gt;\n  rename(sentiment = ave_sentiment) |&gt;\n  ggplot() + geom_boxplot(\n    aes(y = sentiment, x = language),\n    fill = NA,\n    color = \"grey70\",\n    width = 0.55,\n    size = 0.35,\n    outlier.color = NA\n  ) +\n  geom_jitter(\n    aes(y = sentiment, x = language),\n    width = 0.35,\n    height = 0,\n    alpha = 0.15,\n    size = 1.5\n  ) +\n  scale_y_continuous(n.breaks = 5) +\n  # theme_bw() +\n  theme_afri() + \n  geom_point(data = avgs,\n    aes(y = ave_sentiment, x = language),\n    colour = \"red\",\n    shape = 18,\n    size = 4) +\n  geom_point(data = avgs_orig,\n    aes(y = ave_sentiment, x = assigned_long),\n    colour = green1,\n    shape = 19,\n    size = 4) +\n  ylab(\"Sentiment\") + xlab(\"Language\") +\n  labs(title = \"Average Sentiment at the Sentence Level is  Consistently Positive\",\n       subtitle = \"There is wide disagreement with English tweets.\") +\n  annotate(\"text\",y = .7,x=7.5,label = \"Red Diamond is Measured\") + \n  annotate(\"text\",y = .7,x=6.5,label = \"Green Circle is Original\") + \n  coord_flip()\n\n\n\n\nThe dots are the measured sentiment of a sample of tweets. The markers are the average measured sentiment (red diamond) and average original sentiment (green circle), across all tweets. The measured sentiment looks more consistent then the original. The tweets in all languages are scored, on average, positive, by our calculations. We obviously disagree with the original. Note that the original 3-level sentiment as been converted to a numeric range, -1 to 1, to compute the green markers, whereas the range for our measurements is much wider.\n\ntweet_sentence |&gt; \n  ggplot(aes(ave_sentiment)) + geom_histogram(fill=yellow1,bins=50) +\n  scale_x_continuous(limits = c(-3,3))+\n  theme_afri()\n\n\n\n\nWe no longer have the problem where too many tweets are neutral. The median of the distribution above is zero but there are far fewer tweets that are of exactly zero valence. This let’s us expand the range of neutral above and below zero. I tried to balance the data to match the distribution of sentiment in the original data as closely as possible.\nWhile we’re at it let’s put our results together.\n\n# picked these levels to balance the data set\nlow = -0.01\nhigh = 0.12\ntweet_sentence &lt;- tweet_sentence |&gt; \n  as_tibble() |&gt; \n  mutate(label_sentence = cut(ave_sentiment,\n                              breaks = c( -Inf,low,high,Inf), labels=c(\"negative\",\"neutral\",\"positive\")))\n\ntweet_sentiment &lt;-\n  left_join(tweet_sentiment, tweet_sentence,\n            by = join_by(language, tweet_num))\n\nCompare our distribution to the original.\n\ntweet_sentiment |&gt; \n    pivot_longer(c(label_original,label_sentence),\n               names_to = \"source\",values_to = \"sentiment\") |&gt; \n  group_by(source,sentiment) |&gt; \n  count() |&gt; \n  group_by(source) |&gt; \n  reframe(sentiment,proportion = n/sum(n)) |&gt; \n  ungroup() |&gt; \n  ggplot(aes(sentiment,proportion,fill=source)) + geom_col(position = \"dodge\") + \n  scale_fill_manual(values = c(yellow1,green1)) + \n    theme_afri() + \n  labs(title = 'Choose Valence Range of \"Neutral\" to Balance Outcomes')"
  },
  {
    "objectID": "posts/2023-04-16-analyze-tweets-with-google-translate-pt-2/index.html#a-dissapointing-outcome",
    "href": "posts/2023-04-16-analyze-tweets-with-google-translate-pt-2/index.html#a-dissapointing-outcome",
    "title": "Sentiment Analysis Using Google Translate (Pt. 2)",
    "section": "A Dissapointing Outcome",
    "text": "A Dissapointing Outcome\nAs you can see, we are pretty balanced. This is kind of cheating because we are calibrating the category breakpoints to the known data. We should be calibrating to a training set and then see how it works on the test set. Let’s just see if the in-sample fit is any good first.\n\nxt &lt;- with(tweet_sentiment,table(label_original,label_sentence))\nxt\n\n              label_sentence\nlabel_original negative neutral positive\n      negative    17776   11510     7278\n      neutral      9624   18651    10876\n      positive     5558    9859    20588\n\n\n\ncaret::confusionMatrix(xt) |&gt; \n  broom::tidy() |&gt; \n  slice_head(n=2)\n\n# A tibble: 2 × 6\n  term     class estimate conf.low conf.high p.value\n  &lt;chr&gt;    &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 accuracy &lt;NA&gt;     0.510    0.507     0.513       0\n2 kappa    &lt;NA&gt;     0.265   NA        NA          NA\n\n\nWow. That’s disappointing. The agreement with the original sentiment is nearly identical to the word-level measurements. That is, poor. I thought balancing the outcomes to match the frequency of the original would help, at least.\nDo our two measures agree with each other?\n\nxt &lt;- with(tweet_sentiment,table(label_afinn,label_sentence))\nxt\n\n           label_sentence\nlabel_afinn negative neutral positive\n   negative    18667    4586     4311\n   neutral      9301   29272    13115\n   positive     4990    6162    21316\n\n\n\ncaret::confusionMatrix(xt) |&gt; \n  broom::tidy() |&gt; \n  slice_head(n=2)\n\n# A tibble: 2 × 6\n  term     class estimate conf.low conf.high p.value\n  &lt;chr&gt;    &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 accuracy &lt;NA&gt;     0.620    0.617     0.623       0\n2 kappa    &lt;NA&gt;     0.425   NA        NA          NA"
  },
  {
    "objectID": "posts/2023-04-16-analyze-tweets-with-google-translate-pt-2/index.html#conclusion",
    "href": "posts/2023-04-16-analyze-tweets-with-google-translate-pt-2/index.html#conclusion",
    "title": "Sentiment Analysis Using Google Translate (Pt. 2)",
    "section": "Conclusion",
    "text": "Conclusion\nIf we take the sentiment labels in the original data set as “true,” our sentiment valence measurements in English do a poor job. A cursory visual examination reveals that the sentiment we assigned with our valence measures look pretty reasonable in most cases but do not predict what is in the Afrisenti data set very well. We therefore conclude that Google Cloud Translate doesn’t match well with these tweets using tidytext methods.\nWe’re not defeated yet. Our measurements had no knowledge of what the original sentiments were. What if we could learn the inscrutable methods used to assign sentiment in the original? In the next post we’ll apply machine learning to see if we can train a model using the translated tweets that is in closer agreement with the Afrisenti data set."
  },
  {
    "objectID": "posts/2023-04-22-sentiment-analysis-using-google-translate-pt-3/index.html",
    "href": "posts/2023-04-22-sentiment-analysis-using-google-translate-pt-3/index.html",
    "title": "Sentiment Analysis Using Google Translate (Pt. 3 - Machine Learning)",
    "section": "",
    "text": "In Part 2 of this series we learned that summing the valence measurement of the words in the Afrisenti data set did not agree well with the sentiments already provided. While the sentiments we calculated made sense in general, they were measured without any knowledge of how the original sentiments were determined. For this post we will apply machine learning techniques to try to reverse engineer the thinking that went into the sentiment assignments.\nIn our previous analysis we were only in agreement with original data about 50% of the time. Can our trained models do any better? If we pass the tweets through Google Translate first to convert them all to English, will we improve or worsen the accuracy of the model? Let’s see.\nWe will take the usual machine learning approach of splitting the data into test and training sets, then run a classifier model on the training set and finally validate it against the test set.\n\nsuppressPackageStartupMessages({\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(quanteda)\nlibrary(tictoc)\nlibrary(xgboost)})\n\n\n# set up some chart defaults\ntan1 &lt;- \"#FDECCD\"\nyellow1 &lt;- \"#FFBF00\"\ngreen1 &lt;- \"#007000\"\n\ntheme_afri &lt;- function(...){\n  # making a function allows passing further theme elements\n  ggplot2::theme(\n    plot.background = element_rect(fill = tan1, color = NA),\n    panel.background = element_blank(),\n    legend.background = element_blank(),\n    legend.box.background = element_blank(),\n    legend.key = element_blank(),\n    ...\n    ) \n}\n# the the previously translated tweets.\nafrisenti_translated &lt;- readr::read_csv('https://raw.githubusercontent.com/apsteinmetz/tidytuesday/master/2023-02-28_african_language/data/afrisenti_translated.csv',\n                                        show_col_types = FALSE) |&gt;\n  mutate(lang = as.factor(assigned_long)) |&gt; \n  mutate(sentiment = as.factor(label)) |&gt; \n  mutate(intended_use = as.factor(intended_use)) |&gt; \n  select(lang,tweet_num,sentiment,translatedText,tweet,intended_use)\nafrisenti_translated\n\n# A tibble: 111,720 × 6\n   lang    tweet_num sentiment translatedText                 tweet intended_use\n   &lt;fct&gt;       &lt;dbl&gt; &lt;fct&gt;     &lt;chr&gt;                          &lt;chr&gt; &lt;fct&gt;       \n 1 Amharic         1 negative  Amhara region needs moderatio… አማራ … dev         \n 2 Amharic         2 negative  Having a mood in a way that a… ሰውን … dev         \n 3 Amharic         3 negative  Domestic violence - without h… የቤት … dev         \n 4 Amharic         4 negative  Ethiopia, what was your role … Ethi… dev         \n 5 Amharic         5 negative  No matter how Oromo learns, t… ኦሮሞ … dev         \n 6 Amharic         6 negative  Tish, dark powder, are you st… ቲሽ ጨ… dev         \n 7 Amharic         7 negative  Local residents said that the… በምዕራ… dev         \n 8 Amharic         8 negative  What happened to Schimmels is… ያየሰው… dev         \n 9 Amharic         9 negative  An unstable country does not … ያልተረ… dev         \n10 Amharic        10 negative  The idiot and the bubble, the… ደደቡ … dev         \n# ℹ 111,710 more rows"
  },
  {
    "objectID": "posts/2023-04-22-sentiment-analysis-using-google-translate-pt-3/index.html#introduction",
    "href": "posts/2023-04-22-sentiment-analysis-using-google-translate-pt-3/index.html#introduction",
    "title": "Sentiment Analysis Using Google Translate (Pt. 3 - Machine Learning)",
    "section": "",
    "text": "In Part 2 of this series we learned that summing the valence measurement of the words in the Afrisenti data set did not agree well with the sentiments already provided. While the sentiments we calculated made sense in general, they were measured without any knowledge of how the original sentiments were determined. For this post we will apply machine learning techniques to try to reverse engineer the thinking that went into the sentiment assignments.\nIn our previous analysis we were only in agreement with original data about 50% of the time. Can our trained models do any better? If we pass the tweets through Google Translate first to convert them all to English, will we improve or worsen the accuracy of the model? Let’s see.\nWe will take the usual machine learning approach of splitting the data into test and training sets, then run a classifier model on the training set and finally validate it against the test set.\n\nsuppressPackageStartupMessages({\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(quanteda)\nlibrary(tictoc)\nlibrary(xgboost)})\n\n\n# set up some chart defaults\ntan1 &lt;- \"#FDECCD\"\nyellow1 &lt;- \"#FFBF00\"\ngreen1 &lt;- \"#007000\"\n\ntheme_afri &lt;- function(...){\n  # making a function allows passing further theme elements\n  ggplot2::theme(\n    plot.background = element_rect(fill = tan1, color = NA),\n    panel.background = element_blank(),\n    legend.background = element_blank(),\n    legend.box.background = element_blank(),\n    legend.key = element_blank(),\n    ...\n    ) \n}\n# the the previously translated tweets.\nafrisenti_translated &lt;- readr::read_csv('https://raw.githubusercontent.com/apsteinmetz/tidytuesday/master/2023-02-28_african_language/data/afrisenti_translated.csv',\n                                        show_col_types = FALSE) |&gt;\n  mutate(lang = as.factor(assigned_long)) |&gt; \n  mutate(sentiment = as.factor(label)) |&gt; \n  mutate(intended_use = as.factor(intended_use)) |&gt; \n  select(lang,tweet_num,sentiment,translatedText,tweet,intended_use)\nafrisenti_translated\n\n# A tibble: 111,720 × 6\n   lang    tweet_num sentiment translatedText                 tweet intended_use\n   &lt;fct&gt;       &lt;dbl&gt; &lt;fct&gt;     &lt;chr&gt;                          &lt;chr&gt; &lt;fct&gt;       \n 1 Amharic         1 negative  Amhara region needs moderatio… አማራ … dev         \n 2 Amharic         2 negative  Having a mood in a way that a… ሰውን … dev         \n 3 Amharic         3 negative  Domestic violence - without h… የቤት … dev         \n 4 Amharic         4 negative  Ethiopia, what was your role … Ethi… dev         \n 5 Amharic         5 negative  No matter how Oromo learns, t… ኦሮሞ … dev         \n 6 Amharic         6 negative  Tish, dark powder, are you st… ቲሽ ጨ… dev         \n 7 Amharic         7 negative  Local residents said that the… በምዕራ… dev         \n 8 Amharic         8 negative  What happened to Schimmels is… ያየሰው… dev         \n 9 Amharic         9 negative  An unstable country does not … ያልተረ… dev         \n10 Amharic        10 negative  The idiot and the bubble, the… ደደቡ … dev         \n# ℹ 111,710 more rows"
  },
  {
    "objectID": "posts/2023-04-22-sentiment-analysis-using-google-translate-pt-3/index.html#explore-the-data-set",
    "href": "posts/2023-04-22-sentiment-analysis-using-google-translate-pt-3/index.html#explore-the-data-set",
    "title": "Sentiment Analysis Using Google Translate (Pt. 3 - Machine Learning)",
    "section": "Explore the Data Set",
    "text": "Explore the Data Set\n\nsummary(afrisenti_translated)\n\n      lang         tweet_num         sentiment     translatedText    \n Hausa  :22152   Min.   :     1   negative:36564   Length:111720     \n Igbo   :15715   1st Qu.: 27931   neutral :39151   Class :character  \n Yoruba :15127   Median : 55861   positive:36005   Mode  :character  \n Arabic :12061   Mean   : 55861                                      \n English:10556   3rd Qu.: 83790                                      \n Amharic: 9480   Max.   :111720                                      \n (Other):26629                                                       \n    tweet           intended_use \n Length:111720      dev  :13728  \n Class :character   test :34307  \n Mode  :character   train:63685  \n                                 \n                                 \n                                 \n                                 \n\n\nThe data set is already tagged into training, test and “dev” sets. The training set is twice the size of the test set. What is “dev?” I don’t know if this split is random or not but we are concerned whether the profile of the training set is similar to the test set. Let’s split it according to the tags.\n\ntweet_train &lt;- afrisenti_translated |&gt; \n  filter(intended_use == \"train\") |&gt; \n  select(tweet_num,sentiment,lang,tweet,translatedText)\n\ntweet_test &lt;- afrisenti_translated |&gt; \n  filter(intended_use == \"test\") |&gt; \n  select(tweet_num,sentiment,lang,tweet,translatedText)\n\ntweet_dev &lt;- afrisenti_translated |&gt; \n  filter(intended_use == \"dev\") |&gt; \n  select(tweet_num,sentiment,lang,tweet,translatedText)\n\nNow we’ll see if the training set is representative of the test set. Do the languages align?\n\nafrisenti_translated |&gt; \nggplot(aes(lang,group=intended_use)) + \n   geom_bar(aes(y = after_stat(prop)),fill = yellow1) + \n          scale_y_continuous(labels=scales::percent) +\n  theme_afri(axis.text.x = element_text()) +\n  coord_flip() +\n  facet_grid(~intended_use) + \n  labs(title = \"Splits Are Reasonably Aligned by Language\",\n       y = \"Proportion\", x= \"Language\")\n\n\n\n\nLooks okay.\nDo the sentiments align?\n\nafrisenti_translated |&gt; \nggplot(aes(sentiment,group=intended_use)) + \n   geom_bar(aes(y = after_stat(prop)),fill=yellow1) + \n          scale_y_continuous(labels=scales::percent) +\n  theme_afri(axis.text.x = element_text()) +\n  facet_grid(~intended_use) + \n  coord_flip() + \n  labs(title = \"Splits Are Balanced by Sentiment\",\n       y = \"Proportion\", x= \"Sentiment\")\n\n\n\n\nThe splits are well balanced."
  },
  {
    "objectID": "posts/2023-04-22-sentiment-analysis-using-google-translate-pt-3/index.html#approach-to-the-problem",
    "href": "posts/2023-04-22-sentiment-analysis-using-google-translate-pt-3/index.html#approach-to-the-problem",
    "title": "Sentiment Analysis Using Google Translate (Pt. 3 - Machine Learning)",
    "section": "Approach to the Problem",
    "text": "Approach to the Problem\nThe structure of our model is basically a regression with one dependent variable and thousands of independent variables which are all of the words (“tokens”) in all the tweets. This is a “document feature matrix” (DFM). What goes in the cells of the matrix? One approach would simply code each of the words by their presence or absence in the tweet. A more nuanced approach is to code each word in each tweet by how important it is in the tweet. “tf-idf”, is sort of a uniqueness measure. This has the added benefit of down-ranking stop words that appear very frequently all over the place, even if we have no stop-word lexicon for a certain language.\nThere are several machine learning models we might try. There are two R packages suitable for classifiers where there are more than two categores that also work with sparse matrices (see below), ranger and xgboost. In preview, we will use xgboost here because the results are about same but xgboost is much faster. Here is a short comparison of different machine learning approaches to sentiment analysis.(Saifullah, Fauziah, and Aribowo 2021)\n\n\n\n\n\n\nWhy not tidymodels?\n\n\n\nI set out to use RStudio’s suite of machine learning workflow packages, tidymodels, in this project, but quickly became frustrated. I could not get the wrapper around xgboost to give a sensible result (it’s probably my fault) and there is a bug in the wrapper around ranger that prevents making predictions with the model. So we’ll do it the old-fashioned way."
  },
  {
    "objectID": "posts/2023-04-22-sentiment-analysis-using-google-translate-pt-3/index.html#pre-processing.",
    "href": "posts/2023-04-22-sentiment-analysis-using-google-translate-pt-3/index.html#pre-processing.",
    "title": "Sentiment Analysis Using Google Translate (Pt. 3 - Machine Learning)",
    "section": "Pre-Processing.",
    "text": "Pre-Processing.\nThere are some things we can to do boost our chances of a satisfying outcome. As noted above we’re going to build a matrix with the tweet numbers, an arbitrary index to uniquely identify each tweet (“document”), as the rows, and each word in every tweet (“feature”) as the columns. This will create a “document feature matrix.”\nBefore we create the matrix, we first split all the tweets into individual words (“tokens”) and refine that list to make it more managable. Let’s create a few functions to help us with this.\nWhat do we do when a negation flips the sentiment of a tweet? “I Love” is positive but “I do not love” is negative. In our previous post we tried sentence-level analysis to handle negation. Here we are doing word level training. We will address this by creating new tokens where any instance of, say “not love” is replaced by “not_love,” an entirely new word for our analysis. This is very crude and only includes English (9% of the tweets), but it’s something. It’s actually simpler to do use the str_replace_all(tweet,\"not |no \",\"not_\") function to do this before we tokenize the tweets.\nWhen we measured the tweets using a sentiment lexicon, only the words in the lexicon contributed to the sentiment measurement. Everything else was neutral. With machine learning everything counts and the more words we have, the bigger the model and the longer it will take to train. It is common in analyzing text to drop low-information words or “stop words.” In English, we drop words like “the” and “that.” We want to build a list of stop words relevant to our data set. On Kaggle I found a list of stop words in various African languages. It doesn’t cover every language in our data set but will reduce the matrix size a bit. We’ll add that to the lexicon of English stop words and a custom lexicon built from a quick inspection of the data set. In practice, the tf-idf score is the biggest indicator of a low-information word, irrespective of language.\n\nstop_words_af &lt;- readr::read_csv('https://raw.githubusercontent.com/apsteinmetz/tidytuesday/master/data/stopwords_af.csv', show_col_types = FALSE)\n\n# add my stop words to defaults\nmy_stop_words = tibble(word = c(\"http\",\"https\",\"dey\",\"de\",\"al\",\"url\",\"na\",\"t.co\",\"rt\",\"user\",\"users\",\"wey\",\"don\",\n                                as.character(1:100)))\n                           \n\n# make a stopword list of any 1-character words\n# this is a somewhat arbitrary rubric for African language stop words\nstop_words_1char &lt;- afrisenti_translated |&gt; \n  unnest_tokens(word,tweet) |&gt; \n  select(word) |&gt; \n  filter(str_length(word)&lt;2) |&gt; \n  unique()\n\nfull_stop_words &lt;-  c(\n  tidytext::stop_words$word,\n  my_stop_words$word,\n  stop_words_af$word,\n  stop_words_1char$word\n) |&gt; \n  enframe(value = \"word\")\n\nremove_stopwords &lt;- function(tokens) {\n  tokens |&gt;\n    # remove stop words\n    anti_join(full_stop_words)\n}\n\nDid we say every word? Well, not EVERY word. 260,000 is more than we can handle so let’s create a helper function to prune the data set to only the words with the highest frequency.\n\nonly_top_words &lt;- function(tokens, word_count = 2000) {\n  chosen_words &lt;- tokens |&gt;\n    ungroup() |&gt;\n    select(word) |&gt;\n    count(word) |&gt;\n    slice_max(order_by = n, n = word_count) |&gt; \n    select(-n)\n  return(inner_join(tokens,chosen_words))\n}\n\nEven after pruning there are over 2,000 unique words in this set of tweets. 2,000 variables and over 110,000 tweets. That’s a pretty big matrix, over 120 million elements, but the vast majority of those elements are filled with zero. We can make the memory size of this monster manageable by using a “sparse matrix.” Such a matrix describes what elements are empty without actually populating them. Fortunately both ranger and xgboost understand sparse matrices.\nThe function make_dfm is our workhorse. It takes the raw tweet data and turns it into sparse document feature matrix after applying our pre-processing steps. Note that the matrix does not contain our independent variable, “sentiment.” That is a separate vector we attach to the matrix as a docvar, part of a quanteda::dfm object.\nNote the order of our pre-processing matters. First we create negation tokens, then we prune stop words, then we compute the tf-idf and finally we take the top words. By trial and error I learned that computing each word’s tf-idf against the whole data set before choosing the top words yields a better result. Spoiler alert: around 2000 words is the sweet spot.\n\n# make sparse document-feature matrix\n\nmake_dfm &lt;- function(tweet_data, translated = FALSE, num_words = 1000) {\n    if (translated) {\n      tweet_tokens &lt;- tweet_data |&gt;\n        mutate(translatedText = str_replace_all(translatedText, \"no |not \", \"not_\")) |&gt;\n        select(tweet_num, translatedText) |&gt;\n        unnest_tokens(word, translatedText)\n    } else{\n      tweet_tokens &lt;- tweet_data |&gt;\n        mutate(tweet = str_replace_all(tweet, \"no |not \", \"not_\")) |&gt;\n        select(tweet_num, tweet) |&gt;\n        unnest_tokens(word, tweet)\n    }\n    tweet_tokens &lt;- tweet_tokens |&gt; \n  remove_stopwords() |&gt;\n  count(tweet_num, word) |&gt;\n  bind_tf_idf(word, tweet_num, n) |&gt;\n  only_top_words(num_words) |&gt;\n  select(tweet_num, word, tf_idf)\n\nsentiment_subset &lt;- tweet_tokens |&gt; \n  slice_head(n=1,by=tweet_num) |&gt; \n  left_join(tweet_data) |&gt; \n  pull(sentiment)\n\ntweet_dfm &lt;- cast_dfm(tweet_tokens,tweet_num,word,tf_idf)\n# add dependent variable to sparse matrix\ndocvars(tweet_dfm,\"sentiment\") &lt;- sentiment_subset\n\nreturn(tweet_dfm)\n}"
  },
  {
    "objectID": "posts/2023-04-22-sentiment-analysis-using-google-translate-pt-3/index.html#train-on-african-language-tweets",
    "href": "posts/2023-04-22-sentiment-analysis-using-google-translate-pt-3/index.html#train-on-african-language-tweets",
    "title": "Sentiment Analysis Using Google Translate (Pt. 3 - Machine Learning)",
    "section": "Train on African Language Tweets",
    "text": "Train on African Language Tweets\nWe’ll establish a baseline by training a model on the African-language tweets.\nNote that we don’t care what language the token is. It could be any language or no language. It could be an emoji, as long as it is associated with a sentiment. There is a risk that the same word could convey the opposite sentiment in two different languages but I assume it is rare enough to ignore.\n\n# more words in common in the translated word list\ntranslated = FALSE\ntweet_train_dfm &lt;- make_dfm(tweet_train,translated = translated,num_words = 2000)\ntweet_test_dfm &lt;- make_dfm(tweet_test,translated = translated,num_words = 2000)\n\nHow sparse is the training DFM? 99.8% of the the entries are zero.\nAfter creating DFMs for both training and testing we see that the DFMs have about 3/4 of the words in common so there is a good bit of information for prediction out-of-sample.\n\n# how close are the word lists?}\n# more words in common in the translated word list\ninner_join(enframe(dimnames(tweet_train_dfm)[[2]]),\n           enframe(dimnames(tweet_test_dfm)[[2]]),\n           by = \"value\") |&gt; nrow() |&gt; paste(\"Words in both train and test sets\")\n\n[1] \"1508 Words in both train and test sets\"\n\n\nRunning predictions on a test set requires the feature list of the training and test set be the same. Three quarters, but not all, of the tokens overlap in our DFMs. The dfm_match function will ensure the test set features are congruent with the training set.\n\n# make sure test set has all variables in both train and test sets\ntweet_test_dfm &lt;- dfm_match(tweet_test_dfm, \n                      features = featnames(tweet_train_dfm))\n\nWe will use the gradient boosted tree approach to training our model. An excellent introduction to the theory is contatined in the documentation to the xgboost package, which is available for many languages, by the way.\nThis is a supervised model, meaning we know all the possible predictions ahead of time. In this case, “negative,” “neutral” and “positive.” A slight “gotcha” is xgboost requires numeric classifiers, with the first one as “0.” As such, we convert our dependent variable, which is a factor, to numeric and then covert it back after predicting.\n\n# run the models\ntic()\nxg_fit &lt;- xgboost(\n  data = tweet_train_dfm,\n  max.depth = 100,\n  nrounds = 100,\n  objective = \"multi:softmax\",\n  num_class = 3,\n  label = as.numeric(tweet_train_dfm$sentiment)-1,\n  print_every_n = 10\n)\n\n[1] train-mlogloss:0.984030 \n[11]    train-mlogloss:0.664378 \n[21]    train-mlogloss:0.579607 \n[31]    train-mlogloss:0.530083 \n[41]    train-mlogloss:0.493999 \n[51]    train-mlogloss:0.465732 \n[61]    train-mlogloss:0.441981 \n[71]    train-mlogloss:0.422400 \n[81]    train-mlogloss:0.404571 \n[91]    train-mlogloss:0.389673 \n[100]   train-mlogloss:0.377797 \n\ntoc()\n\n61.94 sec elapsed"
  },
  {
    "objectID": "posts/2023-04-22-sentiment-analysis-using-google-translate-pt-3/index.html#results-with-native-language-tweets",
    "href": "posts/2023-04-22-sentiment-analysis-using-google-translate-pt-3/index.html#results-with-native-language-tweets",
    "title": "Sentiment Analysis Using Google Translate (Pt. 3 - Machine Learning)",
    "section": "Results With Native Language Tweets",
    "text": "Results With Native Language Tweets\nIn the interest of space, we won’t go over tuning the model or cross validation, both of which are used to optimize performance. I played around with the parameters of the model and settled on the ones used here as roughly optimal. If we plot the loss function below we see the diminishing marginal return to additional training rounds. Adding more rounds continues to improve the fit to the training set (in theory we would ultimately achieve a perfect fit) but the fit to the test set doesn’t improve at all.\n\nxg_fit$evaluation_log |&gt; \n  ggplot(aes(iter,train_mlogloss)) + geom_line() +\n  theme_afri()\n\n\n\n\nAlas, the accuracy of our machine learning model is not much better than our simple valence measurements from the last post.\n\n# predict and convert classes back to factors\npredicted &lt;- predict(xg_fit,tweet_test_dfm) |&gt; \n  as.factor()\nlevels(predicted) &lt;- levels(tweet_test$sentiment)\n\npredicted_for_table &lt;- tibble(actual = tweet_test_dfm$sentiment,\n                              predicted)\n\n\ncaret::confusionMatrix(table(predicted_for_table))\n\nConfusion Matrix and Statistics\n\n          predicted\nactual     negative neutral positive\n  negative     5524    4598     1261\n  neutral      2489    7011     1182\n  positive     2100    3287     4981\n\nOverall Statistics\n                                          \n               Accuracy : 0.5401          \n                 95% CI : (0.5346, 0.5455)\n    No Information Rate : 0.4593          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.3095          \n                                          \n Mcnemar's Test P-Value : &lt; 2.2e-16       \n\nStatistics by Class:\n\n                     Class: negative Class: neutral Class: positive\nSensitivity                   0.5462         0.4707          0.6709\nSpecificity                   0.7375         0.7907          0.7846\nPos Pred Value                0.4853         0.6563          0.4804\nNeg Pred Value                0.7820         0.6375          0.8893\nPrevalence                    0.3118         0.4593          0.2289\nDetection Rate                0.1703         0.2162          0.1536\nDetection Prevalence          0.3510         0.3294          0.3197\nBalanced Accuracy             0.6419         0.6307          0.7278"
  },
  {
    "objectID": "posts/2023-04-22-sentiment-analysis-using-google-translate-pt-3/index.html#train-on-english-translated-tweets",
    "href": "posts/2023-04-22-sentiment-analysis-using-google-translate-pt-3/index.html#train-on-english-translated-tweets",
    "title": "Sentiment Analysis Using Google Translate (Pt. 3 - Machine Learning)",
    "section": "Train on English Translated Tweets",
    "text": "Train on English Translated Tweets\n\n# more words in common in the translated word list\ntranslated = TRUE\ntweet_train_dfm &lt;- make_dfm(tweet_train,translated = translated,num_words = 2000)\ntweet_test_dfm &lt;- make_dfm(tweet_test,translated = translated,num_words = 2000)\n\nNow how many words are shared among the training and test sets? If there is a case to be made for a better outcome with the translated tweets, this is it. We have more words in common across the training and test sets since, by converting 13 languages to 1, we have fewer unique words and thus more information in each word. In practice this doesn’t make much of a difference. There are less than 200 additional words in common. Surprising.\n\n# how close are the word lists?}\n# more words in common in the translated word list\ninner_join(enframe(dimnames(tweet_train_dfm)[[2]]),\n           enframe(dimnames(tweet_test_dfm)[[2]]),\n           by = \"value\") |&gt; nrow() |&gt; paste(\"Words in both train and test sets\")\n\n[1] \"1678 Words in both train and test sets\"\n\n\nOnce again we match the features.\n\n# make sure test set has all variables in both train and test sets\ntweet_test_dfm &lt;- dfm_match(tweet_test_dfm, \n                      features = featnames(tweet_train_dfm))\n\nWe will use the same model parameters as we did with the native tweets.\n\n# run the models\ntic()\nxg_fit &lt;- xgboost(\n  data = tweet_train_dfm,\n  max.depth = 100,\n  nrounds = 100,\n  objective = \"multi:softmax\",\n  num_class = 3,\n  label = as.numeric(tweet_train_dfm$sentiment)-1,\n  print_every_n = 10\n)\n\n[1] train-mlogloss:1.000106 \n[11]    train-mlogloss:0.721196 \n[21]    train-mlogloss:0.647356 \n[31]    train-mlogloss:0.604540 \n[41]    train-mlogloss:0.573208 \n[51]    train-mlogloss:0.548874 \n[61]    train-mlogloss:0.528251 \n[71]    train-mlogloss:0.511289 \n[81]    train-mlogloss:0.495794 \n[91]    train-mlogloss:0.482234 \n[100]   train-mlogloss:0.470904 \n\ntoc()\n\n58.2 sec elapsed\n\n\nThe loss function looks the same\n\nxg_fit$evaluation_log |&gt; \n  ggplot(aes(iter,train_mlogloss)) + geom_line() + \n  theme_afri()"
  },
  {
    "objectID": "posts/2023-04-22-sentiment-analysis-using-google-translate-pt-3/index.html#results-with-english-language-tweets",
    "href": "posts/2023-04-22-sentiment-analysis-using-google-translate-pt-3/index.html#results-with-english-language-tweets",
    "title": "Sentiment Analysis Using Google Translate (Pt. 3 - Machine Learning)",
    "section": "Results With English Language Tweets",
    "text": "Results With English Language Tweets\n\n# predict and convert classes back to factors\npredicted &lt;- predict(xg_fit,tweet_test_dfm) |&gt; \n  as.factor()\nlevels(predicted) &lt;- levels(tweet_test$sentiment)\n\npredicted_for_table &lt;- tibble(actual = tweet_test_dfm$sentiment,\n                              predicted)\n\n\ncaret::confusionMatrix(table(predicted_for_table))\n\nConfusion Matrix and Statistics\n\n          predicted\nactual     negative neutral positive\n  negative     5678    4146     1432\n  neutral      2233    6607     1483\n  positive     1905    3245     5178\n\nOverall Statistics\n                                          \n               Accuracy : 0.5473          \n                 95% CI : (0.5418, 0.5528)\n    No Information Rate : 0.4387          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.3217          \n                                          \n Mcnemar's Test P-Value : &lt; 2.2e-16       \n\nStatistics by Class:\n\n                     Class: negative Class: neutral Class: positive\nSensitivity                   0.5784         0.4720          0.6398\nSpecificity                   0.7475         0.7925          0.7837\nPos Pred Value                0.5044         0.6400          0.5014\nNeg Pred Value                0.7996         0.6576          0.8649\nPrevalence                    0.3076         0.4387          0.2536\nDetection Rate                0.1780         0.2071          0.1623\nDetection Prevalence          0.3528         0.3235          0.3237\nBalanced Accuracy             0.6630         0.6323          0.7118"
  },
  {
    "objectID": "posts/2023-04-22-sentiment-analysis-using-google-translate-pt-3/index.html#conclusion",
    "href": "posts/2023-04-22-sentiment-analysis-using-google-translate-pt-3/index.html#conclusion",
    "title": "Sentiment Analysis Using Google Translate (Pt. 3 - Machine Learning)",
    "section": "Conclusion",
    "text": "Conclusion\nAgain, we see a disappointing result. Machine learning did not significantly increase the accuracy of our sentiment measurement. It is interesting that translating the tweets to English DOES improve the model accuracy, but by a puny amount. I’m not sure the conclusion would survive cross-validation. Here’s the summary for all the methods we’ve done.\n\n\n\n\n\n\n\n\n\n\n\nValence by Word\nValence by Sentence\nXGBoost - Native\nXGBoost - English\n\n\n\n\nAccuracy\n51%\n51%\n54%\n55%\n\n\nKappa Statistic\n26%\n27%\n31%\n32%\n\n\n\nObviously, we’re doing “toy” models. Serious researchers on this subject have achieved better results. In particular, a model called “AfroXLMR-Large” cited in the original paper (Muhammad et al. 2023) achieved an average accuracy across all languages of 67.2% on this data set. This is a pre-trained model with over 270 million variables.\nHopefully this was useful as a introduction to some of the techniques used in sentiment analysis and the R tools we can use for the task.\nIn the final post in this series we’ll look at something of more general interest. There has been much talk about how the “Global South” feels neglected by the rich countries in the northern hemisphere. Can we use the sentiments expressed in the Afrisenti data set to get feelings about specific regions?"
  },
  {
    "objectID": "posts/2023-04-30-sentiment-analysis-using-google-translate-pt-4--a-painful-sight/index.html",
    "href": "posts/2023-04-30-sentiment-analysis-using-google-translate-pt-4--a-painful-sight/index.html",
    "title": "Sentiment Analysis Using Google Translate (Pt. 4 - A Painful Sight)",
    "section": "",
    "text": "Many of us in the U.S. have been surprised by the indifference many in less developed countries have shown about the Ukraine war. There has been much talk lately about how the “Global South” is feeling slighted by the rich countries in the northern hemisphere. The Afrisenti data set (Muhammad et al. 2023) is a collection of over 110,000 tweets from Africa. Its purpose is to train language sentiment models but maybe it can give us some insight into how Africans view different regions of the “Global North.”\nFor the final bite of the apple in this series, we’ll look at the translated tweets and tally the sentiment expressed in each tweet mentioning the U.S., Russia, China and/or Europe. What is the balance of sentiment expressed in this data set about each of these regions?\nThere are some major caveats with this social science experiment. First, while we are using the sentiments coded by native language-speaking curators, there is no stipulation that the sentiment toward a region and the sentiment of the tweet are the same. “I hate enemies of Russia” is a negative tweet, although positive toward Russia. Second, the sample size, while large overall, contains merely hundreds of tweets mentioning our regions of interest. We can’t claim this is representative. Finally, we don’t know the dates of the tweets. We know by some of the news events they reference they probably span the period from 2012 to sometime in 2022. The start of the Ukraine war at the beginning of 2022 and other World events may have shifted attitudes but we can’t split up the tweets by time.\nUnlike our previous posts in this series, this one will be mercifully short."
  },
  {
    "objectID": "posts/2023-04-30-sentiment-analysis-using-google-translate-pt-4--a-painful-sight/index.html#introduction",
    "href": "posts/2023-04-30-sentiment-analysis-using-google-translate-pt-4--a-painful-sight/index.html#introduction",
    "title": "Sentiment Analysis Using Google Translate (Pt. 4 - A Painful Sight)",
    "section": "",
    "text": "Many of us in the U.S. have been surprised by the indifference many in less developed countries have shown about the Ukraine war. There has been much talk lately about how the “Global South” is feeling slighted by the rich countries in the northern hemisphere. The Afrisenti data set (Muhammad et al. 2023) is a collection of over 110,000 tweets from Africa. Its purpose is to train language sentiment models but maybe it can give us some insight into how Africans view different regions of the “Global North.”\nFor the final bite of the apple in this series, we’ll look at the translated tweets and tally the sentiment expressed in each tweet mentioning the U.S., Russia, China and/or Europe. What is the balance of sentiment expressed in this data set about each of these regions?\nThere are some major caveats with this social science experiment. First, while we are using the sentiments coded by native language-speaking curators, there is no stipulation that the sentiment toward a region and the sentiment of the tweet are the same. “I hate enemies of Russia” is a negative tweet, although positive toward Russia. Second, the sample size, while large overall, contains merely hundreds of tweets mentioning our regions of interest. We can’t claim this is representative. Finally, we don’t know the dates of the tweets. We know by some of the news events they reference they probably span the period from 2012 to sometime in 2022. The start of the Ukraine war at the beginning of 2022 and other World events may have shifted attitudes but we can’t split up the tweets by time.\nUnlike our previous posts in this series, this one will be mercifully short."
  },
  {
    "objectID": "posts/2023-04-30-sentiment-analysis-using-google-translate-pt-4--a-painful-sight/index.html#setup",
    "href": "posts/2023-04-30-sentiment-analysis-using-google-translate-pt-4--a-painful-sight/index.html#setup",
    "title": "Sentiment Analysis Using Google Translate (Pt. 4 - A Painful Sight)",
    "section": "Setup",
    "text": "Setup\nWe start by loading the packages and the file with the already translated tweets.\n\nsuppressPackageStartupMessages({\n  library(tidyverse)\n  library(tidytext)\n})\n\n# set up some chart defaults\ntan1 &lt;- \"#FDECCD\"\nyellow1 &lt;- \"#FFBF00\"\ngreen1 &lt;- \"#007000\"\n\ntheme_afri &lt;- function(...){\n  # making a function allows passing further theme elements\n  ggplot2::theme(\n    plot.background = element_rect(fill = tan1, color = NA),\n    panel.background = element_blank(),\n    legend.background = element_blank(),\n    legend.box.background = element_blank(),\n    legend.key = element_blank(),\n    ...\n    ) \n}\n\n# the the previously translated tweets.\nafrisenti_translated &lt;-\n  readr::read_csv(\n    'https://raw.githubusercontent.com/apsteinmetz/tidytuesday/master/2023-02-28_african_language/data/afrisenti_translated.csv',\n    show_col_types = FALSE\n  ) |&gt;\n  mutate(lang = as.factor(assigned_long)) |&gt;\n  mutate(sentiment = as.factor(label)) |&gt;\n  mutate(intended_use = as.factor(intended_use)) |&gt;\n  select(lang, tweet_num, sentiment, translatedText)"
  },
  {
    "objectID": "posts/2023-04-30-sentiment-analysis-using-google-translate-pt-4--a-painful-sight/index.html#data-wrangling",
    "href": "posts/2023-04-30-sentiment-analysis-using-google-translate-pt-4--a-painful-sight/index.html#data-wrangling",
    "title": "Sentiment Analysis Using Google Translate (Pt. 4 - A Painful Sight)",
    "section": "Data Wrangling",
    "text": "Data Wrangling\nWe’ll do some light data wranglng to bring some consistency to how areas are referred to. China and Russia are easy to spot. What does “us” mean? We capture a few variants but there will be some false positives and negatives, I suspect.\n\nafrisenti_translated &lt;- afrisenti_translated |&gt;\n  # can we separate \"US\" from \"us\"?\n  # What about when the tweet is in ALL CAPS BECAUSE I LIKE TO SHOUT?\n  mutate(text = translatedText) |&gt;\n  mutate(text = str_replace(text, \"[a-z]+ US \", \" united_states \")) |&gt;\n  mutate(text = str_replace(text, \"THE US\", \"THE united_states\")) |&gt;\n  mutate(text = str_replace(text, \"U\\\\.S\\\\.A\\\\.\", \" united_states \")) |&gt;\n  mutate(text = str_replace(text, \"U\\\\.S\\\\.\", \" united_states \")) |&gt;\n  mutate(text = str_replace(text, \" USA \", \" united_states \")) |&gt;\n  mutate(text = str_replace(text, \"^USA \", \" united_states \")) |&gt;\n  mutate(text = str_replace(text, \"^US \", \" united_states \")) |&gt;\n  mutate(text = tolower(text)) |&gt;\n  # it's all lower case from here\n  mutate(text = str_replace(text, \"united states\", \"united_states \")) |&gt;\n  # don't label \"south america\" as \"south usa\"\n  mutate(text = str_replace(text, \"south america\", \"south_america\")) |&gt;\n  mutate(text = str_replace(text, \" america\", \" united_states \"))  |&gt; \n  mutate(text = str_replace(text, \"^america\", \"united_states \"))  |&gt; \n  # \"American\" \"European\" and \"Russian\" are stemmed to area name, not so Chinese\n  mutate(text = str_replace(text, \"chinese\", \"china\"))  |&gt; \n  mutate(text = str_replace(text, \" eu \", \" europe \"))\n\nLet’s look at a few random tweets mentioning our areas of interest. We can compare text to the unaltered translatedText to see how well our wrangling worked.\n\nafrisenti_translated |&gt; \n  select(sentiment,text,translatedText) |&gt; \n  filter(str_detect(text,\"china|russia|united_states|europe\")) |&gt; \n  slice_sample(n=10)\n\n# A tibble: 10 × 3\n   sentiment text                                                 translatedText\n   &lt;fct&gt;     &lt;chr&gt;                                                &lt;chr&gt;         \n 1 negative  in addition to this, it is necessary to stop the re… In addition t…\n 2 negative  don't touch united_states , jokes and conspiracy wi… Don't touch A…\n 3 negative  intimidating the people is a policy practiced by ri… Intimidating …\n 4 neutral   russian foreign minister: kane is making positive p… Russian Forei…\n 5 neutral   a disappointing blow to the far-right candidate, co… A disappointi…\n 6 neutral   dr. bashiru will discuss four issues with  united_s… Dr. Bashiru w…\n 7 negative  the grandchildren of the generation that followed e… The grandchil…\n 8 neutral   a senior delegation led by ethiopia's minister of n… A senior dele…\n 9 positive  wow your shoes are nice responses united_states ns … wow your shoe…\n10 positive  in the marathon held in china, ethiopian athletes w… In the marath…"
  },
  {
    "objectID": "posts/2023-04-30-sentiment-analysis-using-google-translate-pt-4--a-painful-sight/index.html#split-the-tweets-into-words-and-tally-results",
    "href": "posts/2023-04-30-sentiment-analysis-using-google-translate-pt-4--a-painful-sight/index.html#split-the-tweets-into-words-and-tally-results",
    "title": "Sentiment Analysis Using Google Translate (Pt. 4 - A Painful Sight)",
    "section": "Split the Tweets into Words and Tally Results",
    "text": "Split the Tweets into Words and Tally Results\nAs sort of a control, let’s include “Africa” as one of our areas.\n\nareas &lt;- c(\"united_states\",\"europe\",\"russia\",\"china\",\"africa\")\ntokens &lt;- afrisenti_translated %&gt;%  \n  unnest_tokens(Area, text ) %&gt;% \n  count(Area,sentiment, sort = TRUE) |&gt;\n  filter(Area %in% areas) |&gt; \n  # cosmetic plot improvements\n  mutate(Area = str_to_title(Area)) |&gt; \n  mutate(Area = str_replace(Area,\"United_states\",\"USA\"))"
  },
  {
    "objectID": "posts/2023-04-30-sentiment-analysis-using-google-translate-pt-4--a-painful-sight/index.html#results-the-higher-up-the-tree-the-monkey-climbs-the-more-its-ass-shows",
    "href": "posts/2023-04-30-sentiment-analysis-using-google-translate-pt-4--a-painful-sight/index.html#results-the-higher-up-the-tree-the-monkey-climbs-the-more-its-ass-shows",
    "title": "Sentiment Analysis Using Google Translate (Pt. 4 - A Painful Sight)",
    "section": "Results: The Higher Up the Tree the Monkey Climbs, the More Its Ass Shows",
    "text": "Results: The Higher Up the Tree the Monkey Climbs, the More Its Ass Shows\nSadly, among tweets mentioning the Global North, there are more tagged with negative than positive sentiment. In contrast, the balance of sentiment is positive in tweets mentioning “Africa” (as opposed to particular African countries). It’s perhaps not surprising the the United States is the most mentioned area. It aslo has the worst balance of sentiment. Alone among the areas, Russia has more neutral sentiment tweets than any other sentiment.\n\ncol_set &lt;-   c(\"#952038\",\"#F7C608\",\"#319400\")\n  \n# feelings about world powers\ntokens |&gt;\n  ggplot(aes(Area,n,fill = sentiment)) + \n  geom_col(position = \"dodge\") + \n  scale_fill_manual(values =  col_set) + \n  labs(title = 'Q:How Does Africa Feel About the Global North?\\nA: Not great, but Tweets mentioning Russia are mostly \"neutral.\"',\n       subtitle = \"WARNING: Sample size is not large. Other caveats apply.\",\n       y = \"Tweet Count\",\n       caption = \"Source: Afrisenti data set\") +\n  coord_flip() +\n  annotate(\"text\",x = 2.5,y = 100,label = \"Alternate names for\\nareas are combined.\\ne.g. EU = Europe\") +\n  theme_afri(plot.subtitle = element_text(face = \"italic\"))\n\n\n\n\nWe can calculate a net sentiment ratio as the positive minus negative divided by the sum of all tweets. This shows in stark terms how the balance of sentiment is worse for the United States than the other regions we are looking at. Further, both while both Russia and China are viewed negatively, on balance, they are viewed less poorly than both the USA and Europe.\n\ntokens |&gt; \n  pivot_wider(names_from = \"sentiment\",values_from = \"n\") |&gt; \n  mutate(net_sentiment_ratio = (positive - negative)/(positive+negative+neutral)) |&gt; \n  ggplot(aes(Area,net_sentiment_ratio)) + geom_col(fill = yellow1) + \n  geom_hline(yintercept = 0) + \n  theme_afri() + \n  labs(y = \"Net Sentiment Ratio\",\n       title = \"Not A Good Look For the USA\",\n       subtitle = \"Net Sentiment of Tweets from Africa Mentioning a Northern Area or Africa\",\n       caption = \"Source: Afrisenti Data Set\")"
  },
  {
    "objectID": "posts/2023-04-30-sentiment-analysis-using-google-translate-pt-4--a-painful-sight/index.html#conclusion",
    "href": "posts/2023-04-30-sentiment-analysis-using-google-translate-pt-4--a-painful-sight/index.html#conclusion",
    "title": "Sentiment Analysis Using Google Translate (Pt. 4 - A Painful Sight)",
    "section": "Conclusion",
    "text": "Conclusion\nWhile I think these results are truly interesting, please don’t read too much into them. This is not a carefully designed social science research project. The results do align with our intuition gleaned from recent news about the “Global South” vs. the “Global North.” Given that this data set spans roughly ten years, this tension is not a new phenomenon.\nThe Afrisenti data set is a fascinating corpus and there are many more explorations we might do in it. As I write, it looks like Twitter is going to put its academic API behind a paywall. This is quite sad and will stifle the kind of important explorations serious researchers are able to do, not to mention curious amateurs like us."
  },
  {
    "objectID": "posts/2023-05-03-animating-a-heatmap-with-rayshader/index.html",
    "href": "posts/2023-05-03-animating-a-heatmap-with-rayshader/index.html",
    "title": "Animating a Heatmap with Rayshader",
    "section": "",
    "text": "suppressPackageStartupMessages({\nlibrary(tidyverse)\nlibrary(rayshader)\nlibrary(gt)\nlibrary(gifski)\n})\n\nThe best part of any analysis are the visualizations. I couldn’t resist taking the results of our earlier sentiment analysis and turning it into some appealing eye candy.\nYou’ll recall we tried several different approaches to predicting the sentiment of tweets in various African languages. The main test of fit was a “confusion matrix,” a cross tab of the “true” sentiment vs. what we predicted. The results for each were far from perfect.\nFor this post we’ll just cut and paste the confusion matrices from each method of sentiment analysis and turn them into a single data frame. While we’re at it, we’ll add a method called “Perfection” which would be the result of a perfect match.\n\n# word valence \nwv &lt;- tribble( \n~ method, ~truth, ~negative, ~neutral, ~positive,\n\"Word Valence\" ,\"negative\", 15083, 14619, 6862,\n\"Word Valence\" ,\"neutral\" , 7761, 23921, 7469,\n\"Word Valence\" ,\"positive\" , 4720, 13148, 18137\n)\n\nsv &lt;- tribble(\n~ method, ~truth, ~negative, ~neutral, ~positive,\n\"Sentence Valence\" ,\"negative\", 17776, 11510, 7278,\n\"Sentence Valence\" ,\"neutral\", 9624, 18651, 10876,\n\"Sentence Valence\" ,\"positive\", 5558, 9859, 20588\n)\n\nmln &lt;- tribble(\n~ method, ~truth, ~negative, ~neutral, ~positive,\n\"ML Native\", \"negative\", 5524, 4598, 1261,\n\"ML Native\", \"neutral\", 2489, 7011, 1182,\n\"ML Native\", \"positive\", 2100, 3287, 4981\n)\nmle &lt;- tribble(\n~ method, ~truth, ~negative, ~neutral, ~positive,\n\"ML Translated\",\"negative\", 5678, 4146, 1432,\n\"ML Translated\",\"neutral\", 2233, 6607, 1483,\n\"ML Translated\",\"positive\", 1905, 3245, 5178\n)\n\nperfect &lt;- tribble(\n~ method, ~truth, ~negative, ~neutral, ~positive,\n\"Perfection\",\"negative\", 100, 0, 0,\n\"Perfection\",\"neutral\", 0, 100, 0,\n\"Perfection\",\"positive\", 0, 0, 100\n)\n\nTo normalize the data we’ll compute the proportion of outcomes in each cell, rather than the count. The perfect match would see 33% of the outcomes in each correct cell and zero in all the others.\n\nxt &lt;- bind_rows(wv,sv,mln,mle,perfect) |&gt; \n  pivot_longer(cols = 3:5,names_to = \"predicted\",values_to = \"count\") |&gt; \n  group_by(method) |&gt; \n  mutate(proportion = count/sum(count))\nxt\n\n# A tibble: 45 × 5\n# Groups:   method [5]\n   method           truth    predicted count proportion\n   &lt;chr&gt;            &lt;chr&gt;    &lt;chr&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n 1 Word Valence     negative negative  15083     0.135 \n 2 Word Valence     negative neutral   14619     0.131 \n 3 Word Valence     negative positive   6862     0.0614\n 4 Word Valence     neutral  negative   7761     0.0695\n 5 Word Valence     neutral  neutral   23921     0.214 \n 6 Word Valence     neutral  positive   7469     0.0669\n 7 Word Valence     positive negative   4720     0.0422\n 8 Word Valence     positive neutral   13148     0.118 \n 9 Word Valence     positive positive  18137     0.162 \n10 Sentence Valence negative negative  17776     0.159 \n# ℹ 35 more rows"
  },
  {
    "objectID": "posts/2023-05-03-animating-a-heatmap-with-rayshader/index.html#wrangle-the-data",
    "href": "posts/2023-05-03-animating-a-heatmap-with-rayshader/index.html#wrangle-the-data",
    "title": "Animating a Heatmap with Rayshader",
    "section": "",
    "text": "suppressPackageStartupMessages({\nlibrary(tidyverse)\nlibrary(rayshader)\nlibrary(gt)\nlibrary(gifski)\n})\n\nThe best part of any analysis are the visualizations. I couldn’t resist taking the results of our earlier sentiment analysis and turning it into some appealing eye candy.\nYou’ll recall we tried several different approaches to predicting the sentiment of tweets in various African languages. The main test of fit was a “confusion matrix,” a cross tab of the “true” sentiment vs. what we predicted. The results for each were far from perfect.\nFor this post we’ll just cut and paste the confusion matrices from each method of sentiment analysis and turn them into a single data frame. While we’re at it, we’ll add a method called “Perfection” which would be the result of a perfect match.\n\n# word valence \nwv &lt;- tribble( \n~ method, ~truth, ~negative, ~neutral, ~positive,\n\"Word Valence\" ,\"negative\", 15083, 14619, 6862,\n\"Word Valence\" ,\"neutral\" , 7761, 23921, 7469,\n\"Word Valence\" ,\"positive\" , 4720, 13148, 18137\n)\n\nsv &lt;- tribble(\n~ method, ~truth, ~negative, ~neutral, ~positive,\n\"Sentence Valence\" ,\"negative\", 17776, 11510, 7278,\n\"Sentence Valence\" ,\"neutral\", 9624, 18651, 10876,\n\"Sentence Valence\" ,\"positive\", 5558, 9859, 20588\n)\n\nmln &lt;- tribble(\n~ method, ~truth, ~negative, ~neutral, ~positive,\n\"ML Native\", \"negative\", 5524, 4598, 1261,\n\"ML Native\", \"neutral\", 2489, 7011, 1182,\n\"ML Native\", \"positive\", 2100, 3287, 4981\n)\nmle &lt;- tribble(\n~ method, ~truth, ~negative, ~neutral, ~positive,\n\"ML Translated\",\"negative\", 5678, 4146, 1432,\n\"ML Translated\",\"neutral\", 2233, 6607, 1483,\n\"ML Translated\",\"positive\", 1905, 3245, 5178\n)\n\nperfect &lt;- tribble(\n~ method, ~truth, ~negative, ~neutral, ~positive,\n\"Perfection\",\"negative\", 100, 0, 0,\n\"Perfection\",\"neutral\", 0, 100, 0,\n\"Perfection\",\"positive\", 0, 0, 100\n)\n\nTo normalize the data we’ll compute the proportion of outcomes in each cell, rather than the count. The perfect match would see 33% of the outcomes in each correct cell and zero in all the others.\n\nxt &lt;- bind_rows(wv,sv,mln,mle,perfect) |&gt; \n  pivot_longer(cols = 3:5,names_to = \"predicted\",values_to = \"count\") |&gt; \n  group_by(method) |&gt; \n  mutate(proportion = count/sum(count))\nxt\n\n# A tibble: 45 × 5\n# Groups:   method [5]\n   method           truth    predicted count proportion\n   &lt;chr&gt;            &lt;chr&gt;    &lt;chr&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n 1 Word Valence     negative negative  15083     0.135 \n 2 Word Valence     negative neutral   14619     0.131 \n 3 Word Valence     negative positive   6862     0.0614\n 4 Word Valence     neutral  negative   7761     0.0695\n 5 Word Valence     neutral  neutral   23921     0.214 \n 6 Word Valence     neutral  positive   7469     0.0669\n 7 Word Valence     positive negative   4720     0.0422\n 8 Word Valence     positive neutral   13148     0.118 \n 9 Word Valence     positive positive  18137     0.162 \n10 Sentence Valence negative negative  17776     0.159 \n# ℹ 35 more rows"
  },
  {
    "objectID": "posts/2023-05-03-animating-a-heatmap-with-rayshader/index.html#get-confused",
    "href": "posts/2023-05-03-animating-a-heatmap-with-rayshader/index.html#get-confused",
    "title": "Animating a Heatmap with Rayshader",
    "section": "Get Confused",
    "text": "Get Confused\nNow consider some approaches to visualization. First, we might look at the conventional crosstab or, in this particular case, also called a confusion matrix which tabulates the number of correct and incorrect classifications. It’s a super simple viz but let’s use the gt package so we can label the table nicely. This shows just one of our methods.\n\nmle |&gt;\n  select(-method) |&gt;\n  gt() |&gt;\n    tab_header(title = mle$method[1]) |&gt;\n    tab_spanner(label = \"predicted\", columns = where(is.numeric)) |&gt;\n    fmt_number(columns = where(is.numeric),decimals = 0)\n\n\n\n\n\n  \n    \n      ML Translated\n    \n    \n    \n      truth\n      \n        predicted\n      \n    \n    \n      negative\n      neutral\n      positive\n    \n  \n  \n    negative\n5,678\n4,146\n1,432\n    neutral\n2,233\n6,607\n1,483\n    positive\n1,905\n3,245\n5,178"
  },
  {
    "objectID": "posts/2023-05-03-animating-a-heatmap-with-rayshader/index.html#no-3d-glasses-needed",
    "href": "posts/2023-05-03-animating-a-heatmap-with-rayshader/index.html#no-3d-glasses-needed",
    "title": "Animating a Heatmap with Rayshader",
    "section": "No 3D Glasses Needed",
    "text": "No 3D Glasses Needed\nGood data visualizations clearly tell the story and are ideally more compelling than a grid of numbers. The plot above is good for seeing the difference between our methods and “Perfection” but it’s a little harder to divine differences among the methods. If we create a 3D heatmap it will bring the addition of depth to the heat map.\n\nrender_confusion &lt;-\n  function(theta = 45, sunangle = 315) {\n    print(sunangle)\n    shadow_matrix &lt;-\n      plot_gg(\n        gg,\n        width = 7,\n        height = 7,\n        multicore = TRUE,\n        sunangle = sunangle,\n        scale = 250,\n        zoom = 0.7,\n        phi = 50,\n        theta = theta,\n        windowsize = c(800, 800)\n      )\n    return(shadow_matrix)\n  }\n\norig_shadow = render_confusion(20)\nrender_snapshot(file = \"img/3dmap.png\")"
  },
  {
    "objectID": "posts/2023-05-03-animating-a-heatmap-with-rayshader/index.html#move-it",
    "href": "posts/2023-05-03-animating-a-heatmap-with-rayshader/index.html#move-it",
    "title": "Animating a Heatmap with Rayshader",
    "section": "Move It!",
    "text": "Move It!\nBy creating the illusion of height and depth we can more clearly see the diagonal where our predictions match the true sentiment and how many false predictions there are. There is still one problem. The height of the bars has the potential to obscure some other bars and some labels. The way to get around this is to animate the chart to change the point of view. This will complete the 3D illusion.\nIf you are running this code interactively, a separate graphics window will open and you can manipulate the view with your mouse. For this web document, we’ll save individual frames and splice them together into an animation. Each frame is rendered from a different angle (“theta”). We use rendercamera() to move the rendered image for us and take snapshots as we move. Alternatively, we could change the sunangle for every frame so the shadows would move as we rotate the plot, but that would drastically increase rendering time.\n\ndestdir = tempdir()\nn = 0\n# rotate\nfor (i in 0:-20){\n  fname &lt;- paste0(\"img_\", formatC(n, width = 3, flag = \"0\"), \".png\")\n  print(fname)\n  render_camera(theta = i, phi = 90)\n  rayshader::render_snapshot(paste0(destdir, \"\\\\\", filename = fname))\n  n &lt;- n + 1\n}\n\n# tilt\nfor (j in 90:50){\n  fname &lt;- paste0(\"img_\", formatC(n, width = 3, flag = \"0\"), \".png\")\n  print(fname)\n  render_camera(theta = i, phi = j)\n  rayshader::render_snapshot(paste0(destdir, \"\\\\\", filename = fname))\n  n &lt;- n + 1\n}\n\n# rotate\nfor (i in -20:20) {\n  fname &lt;- paste0(\"img_\", formatC(n, width = 3, flag = \"0\"), \".png\")\n  print(fname)\n  render_camera(theta = i, phi = j)\n  rayshader::render_snapshot(paste0(destdir, \"\\\\\", filename = fname))\n  n &lt;- n + 1\n}\n\nfnames &lt;- dir(destdir)\nfnames &lt;- fnames[str_detect(fnames,\"img_\")]\nfnames &lt;- paste0(destdir,\"\\\\\",c(sort(fnames),sort(fnames,decreasing = TRUE))) \ngifski(fnames,\"img/confusion.gif\",delay = 1/10,loop = TRUE)\n\n\nIt’s clear from this view that none of our approaches to predict the true sentiment are very good. The best approach, by a little bit, is the machine learning model using the translated tweets.\nJust because we can do something cool doesn’t mean that we are adding to our understanding of a question. I am the first to admit that some data visualizations can get gratuitous. In this post I have sequentially evolved the presentation of the results of our analysis from a table to a 3D animation. I hope I have made the case that for our purpose this animation enhances our understanding."
  },
  {
    "objectID": "posts/2023-06-09-kakhovka-dam/index.html",
    "href": "posts/2023-06-09-kakhovka-dam/index.html",
    "title": "Kakhovka Dam Disaster",
    "section": "",
    "text": "The war in Ukraine has spawned yet another disaster, the destruction of the dam across the Dnipro river, upstream from Kherson City. This is an ecologial and humanitarian disaster as vast acres of settlements, farmlands and wetlands have been destroyed.\nThis marks the third time a dam in this region has been destroyed. First in 1941, the Soviets blew up a dam to impede the advancing Germans in southern city of Zaporizhzhia, upstream of the present dam. Tens of thousands of people were killed among Ukrainians and Red Army soldiers (the Germans were not yet in the area). The Germans repaired it. Then, in 1943, the Germans blew it up again to thwart the advancing Soviets.\n\n\n\nSource: Bundesarchiv\n\n\nIronically, just as the destruction of the Kakhovka dam is an ecological tragedy, so too, was its construction. Many settlements and wetlands were submerged as the Dnipro river backed up to serve Stalin’s drive to collectivize and modernize agriculture."
  },
  {
    "objectID": "posts/2023-06-09-kakhovka-dam/index.html#some-history",
    "href": "posts/2023-06-09-kakhovka-dam/index.html#some-history",
    "title": "Kakhovka Dam Disaster",
    "section": "",
    "text": "The war in Ukraine has spawned yet another disaster, the destruction of the dam across the Dnipro river, upstream from Kherson City. This is an ecologial and humanitarian disaster as vast acres of settlements, farmlands and wetlands have been destroyed.\nThis marks the third time a dam in this region has been destroyed. First in 1941, the Soviets blew up a dam to impede the advancing Germans in southern city of Zaporizhzhia, upstream of the present dam. Tens of thousands of people were killed among Ukrainians and Red Army soldiers (the Germans were not yet in the area). The Germans repaired it. Then, in 1943, the Germans blew it up again to thwart the advancing Soviets.\n\n\n\nSource: Bundesarchiv\n\n\nIronically, just as the destruction of the Kakhovka dam is an ecological tragedy, so too, was its construction. Many settlements and wetlands were submerged as the Dnipro river backed up to serve Stalin’s drive to collectivize and modernize agriculture."
  },
  {
    "objectID": "posts/2023-06-09-kakhovka-dam/index.html#modeling-the-disaster",
    "href": "posts/2023-06-09-kakhovka-dam/index.html#modeling-the-disaster",
    "title": "Kakhovka Dam Disaster",
    "section": "Modeling the Disaster",
    "text": "Modeling the Disaster\nLast week, as water poured through the gap where the Kakhovka dam used to be, water levels surged downstream. This was a rolling disaster, with the river delta at the Black Sea seeing the flow last. We can make a rough attempt to visualize the effect of the rising water with an animated inundation map. This is a pretty sophisticated undertaking. Fortunately, there are tools in the form of R packages that have been developed in the last several years that make it easy for anyone to make inundation maps. There are many pieces of the project below where I can not believe how few lines of code are needed.\nLet’s see how easy it is.\n\nGet the Lower Dnipro River.\nAs usual, we load the required packages. Then I just looked at Google Maps to find the coordinates of a region spanning from the Kakhovka dam in the east to the Dnipro delta in the west. Since they were displayed in degrees, minutes and seconds, I wrote a quickie function to convert them to decimal. Note that longitude west of zero and latitude south of zero are expressed as negative, neither of which is true here. Longitudes in the United States, for instance, have a negative sign.\n\nlibrary(tidyverse)\n# GIS packages\nlibrary(elevatr)\nlibrary(rayshader)\nlibrary(sf)\nlibrary(raster)\nlibrary(leaflet)\nlibrary(osmdata)\nlibrary(basemaps)\n# Make GIFs\nlibrary(magick)\n\ndms_to_dec &lt;- function(deg=0, min=0, sec=0) {\n  return(deg + min / 60 + sec / 3600)\n}\nkh_loc &lt;- data.frame(lat = c(dms_to_dec(46,51,00),dms_to_dec(46,18,00)),\n                             lon= c(dms_to_dec(32,09,00),dms_to_dec(33,25,00)) )\n\nHere is the first “wow, that was easy” moment. We can confirm our coordinates by quickly pulling in a map with multiple layers, one for geographic names, and one for the satellite image. This is done with the leaflet package.\n\nleaflet() |&gt; \n  fitBounds(kh_loc$lon[1],kh_loc$lat[1],kh_loc$lon[2],kh_loc$lat[2]) |&gt; \n  addProviderTiles(providers$Esri.WorldImagery) |&gt; \n  addProviderTiles(providers$CartoDB.PositronOnlyLabels,\n                   options = providerTileOptions(opacity = 1))\n\n\n\n\n\nThis is an interactive map. We can pan and zoom to our heart’s content. There are some interesting observations we can make. The dam is at the northeast corner of the image. We can see the swampy lowlands below the dam. They lie mainly to the south of the primary course of the river. Notice the crop circles to the east and south of the reservoir. There are many visible if we pan a bit to the east. This tells us that the crops grown there are irrigated by water pumped to center-pivot sprinkler systems. Presumably the water comes from the reservoir which is now disappearing.\n\nsource: iStock\nJust to the east of the dam we also see the Crimean canal stretching south, which is a major source of fresh water for the Crimean peninsula. This, too, will cease to function.\n\n\nGet the Elevations\nWe are interested in modeling which parts of the river basin got submerged after the destruction of the dam. Obviously we need to know the elevation of the land. Using the elevatr package it only takes one line of code to retrieve a topographic map from the OpenTopography data set. This gets us a raster object that contains a matrix with elevations at each point. It also contains metadata with the coordinate system.\n\nkherson_dnipro &lt;- kh_loc |&gt; \n  st_as_sf(coords = c(\"lon\",\"lat\"),crs = 4326)\n\n# get the topo matrix\nkherson_elev &lt;- get_elev_raster(kherson_dnipro, \n                                src = \"gl1\", \n                                clip = \"bbox\",\n                                verbose = FALSE,\n                                serial=TRUE) \n\n\nDownloading: 8.2 kB     \nDownloading: 8.2 kB     \nDownloading: 41 kB     \nDownloading: 41 kB     \nDownloading: 74 kB     \nDownloading: 74 kB     \nDownloading: 98 kB     \nDownloading: 98 kB     \nDownloading: 140 kB     \nDownloading: 140 kB     \nDownloading: 170 kB     \nDownloading: 170 kB     \nDownloading: 170 kB     \nDownloading: 170 kB     \nDownloading: 210 kB     \nDownloading: 210 kB     \nDownloading: 260 kB     \nDownloading: 260 kB     \nDownloading: 310 kB     \nDownloading: 310 kB     \nDownloading: 360 kB     \nDownloading: 360 kB     \nDownloading: 380 kB     \nDownloading: 380 kB     \nDownloading: 400 kB     \nDownloading: 400 kB     \nDownloading: 400 kB     \nDownloading: 400 kB     \nDownloading: 440 kB     \nDownloading: 440 kB     \nDownloading: 470 kB     \nDownloading: 470 kB     \nDownloading: 500 kB     \nDownloading: 500 kB     \nDownloading: 520 kB     \nDownloading: 520 kB     \nDownloading: 520 kB     \nDownloading: 520 kB     \nDownloading: 560 kB     \nDownloading: 560 kB     \nDownloading: 620 kB     \nDownloading: 620 kB     \nDownloading: 640 kB     \nDownloading: 640 kB     \nDownloading: 700 kB     \nDownloading: 700 kB     \nDownloading: 750 kB     \nDownloading: 750 kB     \nDownloading: 760 kB     \nDownloading: 760 kB     \nDownloading: 810 kB     \nDownloading: 810 kB     \nDownloading: 810 kB     \nDownloading: 810 kB     \nDownloading: 900 kB     \nDownloading: 900 kB     \nDownloading: 910 kB     \nDownloading: 910 kB     \nDownloading: 950 kB     \nDownloading: 950 kB     \nDownloading: 980 kB     \nDownloading: 980 kB     \nDownloading: 990 kB     \nDownloading: 990 kB     \nDownloading: 1.1 MB     \nDownloading: 1.1 MB     \nDownloading: 1.1 MB     \nDownloading: 1.1 MB     \nDownloading: 1.1 MB     \nDownloading: 1.1 MB     \nDownloading: 1.1 MB     \nDownloading: 1.1 MB     \nDownloading: 1.1 MB     \nDownloading: 1.1 MB     \nDownloading: 1.2 MB     \nDownloading: 1.2 MB     \nDownloading: 1.2 MB     \nDownloading: 1.2 MB     \nDownloading: 1.3 MB     \nDownloading: 1.3 MB     \nDownloading: 1.4 MB     \nDownloading: 1.4 MB     \nDownloading: 1.4 MB     \nDownloading: 1.4 MB     \nDownloading: 1.5 MB     \nDownloading: 1.5 MB     \nDownloading: 1.6 MB     \nDownloading: 1.6 MB     \nDownloading: 1.6 MB     \nDownloading: 1.6 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 2 MB     \nDownloading: 2 MB     \nDownloading: 2 MB     \nDownloading: 2 MB     \nDownloading: 2 MB     \nDownloading: 2 MB     \nDownloading: 2.1 MB     \nDownloading: 2.1 MB     \nDownloading: 2.1 MB     \nDownloading: 2.1 MB     \nDownloading: 2.1 MB     \nDownloading: 2.1 MB     \nDownloading: 2.2 MB     \nDownloading: 2.2 MB     \nDownloading: 2.2 MB     \nDownloading: 2.2 MB     \nDownloading: 2.4 MB     \nDownloading: 2.4 MB     \nDownloading: 2.4 MB     \nDownloading: 2.4 MB     \nDownloading: 2.4 MB     \nDownloading: 2.4 MB     \nDownloading: 2.6 MB     \nDownloading: 2.6 MB     \nDownloading: 2.7 MB     \nDownloading: 2.7 MB     \nDownloading: 2.7 MB     \nDownloading: 2.7 MB     \nDownloading: 2.7 MB     \nDownloading: 2.7 MB     \nDownloading: 2.8 MB     \nDownloading: 2.8 MB     \nDownloading: 2.9 MB     \nDownloading: 2.9 MB     \nDownloading: 2.9 MB     \nDownloading: 2.9 MB     \nDownloading: 2.9 MB     \nDownloading: 2.9 MB     \nDownloading: 2.9 MB     \nDownloading: 2.9 MB     \nDownloading: 3 MB     \nDownloading: 3 MB     \nDownloading: 3 MB     \nDownloading: 3 MB     \nDownloading: 3.1 MB     \nDownloading: 3.1 MB     \nDownloading: 3.1 MB     \nDownloading: 3.1 MB     \nDownloading: 3.2 MB     \nDownloading: 3.2 MB     \nDownloading: 3.2 MB     \nDownloading: 3.2 MB     \nDownloading: 3.2 MB     \nDownloading: 3.2 MB     \nDownloading: 3.3 MB     \nDownloading: 3.3 MB     \nDownloading: 3.3 MB     \nDownloading: 3.3 MB     \nDownloading: 3.3 MB     \nDownloading: 3.3 MB     \n\nkherson_elev\n\nclass      : RasterLayer \ndimensions : 1980, 4560, 9028800  (nrow, ncol, ncell)\nresolution : 0.0002777778, 0.0002777778  (x, y)\nextent     : 32.14986, 33.41653, 46.30014, 46.85014  (xmin, xmax, ymin, ymax)\ncrs        : +proj=longlat +datum=WGS84 +no_defs \nsource     : memory\nnames      : file571c64e2c39 \nvalues     : -51, 107  (min, max)\n\n\nA quick and dirty plot of the matrix shows the flood situation. Bright green is high ground. The blue isn’t necessarily water, just water and land very nearly at sea level. The color scale is in meters above sea level. South of the river are mostly very low-lying areas. Notably, these areas are Russian controlled today. The north side is in Ukrainian hands.\n\npal &lt;- colorRampPalette(c(\"darkblue\",\"limegreen\"))\nplot(kherson_elev, col = pal(10))\n\n\n\n\nNow let’s create a more realistic view of the ground and show the rising water. To do this we’ll use the rayshader package. When Tyler Morgan-Wall released this package a few years ago, he opened up huge visualization vistas to the R community. We will just be touching the surface of its capabilities here. We’ll use two key features, the ability to shade the surface for a 3D effect and to detect and color bodies of water. Let’s exaggerate the z-axis a bit since this is generally very flat ground and we want to see subtle terrain changes. Like the ggplot2 package, we can start with a base plot and add layers.\n\nkh_elmat &lt;- raster_to_matrix(kherson_elev)\nbase_map &lt;- kh_elmat |&gt; \n  sphere_shade(texture = \"imhof1\",zscale = .8)\n\nbase_map |&gt; \n  add_water(detect_water(kh_elmat),color=\"desert\") |&gt; \n  plot_map()\n\n\n\n\nWe can clearly see the reservoir in the east, the delta in the west, the (exaggerated) canyons carved by the Dnipro tributaries and the narrow river connecting them.\n\n\nThe Human Dimension\nWhat we don’t see are where the people are and this tragedy is very much a human one. We can get a sense of human presence by creating a road overlay using OpenStreetMap data. Again, the code needed to do this is trivially simple. Note the returned object has several layers but we are only interested in the osm_points.\n\nkherson_roads &lt;- osmdata::opq(st_bbox(kherson_dnipro)) %&gt;% \n   osmdata::add_osm_feature(\"highway\") %&gt;% \n   osmdata::osmdata_sf() \n \n# Take just the road layer and transform coordinates to our existing projection\nkherson_lines &lt;- sf::st_transform(kherson_roads$osm_lines,\n                                  crs = raster::crs(kherson_dnipro))\n\n# a rayshader object we pre-generate now for speed later\nroads &lt;-  generate_line_overlay(\n  kherson_lines,\n  heightmap = kh_elmat,\n  extent = extent(extent(kherson_dnipro)),\n  linewidth = 2\n)\n\nkherson_roads\n\nOnce again we do a quick and dirty plot to visualize the new layer. This is also super easy with the geom_sf geom in ggplot2.\n\n# View streets as a ggplot2 render \nggplot(kherson_lines, aes(color = osm_id)) + \n  geom_sf() +\n  theme(legend.position = \"none\") +\n  labs(title = \"Kherson Roads from Open Street Map\")\n\n\n\n\n\n\nAdd More Realism\nLet’s overlay a satellite image to lend more realism to the image. In particular this lets us see the individual farms. Once again, the task is trivially easy using the basemaps package and the freely available ESRI “World Imagery” maps. This is the same image we saw at the beginning of this project but now saved as a “PNG” file. Rayshader will automatically scale the image to match the other layers in the map.\n\n# get image overlay\nsatview &lt;- basemap_png(\n  ext = kherson_dnipro,\n  map_service = \"esri\",\n  map_type = \"world_imagery\",\n  map_res = NULL,\n  verbose = TRUE,\n  browse = FALSE\n)\n\nLoading basemap 'world_imagery' from map service 'esri'...\n\n\n\n\nVisualize the Rising Water\nWhen we put it all together, what do we get?\n\nbase_map &lt;- base_map |&gt;\n  add_overlay(satview)\n\nbase_map |&gt;\n  add_water(detect_water(kh_elmat), color = \"desert\") |&gt;\n  add_overlay(roads)|&gt;\n  plot_map()\n\n\n\n\nWe can see the city of Kherson on the north side, the settlements on the south side, mostly out of the swampy areas along the river and, on the upper east side, the town of Nova Kakhovka where the dam and hydroelectric station were.\nUsing this as a base, let’s create a function that shows the impact of rising water levels. Rayshader will color totally flat areas as water so we mimic rising water by setting the elevations of any point below the rising water level to zero. We can iterate by setting the water level higher and higher to put more of the land “under water.”\nWe have several layers in this map. The shading, the satellite view, the water level and the streets. Order matters. The shading gets obscured by the satellite image. The rising water covers the satellite image. The streets are drawn last and remain visible so we can see just where human-occupied areas are being flooded. All these are aesthetic choices.\n\n\n\n\n\n\nNote:\n\n\n\nAt this point we should be clear about what this is and isn’t. The method we use here more properly would model rising sea levels. We don’t know exactly how the water affected each area at what time. We don’t know what the peak inundation level was for each area. Further, once the reservoir empties out, the water will recede. Let’s call this a “dramatization.” This does give a dramatic view of the scope of the destruction.\n\n\nThe function below will render a view of our map with a water level rise of zero though eight meters. Again, this doesn’t assume the water rose eight meters, only that it reached eight meters above sea level. That means land at an altitude of seven meters will be one meter underwater for an eight meter water rise. We save each a single image for each meter of water rise and speed things up by changing only the parts of the map which change with the water level.\n\nplot_rising_water &lt;- function(water_level = 0) {\n\n  # adjust elevations to simulate water\n  flood_elmat &lt;- ifelse(kh_elmat &lt; water_level, 0, kh_elmat)\n  \n  base_map |&gt;\n    add_water(detect_water(flood_elmat), color = \"desert\") |&gt;\n    # add roads on top of water\n    add_overlay(roads) |&gt;\n    save_png(\n      filename = paste0(\"img/frames/flood_\", \n                        formatC(water_level, width = 3, flag = \"0\"), \".png\"),\n      title_text = paste0(\n        \"Flood Inundation of the Dnipro\\nAfter Kakhovka Dam Destruction\\nWater Level:\",\n        formatC(water_level, width = 3, flag = \" \"),\n        \" Meters\"\n      ),\n      title_size = 60,\n      title_bar_color = \"white\"\n    )\n  \n  \n}\n# generate frames\n0:8 |&gt; walk(plot_rising_water)\n\nNow that we have saved nine still frames we can animate them into a short GIF file with almost no work at all. There are many ways to save a GIF animation with R. Here we use the magick package because we can create a smooth morph between frames which simulates rising water well.\n\n\n\n\n\n\nNote:\n\n\n\nWe use magick::image_resize() to downscale the individual frames to get a GIF of manageable size. Careful readers might wonder why we didn’t earlier downscale the elevation matrix instead. That would have sped up all the operations above. Unfortunately, that would make the flooded regions look very blocky as we would lose too much terrain data.\n\n\n\n# make GIF\ntarget_dir &lt;- \"img/frames\"\nfnames &lt;- paste0(\"img\\\\frames\\\\\", dir(\"img/frames\"))\nimage_read(fnames[1:9]) |&gt; \n  image_resize(\"1000x\") |&gt; \n  image_morph() |&gt; \n  image_animate() |&gt; \n  image_write(path = \"img/flood.gif\",format = \"gif\")\n\n\nWhile we don’t know what the maximum water height was or if all of the specific areas shown above got inundated but we do know many did and this clearly shows the vulnerable areas and where people have been potentially displaced."
  },
  {
    "objectID": "posts/2023-06-09-kakhovka-dam/index.html#now-you-try",
    "href": "posts/2023-06-09-kakhovka-dam/index.html#now-you-try",
    "title": "Kakhovka Dam Disaster",
    "section": "Now You Try",
    "text": "Now You Try\nHere we modeled the disaster in Ukraine but you might use these techniques to visualize the risk of flooding where you are considering buying a house, for example. You might also check out the USGS real-time stream gauge data to see what happens when the creek rises. All you need are four corners on a map to get started."
  },
  {
    "objectID": "posts/2023-06-09-kakhovka-dam/index.html#my-speculation",
    "href": "posts/2023-06-09-kakhovka-dam/index.html#my-speculation",
    "title": "Kakhovka Dam Disaster",
    "section": "My Speculation",
    "text": "My Speculation\nIt’s irrelevant to our project today but as of this writing, we don’t know for sure who blew up the dam. I certainly don’t know but I will make a couple observations. The dam is under Russian control and it looks like the explosion came from inside the structure. The Ukrainians control the Zaporizhzhia dam and there has been no talk of blowing that up. As we saw in WWII, both sides blew up dams to impede enemy advancement. There is one side that is advancing today and it’s the Ukrainians. The Russians would like to slow them down. Finally, while this potentially has one benefit for the Ukrainians by cutting the fresh water canal to Crimea, the Ukrainians could have destroyed just the canal and prevented the Russians from rebuilding it without blowing the dam. We shall see in the fullness of time."
  }
]