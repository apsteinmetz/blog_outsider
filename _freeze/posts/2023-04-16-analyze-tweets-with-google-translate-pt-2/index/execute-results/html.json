{
  "hash": "aaf75395894ad8918762a9f30e3e5d4a",
  "result": {
    "markdown": "---\ntitle: \"Sentiment Analysis Using Google Translate (Pt. 2)\"\nauthor: Art Steinmetz\ndate: 2023-04-16\ncategories: \n  - tidytext\n  - twitter\ndraft: false\nformat: html\nimage: img/preview.png\neditor: visual\nbibliography: references.bib\nexecute: \n  freeze: true\n  warning: false\n---\n\n\n## Introduction\n\nSentiment analysis is a common task in natural language processing. Many of the tools available for this are calibrated to the English language. Can Google Translate let us apply this toolset to a broader set of languages? We will use the tidytext framework to measure sentiment of tweets originally in African languages but translated to English. We'll measure sentiment at both the word level and sentence level and see how it agrees with the sentiment, positive, negative or neutral, already assigned in the Afrisenti data set.\n\n## In Our Last Episode...\n\nIn [part one of this series](https://outsiderdata.netlify.app/posts/2023-04-15-analyze-tweets-with-google-translate-pt-1/) we translated the afrisenti data set using the Google Cloud Translate API. This data set contains a little over 110,000 tweets in 13 different African languages[@Muhammad2023AfriSentiAT]. We saw that , at first glance, Google Translate does a good job on these languages. So now let's ask:\n\n1.  Are the translations good enough to run sentiment analysis on?\n2.  Will the sentiments we measure agree with the original data? This is a higher standard, of course.\n\n## Measuring Sentiment\n\nNow that we have all of the tweets translated into English we can use sentiment tools calibrated to the English language. We will try two different approaches. For both we'll compare the measured sentiment of the English tweet to the assigned sentiment in the data set.\n\n1.  Measure the sentiment \"valence\" of each word as either positive or negative and take the balance of positive vs. negative as the sentiment of the tweet.\n2.  Negation turns a positive sentiment into a negative one. \"I do not like\" has one positive word, \"like,\" but is clearly a negative sentiment. Rather than measure each word, we can run a sentence-level model to see if we get a better result.\n\n## Sentiment Measurement at the Word Level\n\nWe will be using the tidytext package in this project and following the approach shown in the [\"Introduction to tidytext\" by Julia Silge and Dave Robinson](https://cran.r-project.org/web/packages/tidytext/vignettes/tidytext.html).\n\nFirst load the needed packages and the translated tweets from part one of this project.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#load libraries\nsuppressPackageStartupMessages({\n  library(tidyverse)\n  library(tidytext)\n  library(hunspell)\n  library(sentimentr)\n  library(magrittr)\n})\n\n# set up some chart defaults\ntan1 <- \"#FDECCD\"\nyellow1 <- \"#FFBF00\"\ngreen1 <- \"#007000\"\n\ntheme_afri <- function(...){\n  # making a function allows passing further theme elements\n  ggplot2::theme(\n    plot.background = element_rect(fill = tan1, color = NA),\n    panel.background = element_blank(),\n    legend.background = element_blank(),\n    legend.box.background = element_blank(),\n    legend.key = element_blank(),\n    ...\n    ) \n}\nafrisenti_translated <- readr::read_csv('https://raw.githubusercontent.com/apsteinmetz/tidytuesday/master/2023-02-28_african_language/data/afrisenti_translated.csv',\n                                        show_col_types = FALSE) |>\n  select(tweet_num,assigned_long,translatedText,label) |> \n  mutate(label = as.factor(label))\nafrisenti_translated\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 111,720 Ã— 4\n   tweet_num assigned_long translatedText                                  label\n       <dbl> <chr>         <chr>                                           <fct>\n 1         1 Amharic       Amhara region needs moderation!!! He hates Musâ€¦ negaâ€¦\n 2         2 Amharic       Having a mood in a way that annoys someone....  negaâ€¦\n 3         3 Amharic       Domestic violence - without hearing - without â€¦ negaâ€¦\n 4         4 Amharic       Ethiopia, what was your role in overthrowing Tâ€¦ negaâ€¦\n 5         5 Amharic       No matter how Oromo learns, they don't realizeâ€¦ negaâ€¦\n 6         6 Amharic       Tish, dark powder, are you standing at 97? Timâ€¦ negaâ€¦\n 7         7 Amharic       Local residents said that the internet and telâ€¦ negaâ€¦\n 8         8 Amharic       What happened to Schimmels is sad. By the wordâ€¦ negaâ€¦\n 9         9 Amharic       An unstable country does not need elections     negaâ€¦\n10        10 Amharic       The idiot and the bubble, the Ethiopian PM Abiâ€¦ negaâ€¦\n# â€¦ with 111,710 more rows\n```\n:::\n:::\n\n\nThe `tidytext` package makes it easy to divide each tweet into separate words and measure the sentiment valence of each one. The `tidytext` package has a few sentiment lexicons. Here, we decide the sentiment of each word using the \"afinn\" lexicon. We chose this because it has 11 shades of sentiment and we hope the finer granularity will be helpful. If a word is not in the lexicon we code it as\"0\" or neutral. Further, we need to use the stems of words. \"Idiots\" is not in our sentiment sources, \"Idiot\" is. Word stemming will fix that. The `hunspell` package using `hunspell_stem()` will do the trick. It returns a list of possible stems (usually just one, but not always) so we have to `unnest()` the list column. The trade-off is that if the word is not in `hunspell`'s dictionary, it drops the word. Fortunately, there are over 49,000 words in the dictionary.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntweet_word_sentiment <- afrisenti_translated %>%\n  select(tweet_num, translatedText, label) %>%\n  unnest_tokens(word, translatedText) |>\n  anti_join(stop_words, by = join_by(word)) %>%\n  mutate(word = hunspell_stem(word)) |> \n  unnest(word) |> \n  left_join(get_sentiments(\"afinn\"),\n            multiple = \"all\",\n            by = join_by(word)) %>%\n  mutate(value = replace_na(value, 0)) %>%\n  rename(sentiment_afinn = value)\n```\n:::\n\n\nI don't know who decided that \"superb\" gets a \"+5\" and \"euphoric\" gets a \"+4,\" but there you are. We can see how the valences are distributed in our tweets. The vast majority of words are not in the lexicon and therefore are coded as zero, neutral.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntweet_word_sentiment |> \n  ggplot(aes(sentiment_afinn)) + \n  geom_histogram(bins = 10,fill = yellow1,color = tan1,binwidth = 1) + \n  scale_x_continuous(n.breaks = 10,limits = c(-5,+5)) + \n  theme_afri() + \n  labs(title = '\"Afinn\" Sentiment Database Allows More Nuance')\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/plot distribution-1.png){width=672}\n:::\n:::\n\n\nNow let's look at how stemming works. The tweet below is coded as negative in the Afrisenti data set.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nafrisenti_translated$translatedText[45]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"These idiots are cannibals and unforgivable renegades of the Eritrean people\"\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntweet_word_sentiment |> \n  filter(tweet_num == \"45\") |> \n  select(word,sentiment_afinn)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 Ã— 2\n  word       sentiment_afinn\n  <chr>                <dbl>\n1 idiot                   -3\n2 cannibal                 0\n3 forgivable               0\n4 renegade                 0\n5 people                   0\n```\n:::\n:::\n\n\nThe benefits and drawbacks of stemming are apparent. We can find the stem of \"idiots\" and \"cannibals\" but \"unforgivable\" gets changed to \"forgivable\" which flips its sentiment. No matter because neither word is in our sentiment lexicon. There are no positive words in this tweet so the sum of all the valences is negative, which matches with the assigned sentiment.\n\nNext we add up the valences for each tweet to arrive at the net sentiment.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntweet_sentiment <- tweet_word_sentiment %>% \n  group_by(tweet_num) %>% \n  summarise(sentiment_afinn = as.integer(sum(sentiment_afinn))) %>% \n  ungroup() |> \n  full_join(afrisenti_translated,by = join_by(tweet_num)) |> \n  mutate(language = as_factor(assigned_long)) |> \n  rename(label_original = label) |> \n  # set NA sentiment to neutral\n  mutate(across(contains(\"sentiment\"),~replace_na(.x,0)))\n```\n:::\n\n\nWe have numeric valences. If we want to compare our measurment to the original we have to make a choice what to label each number. Obviously, zero is neutral but should we expand the neutral range to include, say -1 and +1? Sadly, no. The summary below shows that we already assigned far more neutral labels than the original data set has. This is not a good omen. We wish we had more granularity.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntweet_sentiment <- tweet_sentiment |> \n  mutate(label_afinn = as.factor(cut(sentiment_afinn,\n                                     breaks = c( -Inf,-1,0,Inf),                         labels=c(\"negative\",\"neutral\",\"positive\"))))  |> \n  select(language,tweet_num,translatedText,label_original,label_afinn)\n\nsummary(select(tweet_sentiment,label_original,label_afinn))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  label_original    label_afinn   \n negative:36564   negative:27564  \n neutral :39151   neutral :51688  \n positive:36005   positive:32468  \n```\n:::\n:::\n\n\n## Results\n\nHow did we do? Here are samples of positive-scored tweets along with the original sentiment. If it's not \"positive\" we disagree.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\ntweet_sentiment |> \n  filter(label_afinn == \"positive\") |> \n  slice_sample(n=10) |> \n  select(label_original,translatedText)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 10 Ã— 2\n   label_original translatedText                                                \n   <fct>          <chr>                                                         \n 1 positive       \"@user Amen Thanks so much fam... God bless you all\"          \n 2 neutral        \"No need to extend us. https://t.co/cKIft7Aqk5\"               \n 3 positive       \"Our king is one Mohammed VI... Long live the king and long lâ€¦\n 4 positive       \"'@user @user @user @user and every time the son of their mouâ€¦\n 5 positive       \"@user This problem of insecurity that we are dealing with maâ€¦\n 6 positive       \"Just as our bodies have a great need for food, so even our hâ€¦\n 7 neutral        \"RT @user: You definitely can improve your #yoruba by watchinâ€¦\n 8 positive       \"@user Tanemi forgiveness and forgiveness, God is the best, Gâ€¦\n 9 positive       \"@user Don't agree! We want him to continue as president becaâ€¦\n10 negative       \"This is the appropriate response that deserves the likes of â€¦\n```\n:::\n:::\n\n\nOur scoring looks pretty good. Where we disagree with Afrisenti, I side with tidytext for the most part.\n\nHere are samples of negative-scored tweets with along the original sentiment.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(5)\ntweet_sentiment |> \n  filter(label_afinn == \"negative\") |> \n  slice_sample(n=10) |> \n  select(label_original,translatedText)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 10 Ã— 2\n   label_original translatedText                                                \n   <fct>          <chr>                                                         \n 1 positive       \"@user Good man, this time is up. The children and the childrâ€¦\n 2 neutral        \"who died in power\"                                           \n 3 positive       \"SAY NO TO SUICIDE!!! Reposted from @user - We have some tipsâ€¦\n 4 negative       \"Hunger has no respect \\U0001fae1\"                            \n 5 negative       \"@user And you started ruining it with lies \\U0001f914\\U0001fâ€¦\n 6 positive       \"@user @user @user @user @user @user God forbid evil!\"        \n 7 neutral        \"'It's father Kashi, you can leave this beautifully and go hoâ€¦\n 8 neutral        \"Tete provincial finance director arrested https://t.co/guJWNâ€¦\n 9 negative       \"@user Just as Barcelona were tired, PSG will also be tired, â€¦\n10 negative       \"u see diz life e no go make u hype somethings see diz otondoâ€¦\n```\n:::\n:::\n\n\nIt's the same story with the negative tweets. We do a reasonable job and the correctness of the original sentiment is arguable. There is a lot of disagreement.\n\nWe can generate a confusion matrix with some additional statistics to look at the agreement of our measurements vs. the human classifiers. Ideally all the observations would lie on the diagonal from top left to bottom right.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxt <- table(tweet_sentiment$label_original,tweet_sentiment$label_afinn)\nxt\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          \n           negative neutral positive\n  negative    15083   14619     6862\n  neutral      7761   23921     7469\n  positive     4720   13148    18137\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncaret::confusionMatrix(xt) |> \n  broom::tidy() |> \n  slice_head(n=2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 Ã— 6\n  term     class estimate conf.low conf.high    p.value\n  <chr>    <chr>    <dbl>    <dbl>     <dbl>      <dbl>\n1 accuracy <NA>     0.511    0.509     0.514  2.58e-234\n2 kappa    <NA>     0.264   NA        NA     NA        \n```\n:::\n:::\n\n\n## It's not me, it's you.\n\nAs we look at the cross-tab there are many, many incorrect classifications. The accuracy for both of these measures is scarcely more that 50%. The \"Kappa\" statistic shows that we are only about 26% better than random chance. It's not zero but it's not good. Why the disappointing result? First of all, our valence measure isn't opinionated enough. There are far too many neutral tweets.\n\n\n::: {.cell}\n\n```{.r .cell-code}\navgs <- tweet_sentiment |> \n  group_by(language) |> \n  summarise(across(contains(\"label\"),\n                   ~mean(as.numeric(.x)-2),\n                   .names = \"mean_{str_remove(.col,'label_')}\")) |> \n  pivot_longer(cols = contains(\"mean\"), names_to = \"source\",values_to = \"average\") |> \n  mutate(source = str_remove(source,\"mean_\"))\n\n# plot the sentiment distribution by language\navgs |> ggplot(aes(y = average,x = language)) +\n  geom_point(aes(color = source,shape = source),\n             size = 4) +\n  geom_line() + \n  scale_y_continuous(n.breaks = 5) +\n  scale_shape_manual(values = c(afinn = 15,\n                                sentence=19,original =3)) +\n  scale_color_manual(values = c(afinn = \"red\",\n                                sentence=green1,original =\"black\")) +\n  scale_fill_manual(values = c(afinn = \"red\",\n                               sentence=green1,original =\"black\")) +\n  \n  theme_afri() +\n  labs(y=\"Sentiment\",x=\"Language\",\n       title = \"There is Disagreement About The Average Sentiment\",\n       subtitle = \"Why is English the most divergent?\") +\n  ggplot2::coord_flip()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/plot avg sentiment-1.png){width=672}\n:::\n:::\n\n\nYou may wonder if one of the other sentiment lexicons would produce a better result. I have tried the others but I don't include them here because the results are substantially identical.\n\nIn our defense, I'm not sure the Afrisenti sentiment assignments are better, as we saw above. But maybe that just means Google Translate has stripped some of the emotion out of them that is present in the original language. I don't know, but this would mean Google Cloud Translate doesn't work for this purpose.\n\nHere's the puzzle, though. The biggest disagreement about sentiment is in English-language tweets, where **no translation is needed** so we can't blame Google for this. A look at some English tweets reveals that they are mostly in [Pidgin](https://en.wikipedia.org/wiki/Pidgin) so the vocabulary is not what we would expect in the sentiment sources we're using. Here are some random English tweets:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nafrisenti_translated |> \n  filter(assigned_long == \"English\") |> \n  slice_sample(n=5) |> \n  select(translatedText)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 Ã— 1\n  translatedText                                                                \n  <chr>                                                                         \n1 jesus taught i had sef                                                        \n2 better all those figures they are generating if u never dey vex reach na the â€¦\n3 people don dey find themselves from twitter oo and that ilorin where everyoneâ€¦\n4 it's sweet soul abeg                                                          \n5 as a guyman used to paying the lump sum and calling it a day with the mindsetâ€¦\n```\n:::\n:::\n\n\nSo, paradoxically, the translated tweets of truly foreign languages are rendered into more standard English.\n\nMaybe we'll get more agreement if we address negation, as mentioned above. The tweet below illustrates the problem.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nafrisenti_translated$translatedText[28670]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"@user Why don't America want the world to be peaceful ðŸ¤”ðŸ¤”ðŸ¤”\"\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntweet_word_sentiment |> \n  filter(tweet_num == \"28670\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 Ã— 4\n  tweet_num label    word     sentiment_afinn\n      <dbl> <fct>    <chr>              <dbl>\n1     28670 negative user                   0\n2     28670 negative world                  0\n3     28670 negative peaceful               2\n```\n:::\n:::\n\n\nUnless the negation is itself a negative valence word, many negative tweets will test as positive. In this example \"peaceful\" is positive but \"don't want\" clearly flips it.\n\n**A More Sophisticated Approach**\n\nThere are a number of approaches to solve the negation problem. A simple one is to combine negation words like \"no\" and \"not\" with the subsequent one. We will use this approach in our next post attempting machine learning. Here we'll try sentence-level measurement using the `sentimentr` package. This package understands negation better. Our test tweet from above is a challenge. This is since the negation of \"peaceful\" comes eight words before \"not.\" The default of `n.before = 5` doesn't give us the right answer.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmytweets <- \"Why don't America want the world to be peaceful\"\n\nsentimentr::sentiment(mytweets,n.before = 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   element_id sentence_id word_count sentiment\n1:          1           1          9      0.25\n```\n:::\n:::\n\n\nSetting `n.before = Inf` captures this but comes at the expense of slower processing speed and potentially more false negatives.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsentimentr::sentiment(mytweets,n.before = Inf)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   element_id sentence_id word_count sentiment\n1:          1           1          9     -0.25\n```\n:::\n:::\n\n\nWith that noted, lets forge ahead.\n\nAs before, we start by breaking the tweets up into constituent parts, sentences in this case. Most tweets will be just one sentence of course. Again we compute a sentiment score, this time with the `sentiment_by()` function. It yields a finer grained sentiment score than our earlier measures.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntweet_sentence <- afrisenti_translated |>\n  mutate(language = as.factor(assigned_long)) |>\n  mutate(sentence = get_sentences(translatedText)) %$%\n  sentiment_by(sentence,list(language,tweet_num),n.before = Inf)\n```\n:::\n\n\n::: callout-note\nI am using a less common pipe operator below, `%$%` from the `magrittr` package, which expands the list-column created by `get_sentences`. Normally I would use `tidyr::unnest()` to do this but it loses the special object class that `sentiment_by()` needs. The `sentimentr` package uses the fast `data.table` vernacular, not the `dplyr` one, which I mostly use here.\n:::\n\nBefore we look at the tweet-level sentiment let's group by language. This lets us see if the sentiment measure is consistent across languages.\n\n\n::: {.cell}\n\n```{.r .cell-code}\navgs <- tweet_sentence |> \n  group_by(language) |> \n  summarise(ave_sentiment = mean(ave_sentiment))\n\navgs_orig <- afrisenti_translated |> \n  group_by(assigned_long) |> \n  summarise(ave_sentiment = mean(as.numeric(label)-2))\n\n# plot the sentiment distribution by language\ntweet_sentence |>\n  as_tibble() |>\n  # make the size manageable\n  slice_sample(n=1000) |> \n  group_by(language) |>\n  rename(sentiment = ave_sentiment) |>\n  ggplot() + geom_boxplot(\n    aes(y = sentiment, x = language),\n    fill = NA,\n    color = \"grey70\",\n    width = 0.55,\n    size = 0.35,\n    outlier.color = NA\n  ) +\n  geom_jitter(\n    aes(y = sentiment, x = language),\n    width = 0.35,\n    height = 0,\n    alpha = 0.15,\n    size = 1.5\n  ) +\n  scale_y_continuous(n.breaks = 5) +\n  # theme_bw() +\n  theme_afri() + \n  geom_point(data = avgs,\n    aes(y = ave_sentiment, x = language),\n    colour = \"red\",\n    shape = 18,\n    size = 4) +\n  geom_point(data = avgs_orig,\n    aes(y = ave_sentiment, x = assigned_long),\n    colour = green1,\n    shape = 19,\n    size = 4) +\n  ylab(\"Sentiment\") + xlab(\"Language\") +\n  labs(title = \"Average Sentiment at the Sentence Level is  Consistently Positive\",\n       subtitle = \"There is wide disagreement with English tweets.\") +\n  annotate(\"text\",y = .7,x=7.5,label = \"Red Diamond is Measured\") + \n  annotate(\"text\",y = .7,x=6.5,label = \"Green Circle is Original\") + \n  coord_flip()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/plot sentence sentiment by langauge-1.png){width=672}\n:::\n:::\n\n\nThe dots are the measured sentiment of a sample of tweets. The markers are the average measured sentiment (red diamond) and average original sentiment (green circle), across all tweets. The measured sentiment looks more consistent then the original. The tweets in all languages are scored, on average, positive, by our calculations. We obviously disagree with the original. Note that the original 3-level sentiment as been converted to a numeric range, -1 to 1, to compute the green markers, whereas the range for our measurements is much wider.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntweet_sentence |> \n  ggplot(aes(ave_sentiment)) + geom_histogram(fill=yellow1,bins=50) +\n  scale_x_continuous(limits = c(-3,3))+\n  theme_afri()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/sentences by tweetnum-1.png){width=672}\n:::\n:::\n\n\nWe no longer have the problem where too many tweets are neutral. The median of the distribution above is zero but there are far fewer tweets that are of exactly zero valence. This let's us expand the range of neutral above and below zero. I tried to balance the data to match the distribution of sentiment in the original data as closely as possible.\n\nWhile we're at it let's put our results together.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# picked these levels to balance the data set\nlow = -0.01\nhigh = 0.12\ntweet_sentence <- tweet_sentence |> \n  as_tibble() |> \n  mutate(label_sentence = cut(ave_sentiment,\n                              breaks = c( -Inf,low,high,Inf), labels=c(\"negative\",\"neutral\",\"positive\")))\n\ntweet_sentiment <-\n  left_join(tweet_sentiment, tweet_sentence,\n            by = join_by(language, tweet_num))\n```\n:::\n\n\nCompare our distribution to the original.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntweet_sentiment |> \n    pivot_longer(c(label_original,label_sentence),\n               names_to = \"source\",values_to = \"sentiment\") |> \n  group_by(source,sentiment) |> \n  count() |> \n  group_by(source) |> \n  reframe(sentiment,proportion = n/sum(n)) |> \n  ungroup() |> \n  ggplot(aes(sentiment,proportion,fill=source)) + geom_col(position = \"dodge\") + \n  scale_fill_manual(values = c(yellow1,green1)) + \n    theme_afri() + \n  labs(title = 'Choose Valence Range of \"Neutral\" to Balance Outcomes')\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/plot compare distribution-1.png){width=672}\n:::\n:::\n\n\n## A Dissapointing Outcome\n\nAs you can see, we are pretty balanced. This is kind of cheating because we are calibrating the category breakpoints to the known data. We should be calibrating to a training set and then see how it works on the test set. Let's just see if the in-sample fit is any good first.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxt <- with(tweet_sentiment,table(label_original,label_sentence))\nxt\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n              label_sentence\nlabel_original negative neutral positive\n      negative    17776   11510     7278\n      neutral      9624   18651    10876\n      positive     5558    9859    20588\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncaret::confusionMatrix(xt) |> \n  broom::tidy() |> \n  slice_head(n=2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 Ã— 6\n  term     class estimate conf.low conf.high p.value\n  <chr>    <chr>    <dbl>    <dbl>     <dbl>   <dbl>\n1 accuracy <NA>     0.510    0.507     0.513       0\n2 kappa    <NA>     0.265   NA        NA          NA\n```\n:::\n:::\n\n\nWow. That's disappointing. The agreement with the original sentiment is nearly identical to the word-level measurements. That is, poor. I thought balancing the outcomes to match the frequency of the original would help, at least.\n\nDo our two measures agree with each other?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxt <- with(tweet_sentiment,table(label_afinn,label_sentence))\nxt\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           label_sentence\nlabel_afinn negative neutral positive\n   negative    18667    4586     4311\n   neutral      9301   29272    13115\n   positive     4990    6162    21316\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncaret::confusionMatrix(xt) |> \n  broom::tidy() |> \n  slice_head(n=2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 Ã— 6\n  term     class estimate conf.low conf.high p.value\n  <chr>    <chr>    <dbl>    <dbl>     <dbl>   <dbl>\n1 accuracy <NA>     0.620    0.617     0.623       0\n2 kappa    <NA>     0.425   NA        NA          NA\n```\n:::\n:::\n\n\n## Conclusion\n\nIf we take the sentiment labels in the original data set as \"true,\" our sentiment valence measurements in English do a poor job. A cursory visual examination reveals that the sentiment we assigned with our valence measures look pretty reasonable in most cases but do not predict what is in the Afrisenti data set very well. We therefore conclude that Google Cloud Translate doesn't match well with these tweets using tidytext methods.\n\nWe're not defeated yet. Our measurements had no knowledge of what the original sentiments were. What if we could learn the inscrutable methods used to assign sentiment in the original? In the next post we'll apply machine learning to see if we can train a model using the translated tweets that is in closer agreement with the Afrisenti data set.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}